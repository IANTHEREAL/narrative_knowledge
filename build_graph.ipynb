{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ian/miniconda3/envs/nk/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Base URL: http://192.168.206.101:1234/v1\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from llm.factory import LLMInterface\n",
    "from llm.embedding import get_text_embedding\n",
    "from knowledge_graph.knowledge import KnowledgeBuilder\n",
    "from knowledge_graph.graph_builder import IterativeKnowledgeGraphBuilder\n",
    "\n",
    "# llm_client = LLMInterface(\"bedrock\", \"us.deepseek.r1-v1:0\")\n",
    "llm_client = LLMInterface(\"openai_like\", \"qwen3-32b\")\n",
    "kb_builder = KnowledgeBuilder()\n",
    "graph_builder = IterativeKnowledgeGraphBuilder(llm_client, get_text_embedding)\n",
    "\n",
    "# Initialize logging module with a basic configuration for console output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s] %(levelname)s in %(module)s: %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded configuration from: docs/pdf_metadata.json\n",
      "\n",
      "Example: Accessing first document data:\n",
      "{'path': 'docs/Apple/Apple Customer Questions/Apple Customer Questions.pdf', 'magic_pdf_path': 'docs/Apple/Apple Customer Questions/Apple Customer Questions_magic_pdf', 'pymupdf_path': 'docs/Apple/Apple Customer Questions/Apple Customer Questions_pymupdf', 'magic_pdf_md_path': 'docs/Apple/Apple Customer Questions/Apple Customer Questions_magic_pdf/Apple Customer Questions.md', 'pymupdf_md_path': 'docs/Apple/Apple Customer Questions/Apple Customer Questions_pymupdf/Apple Customer Questions.md', 'client_name': 'Apple', 'created_time': '2023-11-21T20:56:27.253000+00:00', 'modified_time': '2024-01-30T15:52:12.471000+00:00', 'web_view_link': 'https://docs.google.com/document/d/1MG8UTBxv-ZQYWY4AwYTtpQ9uHdfh3ytWaoEg1JN6CaA/edit?usp=drivesdk', 'mime_type': 'application/vnd.google-apps.document'}\n",
      "Client: Rubrik, Number of documents: 6\n",
      "Client: Airbnb, Number of documents: 6\n",
      "Client: Visa - AI Program Control Plane, Number of documents: 9\n",
      "Client: Uber - DocStore Replacement, Number of documents: 10\n",
      "Client: Postman - Collections, Number of documents: 8\n",
      "Client: Remit2Any, Number of documents: 7\n",
      "Client: Theta Lake, Number of documents: 24\n",
      "Client: Visa, Number of documents: 34\n",
      "Client: Block - Expansion, Number of documents: 7\n",
      "Client: Databricks, Number of documents: 4\n",
      "Client: Conga, Number of documents: 24\n",
      "Client: Plaid, Number of documents: 7\n",
      "Client: Airtable, Number of documents: 13\n",
      "Client: Sicar - TiDB Cloud Dedicated GCP, Number of documents: 10\n",
      "Client: Atlassian, Number of documents: 11\n",
      "Client: HubSpot, Number of documents: 9\n",
      "Client: Bottomline, Number of documents: 1\n",
      "Client: Apple, Number of documents: 9\n",
      "Client: OCC, Number of documents: 7\n",
      "Client: LinkedIn, Number of documents: 31\n",
      "Client: N3xt.io, Number of documents: 7\n",
      "Client: Visa VASPD, Number of documents: 5\n",
      "Client: Affirm, Number of documents: 19\n",
      "Client: Visa - Fast Data initiative, Number of documents: 8\n",
      "Client: Fidelity - Distributed SQL Initiative, Number of documents: 9\n",
      "Client: Costco, Number of documents: 8\n",
      "Client: Coupang, Number of documents: 19\n",
      "Client: Robinhood - Robinhood - Postgres Replace, Number of documents: 10\n",
      "Client: Copart -  Maria DB replacement, Number of documents: 11\n",
      "Client: Roblox, Number of documents: 3\n",
      "Client: Giving Gateway, Number of documents: 5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the path to the JSON configuration file\n",
    "config_file_path = 'docs/pdf_metadata.json'\n",
    "\n",
    "# Variable to store the loaded data\n",
    "loaded_docs = []\n",
    "\n",
    "# Read the JSON configuration file\n",
    "try:\n",
    "    with open(config_file_path, 'r', encoding='utf-8') as f:\n",
    "        loaded_docs = json.load(f)\n",
    "    print(f\"Successfully loaded configuration from: {config_file_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Configuration file not found at '{config_file_path}'\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Could not decode JSON from file '{config_file_path}'. Check file format.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while reading the file: {e}\")\n",
    "\n",
    "if len(loaded_docs) > 0:\n",
    "    print(\"\\nExample: Accessing first document data:\")\n",
    "    print(loaded_docs[0])\n",
    "else:\n",
    "    print(\"\\nConfiguration file is empty.\")\n",
    "\n",
    "\n",
    "client_name_list = set([doc['client_name'] for doc in loaded_docs])\n",
    "client_docs = {}\n",
    "for client_name in client_name_list:\n",
    "    client_docs[client_name] = []\n",
    "    for doc in loaded_docs:\n",
    "        if doc['client_name'] == client_name:\n",
    "            client_docs[client_name].append({\n",
    "                'path': doc['path'],  # required\n",
    "                'client_name': doc['client_name'],\n",
    "                'created_time': doc['created_time'],\n",
    "                'modified_time': doc['modified_time'],\n",
    "                'doc_link': doc['web_view_link'],\n",
    "                'mime_type': doc['mime_type']\n",
    "            })\n",
    "    print(f\"Client: {client_name}, Number of documents: {len(client_docs[client_name])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-12 13:54:36,489] INFO in extract: Using magic_pdf to extract data from PDFs\n",
      "[2025-06-12 13:54:36,493] INFO in extract: Processing with magic_pdf: docs/Apple/Apple Customer Questions/Apple Customer Questions.pdf, output directory: docs/Apple/Apple Customer Questions/Apple Customer Questions_magic_pdf\n",
      "\u001b[32m2025-06-12 13:54:36.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mlang: None\u001b[0m\n",
      "\u001b[32m2025-06-12 13:54:36.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.model.doc_analyze_by_custom_model\u001b[0m:\u001b[36mdoc_analyze\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mBatch 1/1: 7 pages/7 pages\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: upload docs to knowledge base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layout Predict: 100%|██████████| 7/7 [00:01<00:00,  4.52it/s]\n",
      "MFD Predict: 100%|██████████| 7/7 [00:02<00:00,  2.44it/s]\n",
      "MFR Predict: 100%|██████████| 4/4 [00:00<00:00, 14.87it/s]\n",
      "OCR-det Predict: 100%|██████████| 7/7 [00:01<00:00,  3.59it/s]\n",
      "Table Predict: 0it [00:00, ?it/s]\n",
      "OCR-rec Predict: 100%|██████████| 212/212 [00:04<00:00, 43.44it/s]\n",
      "Processing pages: 100%|██████████| 7/7 [00:00<00:00, 39.78it/s]\n",
      "[2025-06-12 13:54:48,578] INFO in extract: ✅ Successfully processed with magic_pdf: docs/Apple/Apple Customer Questions/Apple Customer Questions.pdf, output directory: docs/Apple/Apple Customer Questions/Apple Customer Questions_magic_pdf, generated files: Apple Customer Questions.md (main markdown), Apple Customer Questions_content_list.json (content structure), Apple Customer Questions_middle.json (metadata)\n",
      "[2025-06-12 13:54:48,988] INFO in knowledge: Source data already exists for docs/Apple/Apple Customer Questions/Apple Customer Questions.pdf, id: cc5b92b6-ef73-4c4d-8b54-60c610d3443d\n",
      "[2025-06-12 13:54:49,389] INFO in extract: Using magic_pdf to extract data from PDFs\n",
      "[2025-06-12 13:54:49,392] INFO in extract: Processing with magic_pdf: docs/Apple/TiDB's Relationship with MySQL and Security Vulnerabilities/TiDB's Relationship with MySQL and Security Vulnerabilities.pdf, output directory: docs/Apple/TiDB's Relationship with MySQL and Security Vulnerabilities/TiDB's Relationship with MySQL and Security Vulnerabilities_magic_pdf\n",
      "\u001b[32m2025-06-12 13:54:49.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mlang: None\u001b[0m\n",
      "\u001b[32m2025-06-12 13:54:49.429\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.model.doc_analyze_by_custom_model\u001b[0m:\u001b[36mdoc_analyze\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mBatch 1/1: 3 pages/3 pages\u001b[0m\n",
      "Layout Predict: 100%|██████████| 3/3 [00:00<00:00,  4.59it/s]\n",
      "MFD Predict: 100%|██████████| 3/3 [00:01<00:00,  2.44it/s]\n",
      "MFR Predict: 100%|██████████| 2/2 [00:01<00:00,  1.69it/s]\n",
      "OCR-det Predict: 100%|██████████| 3/3 [00:02<00:00,  1.03it/s]\n",
      "Table Predict: 0it [00:00, ?it/s]\n",
      "OCR-rec Predict: 100%|██████████| 59/59 [00:03<00:00, 15.91it/s]\n",
      "Processing pages: 100%|██████████| 3/3 [00:00<00:00,  4.90it/s]\n",
      "[2025-06-12 13:54:59,965] INFO in extract: ✅ Successfully processed with magic_pdf: docs/Apple/TiDB's Relationship with MySQL and Security Vulnerabilities/TiDB's Relationship with MySQL and Security Vulnerabilities.pdf, output directory: docs/Apple/TiDB's Relationship with MySQL and Security Vulnerabilities/TiDB's Relationship with MySQL and Security Vulnerabilities_magic_pdf, generated files: TiDB's Relationship with MySQL and Security Vulnerabilities.md (main markdown), TiDB's Relationship with MySQL and Security Vulnerabilities_content_list.json (content structure), TiDB's Relationship with MySQL and Security Vulnerabilities_middle.json (metadata)\n",
      "[2025-06-12 13:55:00,370] INFO in knowledge: Source data already exists for docs/Apple/TiDB's Relationship with MySQL and Security Vulnerabilities/TiDB's Relationship with MySQL and Security Vulnerabilities.pdf, id: d4f1b212-b354-4e72-b5ea-60b918d16fbf\n",
      "[2025-06-12 13:55:00,766] INFO in extract: Using magic_pdf to extract data from PDFs\n",
      "[2025-06-12 13:55:00,769] INFO in extract: Processing with magic_pdf: docs/Apple/1 - Account Discovery Capture Sheet (with instruction)/1 - Account Discovery Capture Sheet (with instruction).pdf, output directory: docs/Apple/1 - Account Discovery Capture Sheet (with instruction)/1 - Account Discovery Capture Sheet (with instruction)_magic_pdf\n",
      "\u001b[32m2025-06-12 13:55:00.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mlang: None\u001b[0m\n",
      "\u001b[32m2025-06-12 13:55:00.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.model.doc_analyze_by_custom_model\u001b[0m:\u001b[36mdoc_analyze\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mBatch 1/1: 4 pages/4 pages\u001b[0m\n",
      "Layout Predict: 100%|██████████| 4/4 [00:01<00:00,  2.68it/s]\n",
      "MFD Predict: 100%|██████████| 4/4 [00:01<00:00,  2.13it/s]\n",
      "MFR Predict: 100%|██████████| 5/5 [00:01<00:00,  4.21it/s]\n",
      "OCR-det Predict: 100%|██████████| 4/4 [00:02<00:00,  1.48it/s]\n",
      "Table Predict: 100%|██████████| 7/7 [00:21<00:00,  3.03s/it]\n",
      "OCR-rec Predict: 100%|██████████| 24/24 [00:02<00:00,  9.67it/s]\n",
      "Processing pages: 100%|██████████| 4/4 [00:01<00:00,  2.99it/s]\n",
      "[2025-06-12 13:55:33,471] INFO in extract: ✅ Successfully processed with magic_pdf: docs/Apple/1 - Account Discovery Capture Sheet (with instruction)/1 - Account Discovery Capture Sheet (with instruction).pdf, output directory: docs/Apple/1 - Account Discovery Capture Sheet (with instruction)/1 - Account Discovery Capture Sheet (with instruction)_magic_pdf, generated files: 1 - Account Discovery Capture Sheet (with instruction).md (main markdown), 1 - Account Discovery Capture Sheet (with instruction)_content_list.json (content structure), 1 - Account Discovery Capture Sheet (with instruction)_middle.json (metadata)\n",
      "[2025-06-12 13:55:33,871] INFO in knowledge: Source data already exists for docs/Apple/1 - Account Discovery Capture Sheet (with instruction)/1 - Account Discovery Capture Sheet (with instruction).pdf, id: 3afc7b8a-092f-424a-a862-f50050b0ebea\n",
      "[2025-06-12 13:55:34,266] INFO in extract: Using magic_pdf to extract data from PDFs\n",
      "[2025-06-12 13:55:34,271] INFO in extract: Processing with magic_pdf: docs/Apple/Copy of Apple TiDB Overview- Nabil/Louis FINAL 10102023/Copy of Apple TiDB Overview- Nabil/Louis FINAL 10102023.pdf, output directory: docs/Apple/Copy of Apple TiDB Overview- Nabil/Louis FINAL 10102023/Copy of Apple TiDB Overview- Nabil/Louis FINAL 10102023_magic_pdf\n",
      "\u001b[32m2025-06-12 13:55:34.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mlang: None\u001b[0m\n",
      "\u001b[32m2025-06-12 13:55:35.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.model.doc_analyze_by_custom_model\u001b[0m:\u001b[36mdoc_analyze\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mBatch 1/1: 50 pages/50 pages\u001b[0m\n",
      "Layout Predict: 100%|██████████| 50/50 [00:10<00:00,  4.92it/s]\n",
      "MFD Predict: 100%|██████████| 50/50 [00:16<00:00,  2.97it/s]\n",
      "MFR Predict: 100%|██████████| 27/27 [00:03<00:00,  7.73it/s]\n",
      "OCR-det Predict: 100%|██████████| 50/50 [05:49<00:00,  7.00s/it]\n",
      "Table Predict: 100%|██████████| 1/1 [00:03<00:00,  3.11s/it]\n",
      "OCR-rec Predict: 100%|██████████| 1106/1106 [01:51<00:00,  9.92it/s]\n",
      "Processing pages: 100%|██████████| 50/50 [00:22<00:00,  2.24it/s]\n",
      "[2025-06-12 14:04:13,450] INFO in extract: ✅ Successfully processed with magic_pdf: docs/Apple/Copy of Apple TiDB Overview- Nabil/Louis FINAL 10102023/Copy of Apple TiDB Overview- Nabil/Louis FINAL 10102023.pdf, output directory: docs/Apple/Copy of Apple TiDB Overview- Nabil/Louis FINAL 10102023/Copy of Apple TiDB Overview- Nabil/Louis FINAL 10102023_magic_pdf, generated files: Louis FINAL 10102023.md (main markdown), Louis FINAL 10102023_content_list.json (content structure), Louis FINAL 10102023_middle.json (metadata)\n",
      "[2025-06-12 14:04:15,849] INFO in knowledge: Source data already exists for docs/Apple/Copy of Apple TiDB Overview- Nabil/Louis FINAL 10102023/Copy of Apple TiDB Overview- Nabil/Louis FINAL 10102023.pdf, id: 8637134f-3072-4698-8c31-da72c54b8a66\n",
      "[2025-06-12 14:04:16,247] INFO in extract: Using magic_pdf to extract data from PDFs\n",
      "[2025-06-12 14:04:16,253] INFO in extract: Processing with magic_pdf: docs/Apple/Apple Relevant Features/Apple Relevant Features.pdf, output directory: docs/Apple/Apple Relevant Features/Apple Relevant Features_magic_pdf\n",
      "\u001b[32m2025-06-12 14:04:16.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mlang: None\u001b[0m\n",
      "\u001b[32m2025-06-12 14:04:16.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.model.doc_analyze_by_custom_model\u001b[0m:\u001b[36mdoc_analyze\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mBatch 1/1: 32 pages/32 pages\u001b[0m\n",
      "Layout Predict: 100%|██████████| 32/32 [00:05<00:00,  6.05it/s]\n",
      "MFD Predict: 100%|██████████| 32/32 [00:10<00:00,  2.99it/s]\n",
      "MFR Predict: 100%|██████████| 9/9 [00:03<00:00,  2.56it/s]\n",
      "OCR-det Predict: 100%|██████████| 32/32 [01:03<00:00,  1.98s/it]\n",
      "Table Predict: 0it [00:00, ?it/s]\n",
      "OCR-rec Predict: 100%|██████████| 562/562 [00:53<00:00, 10.42it/s] \n",
      "Processing pages: 100%|██████████| 32/32 [00:08<00:00,  3.81it/s]\n",
      "[2025-06-12 14:06:42,427] INFO in extract: ✅ Successfully processed with magic_pdf: docs/Apple/Apple Relevant Features/Apple Relevant Features.pdf, output directory: docs/Apple/Apple Relevant Features/Apple Relevant Features_magic_pdf, generated files: Apple Relevant Features.md (main markdown), Apple Relevant Features_content_list.json (content structure), Apple Relevant Features_middle.json (metadata)\n",
      "[2025-06-12 14:06:42,836] INFO in knowledge: Source data already exists for docs/Apple/Apple Relevant Features/Apple Relevant Features.pdf, id: c0aa6372-0a15-471b-9ada-7f47499e5dd8\n",
      "[2025-06-12 14:06:43,234] INFO in extract: Using magic_pdf to extract data from PDFs\n",
      "[2025-06-12 14:06:43,237] INFO in extract: Processing with magic_pdf: docs/Apple/Apple POC Joint Execution Plan/Apple POC Joint Execution Plan.pdf, output directory: docs/Apple/Apple POC Joint Execution Plan/Apple POC Joint Execution Plan_magic_pdf\n",
      "\u001b[32m2025-06-12 14:06:43.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mlang: None\u001b[0m\n",
      "\u001b[32m2025-06-12 14:06:43.353\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.model.doc_analyze_by_custom_model\u001b[0m:\u001b[36mdoc_analyze\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mBatch 1/1: 13 pages/13 pages\u001b[0m\n",
      "Layout Predict: 100%|██████████| 13/13 [00:02<00:00,  4.44it/s]\n",
      "MFD Predict: 100%|██████████| 13/13 [00:07<00:00,  1.81it/s]\n",
      "MFR Predict: 100%|██████████| 8/8 [00:09<00:00,  1.15s/it]\n",
      "OCR-det Predict: 100%|██████████| 13/13 [00:13<00:00,  1.07s/it]\n",
      "Table Predict:  40%|████      | 4/10 [00:21<00:32,  5.47s/it]\u001b[32m2025-06-12 14:07:37.838\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmagic_pdf.model.batch_analyze\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m199\u001b[0m - \u001b[33m\u001b[1mtable recognition processing fails, not get html return\u001b[0m\n",
      "Table Predict: 100%|██████████| 10/10 [01:17<00:00,  7.74s/it]\n",
      "OCR-rec Predict: 100%|██████████| 35/35 [00:04<00:00,  8.17it/s]\n",
      "Processing pages:   0%|          | 0/13 [00:00<?, ?it/s]\u001b[32m2025-06-12 14:08:38.378\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmagic_pdf.pdf_parse_union_core_v2\u001b[0m:\u001b[36mparse_page_core\u001b[0m:\u001b[36m839\u001b[0m - \u001b[33m\u001b[1mskip this page, not found useful bbox, page_id: 0\u001b[0m\n",
      "Processing pages:  23%|██▎       | 3/13 [00:01<00:06,  1.54it/s]\u001b[32m2025-06-12 14:08:40.328\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmagic_pdf.pdf_parse_union_core_v2\u001b[0m:\u001b[36mparse_page_core\u001b[0m:\u001b[36m839\u001b[0m - \u001b[33m\u001b[1mskip this page, not found useful bbox, page_id: 3\u001b[0m\n",
      "\u001b[32m2025-06-12 14:08:40.328\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmagic_pdf.pdf_parse_union_core_v2\u001b[0m:\u001b[36mparse_page_core\u001b[0m:\u001b[36m839\u001b[0m - \u001b[33m\u001b[1mskip this page, not found useful bbox, page_id: 4\u001b[0m\n",
      "\u001b[32m2025-06-12 14:08:40.329\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmagic_pdf.pdf_parse_union_core_v2\u001b[0m:\u001b[36mparse_page_core\u001b[0m:\u001b[36m839\u001b[0m - \u001b[33m\u001b[1mskip this page, not found useful bbox, page_id: 5\u001b[0m\n",
      "Processing pages: 100%|██████████| 13/13 [00:02<00:00,  5.51it/s]\n",
      "[2025-06-12 14:08:40,850] INFO in extract: ✅ Successfully processed with magic_pdf: docs/Apple/Apple POC Joint Execution Plan/Apple POC Joint Execution Plan.pdf, output directory: docs/Apple/Apple POC Joint Execution Plan/Apple POC Joint Execution Plan_magic_pdf, generated files: Apple POC Joint Execution Plan.md (main markdown), Apple POC Joint Execution Plan_content_list.json (content structure), Apple POC Joint Execution Plan_middle.json (metadata)\n",
      "[2025-06-12 14:08:41,255] INFO in knowledge: Source data already exists for docs/Apple/Apple POC Joint Execution Plan/Apple POC Joint Execution Plan.pdf, id: 7fe32fcd-2a5a-48ea-aec1-cb58a458e915\n",
      "[2025-06-12 14:08:41,664] INFO in extract: Using magic_pdf to extract data from PDFs\n",
      "[2025-06-12 14:08:41,666] INFO in extract: Processing with magic_pdf: docs/Apple/ Technical Discovery Capture Form / Technical Discovery Capture Form .pdf, output directory: docs/Apple/ Technical Discovery Capture Form / Technical Discovery Capture Form _magic_pdf\n",
      "\u001b[32m2025-06-12 14:08:41.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mlang: None\u001b[0m\n",
      "\u001b[32m2025-06-12 14:08:41.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.model.doc_analyze_by_custom_model\u001b[0m:\u001b[36mdoc_analyze\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mBatch 1/1: 4 pages/4 pages\u001b[0m\n",
      "Layout Predict: 100%|██████████| 4/4 [00:07<00:00,  1.80s/it]\n",
      "MFD Predict: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]\n",
      "MFR Predict: 0it [00:00, ?it/s]\n",
      "OCR-det Predict: 100%|██████████| 4/4 [00:17<00:00,  4.34s/it]\n",
      "Table Predict: 0it [00:00, ?it/s]\n",
      "OCR-rec Predict: 100%|██████████| 132/132 [00:12<00:00, 10.28it/s]\n",
      "Processing pages: 100%|██████████| 4/4 [00:02<00:00,  1.89it/s]\n",
      "[2025-06-12 14:09:26,932] INFO in extract: ✅ Successfully processed with magic_pdf: docs/Apple/ Technical Discovery Capture Form / Technical Discovery Capture Form .pdf, output directory: docs/Apple/ Technical Discovery Capture Form / Technical Discovery Capture Form _magic_pdf, generated files:  Technical Discovery Capture Form .md (main markdown),  Technical Discovery Capture Form _content_list.json (content structure),  Technical Discovery Capture Form _middle.json (metadata)\n",
      "[2025-06-12 14:09:27,352] INFO in knowledge: Source data already exists for docs/Apple/ Technical Discovery Capture Form / Technical Discovery Capture Form .pdf, id: e695ddf0-07cc-4120-88c8-bd6829dca2d5\n",
      "[2025-06-12 14:09:27,763] INFO in extract: Using magic_pdf to extract data from PDFs\n",
      "[2025-06-12 14:09:27,767] INFO in extract: Processing with magic_pdf: docs/Apple/Copy of  TiDB 7.0.0 Testing Report/Copy of  TiDB 7.0.0 Testing Report.pdf, output directory: docs/Apple/Copy of  TiDB 7.0.0 Testing Report/Copy of  TiDB 7.0.0 Testing Report_magic_pdf\n",
      "\u001b[32m2025-06-12 14:09:27.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mlang: None\u001b[0m\n",
      "\u001b[32m2025-06-12 14:09:28.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.model.doc_analyze_by_custom_model\u001b[0m:\u001b[36mdoc_analyze\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mBatch 1/1: 26 pages/26 pages\u001b[0m\n",
      "Layout Predict: 100%|██████████| 26/26 [00:06<00:00,  3.77it/s]\n",
      "MFD Predict: 100%|██████████| 26/26 [00:13<00:00,  1.86it/s]\n",
      "MFR Predict: 100%|██████████| 48/48 [00:03<00:00, 15.38it/s]\n",
      "OCR-det Predict: 100%|██████████| 26/26 [00:48<00:00,  1.87s/it]\n",
      "Table Predict: 100%|██████████| 58/58 [04:25<00:00,  4.57s/it]\n",
      "OCR-rec Predict: 100%|██████████| 399/399 [00:33<00:00, 11.77it/s] \n",
      "Processing pages: 100%|██████████| 26/26 [00:09<00:00,  2.65it/s]\n",
      "[2025-06-12 14:15:50,190] INFO in extract: ✅ Successfully processed with magic_pdf: docs/Apple/Copy of  TiDB 7.0.0 Testing Report/Copy of  TiDB 7.0.0 Testing Report.pdf, output directory: docs/Apple/Copy of  TiDB 7.0.0 Testing Report/Copy of  TiDB 7.0.0 Testing Report_magic_pdf, generated files: Copy of  TiDB 7.0.0 Testing Report.md (main markdown), Copy of  TiDB 7.0.0 Testing Report_content_list.json (content structure), Copy of  TiDB 7.0.0 Testing Report_middle.json (metadata)\n",
      "[2025-06-12 14:15:52,230] INFO in knowledge: Source data already exists for docs/Apple/Copy of  TiDB 7.0.0 Testing Report/Copy of  TiDB 7.0.0 Testing Report.pdf, id: dc5dde77-495a-4b16-a57b-957a93d4660c\n",
      "[2025-06-12 14:15:52,678] INFO in extract: Using magic_pdf to extract data from PDFs\n",
      "[2025-06-12 14:15:52,681] INFO in extract: Processing with magic_pdf: docs/Apple/Apple TiDB Meeting Jan 22 2024/Apple TiDB Meeting Jan 22 2024.pdf, output directory: docs/Apple/Apple TiDB Meeting Jan 22 2024/Apple TiDB Meeting Jan 22 2024_magic_pdf\n",
      "\u001b[32m2025-06-12 14:15:52.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mlang: None\u001b[0m\n",
      "\u001b[32m2025-06-12 14:15:53.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.model.doc_analyze_by_custom_model\u001b[0m:\u001b[36mdoc_analyze\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mBatch 1/1: 15 pages/15 pages\u001b[0m\n",
      "Layout Predict: 100%|██████████| 15/15 [00:02<00:00,  5.37it/s]\n",
      "MFD Predict: 100%|██████████| 15/15 [00:04<00:00,  3.29it/s]\n",
      "MFR Predict: 100%|██████████| 6/6 [00:07<00:00,  1.29s/it]\n",
      "OCR-det Predict: 100%|██████████| 15/15 [00:29<00:00,  1.97s/it]\n",
      "Table Predict: 100%|██████████| 4/4 [00:32<00:00,  8.17s/it]\n",
      "OCR-rec Predict: 100%|██████████| 204/204 [00:27<00:00,  7.35it/s] \n",
      "Processing pages: 100%|██████████| 15/15 [00:07<00:00,  2.07it/s]\n",
      "[2025-06-12 14:17:45,776] INFO in extract: ✅ Successfully processed with magic_pdf: docs/Apple/Apple TiDB Meeting Jan 22 2024/Apple TiDB Meeting Jan 22 2024.pdf, output directory: docs/Apple/Apple TiDB Meeting Jan 22 2024/Apple TiDB Meeting Jan 22 2024_magic_pdf, generated files: Apple TiDB Meeting Jan 22 2024.md (main markdown), Apple TiDB Meeting Jan 22 2024_content_list.json (content structure), Apple TiDB Meeting Jan 22 2024_middle.json (metadata)\n",
      "[2025-06-12 14:17:46,186] INFO in knowledge: Source data already exists for docs/Apple/Apple TiDB Meeting Jan 22 2024/Apple TiDB Meeting Jan 22 2024.pdf, id: 863ab62a-490b-4f02-bf0c-411fd54a51d8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cc5b92b6-ef73-4c4d-8b54-60c610d3443d': {'source_id': 'cc5b92b6-ef73-4c4d-8b54-60c610d3443d',\n",
       "  'source_name': 'Apple Customer Questions',\n",
       "  'source_content': '# Question\\n\\nHow does TiCDC handle the processes if a node goes down.\\n\\n# Answer\\n\\nWhen a node failure occurs during TiCDC replication, the system takes the following steps to recover:\\n\\n1.Detect the node failure: TiCDC uses a heartbeat mechanism to detect node failures. If a node does not respond to a heartbeat within a certain timeout, it is considered failed.   \\n2．Elect a new leader: Once a node failure is detected, TiCDC will elect a new leader from the remaining nodes.   \\n3.Synchronize data with the new leader: The new leader will need to synchronize its data with the failed node before it can start replicating changes.   \\n4．Resume replication: Once the new leader is synchronized, it will resume replicating changes from TiDB to the downstream systems.\\n\\nDuring the recovery process, there may be a brief period of time when no changes are replicated. This is because TiCDC needs to wait for the new leader to be elected and synchronized before it can start replicating again.The duration of this downtime will depend on the number of nodes in the TiCDC cluster and the size of the data that needs to be synchronized.\\n\\nTo minimize the impact of node failures,it is recommended to run TiCDC with a high availability mode. These modes provide additional redundancy and fault tolerance, which can help to ensure that TiCDC continues to replicate changes even if there are node failures.\\n\\n# Question\\n\\nIs bidirectional between MySQL and TiDB.\\n\\n# Answer\\n\\nTiDB does not support bidirectional replication between MySQL and TiDB directly. TiDB provides a tool called TiDB Data Migration (DM) that supports full data migration and\\n\\nincremental data replication from MySQL compatible database into TiDB.This means you can replicate data from MySQL to TiDB,but bidirectional replication between MySQL and TiDB is not supported out of the box.\\n\\n# Question\\n\\nAre multiple replication streams supported with TiCDC?\\n\\n# Answer\\n\\nYes,TiCDC (TiDB Change Data Capture) supports multiple replication streams.TiCDC is designed to capture changes made to data/structure in a TiDB cluster and replicate these changes to downstream systems or other databases or storage locations in real-time.\\n\\n# Question\\n\\nDetected MySQL 5.7 EOL libraries in TiDB but they can only use MySQL 8.Ox libraries,Apple scans & check for EOL libraries.\\n\\n# Answer\\n\\nPlease provide us with the list of MySQL 5.7 EOL libraries detected and why this is a concern for Apple.\\n\\n# Question\\n\\nCan TiCDC have multiple sources from multiple masters? From multimaster to a single source/table.\\n\\n# Answer\\n\\nYes,TiCDC supports bi-directional replication and can support having multiple sources and targets for replication,\\n\\n# Question\\n\\nHow do we tune DM for parallel threads?\\n\\n# Answer\\n\\nBy increasing the values of worker-count and max-paralel-applier-count, you can allocate more threads and paralel appliers to handle the replication tasks,which can improve the overall parallelism and performance of DM.\\n\\nIt\\'s important to note that the optimal values for these parameters depend on your specific workload and hardware resources. You may need to experiment and adjust these values based on your environment to achieve the best performance. Additionally, make sure that your hardware resources,such as CPU and memory, are sufficient to handle the increased parallelism.\\n\\n# Question\\n\\nHow does TiDB performance compare with Singlestore.\\n\\nAnswer Overall we are very competitive with SingleStore,this all depends on the specific workload.\\n\\nTiDBvs SingleStore Comparison deck   \\nhttps://docs.google.com/presentation/d/1Xa9S6boCzJiqsHy2laM-6t7h5cL2D4_Nfsf3VWF2_-0/e   \\ndit#slide $\\\\ O =$ id.g446c4deb4d 0 341\\n\\n# Question\\n\\nNot looking to replace MySQL,looking for other use cases that would target Singlestore, Postgres,and Oracle. Target larger use cases and send info on Oracle replacement\\n\\n# Answer\\n\\nWe have a customer story PingAn Group that went from Oracle Exadata to TiDB.\\n\\nPINGAN Finance·Technology\\n\\n# PingAn Group\\n\\na world-leading retail financial services group and China\\'s leading managed care provider.\\n\\n# Problem\\n\\n# Solution\\n\\n# Results\\n\\nPingAn Group was using a large number of Oracle in various core business system, including Billing & Premium, Underwriting, Claims Processing, Marketing, etc.\\n\\nAs the business continues to grow,the single-node Oracle database has become a bottleneck,and the cost of scaling up has sharply increased.\\n\\nImpressed by TiDB\\'s horizontal scalability, high availability, PingAn Group can easily handle the explosive growth in data volume and business requests, which significantly reducing TCO.\\n\\nPing An Group has extensively adopted TiDB to replace multiple Oracle databases currentlyused in core business systems (such as Billing&Premium, Underwriting Processing).\\n\\nTiDB is applied in more than 70 business systems within PingAn Group,including 9 major core business systems.Additional benefits include:\\n\\nReduced overall TCO:   \\nCompared with Oracle,   \\nusing TiDB can reduce TCO by $40 \\\\%$   \\nNo database bottleneck any more,meanwhilereduced the maintenance cost.   \\nProvided longer-term data storage.\\n\\n# Questions waiting for Flipcart reply:\\n\\n# Concern\\n\\nConflict detection - not done w/TiCDC,we don\\'t handle the write conflict Do we have plans to do Write conflict resolution in TiCDC\\n\\nChecking with Ben Meadowcroft on the above two items for TiCDC\\n\\nFlipkart are handling it at the application level (Linkedln also handling at the app level).\\n\\nThey were planning on handling at the application level with different geographic regions being responsible for handling the updates related to user data in a given region.\\n\\nQuestion if we update the primary key vs a regular column how do we handle the write detection for this?\\n\\n# Answer\\n\\nChecking with our Product Management team on this\\n\\nQuestion How is Flipkart handling this at the application level to handle write conflicts with TiCDC?\\n\\n# Answer\\n\\nCheckingwith our Product Management team on this\\n\\n# Question\\n\\nHow to handle leting the application know that conflict happened.Is there any way to detect a write conflict from TiCDC such as from a logfile?\\n\\n# Answer\\n\\nChecking with the Product Manager\\n\\nALL THE ABOVE : We have reached out to Flipcart and are waiting to hear back.\\n\\n# Jan 30,2024\\n\\nQuestion: How to failover from a Primary TiDB cluster to a Standby TiDB cluster with TiCDC replication?\\n\\nAnswer: To failover from a Primary TiDB cluster to a Disaster Recovery (DR) cluster using TiCDC you can follow these steps:\\n\\nEnsure that the primary TiDB cluster is no longer accessible or experiencing a failure.This could be due to a disaster,power outage,or any other reason that renders the primary cluster unavailable.\\n\\nStop all TiCDC processes running in the primary cluster. This is important to prevent data synchronization from resuming and to ensure a clean failover process.\\n\\nOn the DR cluster, start the TiCDC processes to replicate data from the primary cluster. This can be done by running the cdc cli changefeed create command with the appropriate parameters. Here is an example command:\\n\\ncdc cli changefeed create --start-ts $\\\\ c =$ <start-ts> --sink-uri $\\\\ c =$ \"mysql://<username>:<password>@<dr-cluster-ip>:<dr-cluster-port>/<database>\"\\n\\nIn this command:\\n\\nstart-ts specifies the timestamp to start replicating data from. You can choose the appropriate timestamp based on your requirements.\\n\\nsink-uri specifies the address of the DR cluster where the data will be replicated to.\\n\\nOnce the TiCDC processes are running in the DR cluster,it will start replicating data from the primary cluster.The DR cluster will gradually catch up with the state of the primary cluster.\\n\\nUpdate the connection configurations of your applications or clients to connect to the DR cluster instead of the primary cluster.This ensures that the applications are now accessing the DR cluster for data operations.\\n\\nIt\\'s important to note that the above steps assume you have already set up the DR cluster and configured TiCDC to replicate data from the primary cluster to the DR cluster.The DR cluster should be configured and ready to receive data from the primary cluster before the failover process.\\n\\nFor more detailed information and best practices on setting up and using TiCDC for disaster recovery scenarios, you can refer to the official TiCDC documentation\\n\\nQuestion: Provide monitoring metrics for Sink errors.\\n\\n# Answer:\\n\\nhttps://docs.pingcap.com/tidb/dev/monitor-ticdc\\n\\nhttps://docs.pingcap.com/tidb/dev/troubleshoot-ticdc\\n\\n# How do l know whether a TiCDC replication task is interrupted?\\n\\nCheck the changefeed checkpoint monitoring metric of the replication task (choose the right changefeed id) in the Grafana dashboard. If the metric value stays unchanged,or the checkpoint lag metric keeps increasing, the replication task might be interrupted.   \\nCheck the exit error count monitoring metric. If the metric value is greater than O,an error has occurred in the replication task. Execute cdc cli changefeed list and cdc cli changefeed query to check the status of the replication task. stopped means the task has stopped, and the error item provides the detailed error message. After the error occurs,you can search error on running processor in the TiCDC server log to see the error stack for troubleshooting.   \\nIn some extreme cases, the TiCDC service is restarted. You can search the FATAL level log in the TiCDC server log for troubleshooting.\\n\\nQuestion: Can a single ticdc cluster be used by multiple tidb clusters or not?\\n\\nAnswer: l cross checked with our Engineering and Product Manager teams.They stated this is a roadmap item.There is no timeline yet. Currently if a TiDB cluster needs to be replicated then a TiCDC cluster is also required.\\n\\nhttps://docs.pingcap.com/tidb/stable/ticdc-deployment-topology#ticdc-deployment-topology\\n\\nQuestion: Can sink diff inspector detect which tables are having issues?\\n\\nAnswer: Yes.The link below shows an example output.\\n\\nhttps://docs.pingcap.com/tidb/dev/sync-diff-inspector-overview#progress-information',\n",
       "  'source_link': 'docs/Apple/Apple Customer Questions/Apple Customer Questions.pdf'},\n",
       " 'd4f1b212-b354-4e72-b5ea-60b918d16fbf': {'source_id': 'd4f1b212-b354-4e72-b5ea-60b918d16fbf',\n",
       "  'source_name': \"TiDB's Relationship with MySQL and Security Vulnerabilities\",\n",
       "  'source_content': \"Clarifying TiDB's Relationship with MySQL and Security Vulnerabilities\\n\\nIntroduction:\\n\\nThere is a concern among some users regarding the MySQL server version that appears when connecting to a TiDB Cluster. This has led to the misconception that TiDB relies on MySQL libraries,which is inaccurate.This document aims to clarify TiDB's relationship with MySQL and address the issue of false positives generated by security scanning tools.\\n\\nTiDBvs.MySQL: Compatibility vs.Dependency:\\n\\nCompatibility: TiDB is compatible with MySQL versions 5.7 and 8. This allows TiDB to understand and respond to MySQL commands and protocols. However, compatibility does not equate to dependency. TiDB is not dependent on MySQL. TiDB is written entirely from scratch using the Go programming language. There are no MySQL libraries,code or storage engines (i.e. InnoDB). TiDB has its own SQL compute layer and it is built on top of RocksDB as the storage tier.We have taken and customized RocksDB for our own version of RocksDB, fully maintained by PingCAP's Engineering team consisting of over 300 engineers.whereas MySQL uses ${ \\\\mathsf { C } } / { \\\\mathsf { C } } + +$ .This independent development process ensures that TiDB is not directly affected by vulnerabilities specific to MySQL.We are in control of the product roadmap.\\n\\nTiDB and Scanning software:\\n\\nFalse Positives: Security scanners often identify potential vulnerabilities based on the detected MySQL protocol version.This can lead to numerous false positives for TiDB,as the compatibility layer does not translate to actual dependencies on vulnerable libraries.\\n\\nAddressing Security Concerns:\\n\\nPrinciple-based Scans: We encourage users to conduct vulnerability scans based on principles rather than solely relying on scanner outputs based on MySQL versions. Principle-based scans focus on identifying real and exploitable vulnerabilities, regardless of the underlying protocols. Fixed Vulnerabilities: We maintain a comprehensive list of fixed security vulnerabilities documented publicly at https://www.pingcap.com/security/. This list provides transparency and allows users to verify the status of known vulnerabilities. To distinguish whether a reported vulnerability is a false positive,we recommend modifying the server version to a version that doesn't contain the reported vulnerability, following the configuration provided on our official website,and rescanning to confirm if the security vulnerability scanning tool is generating false positives based on version scans. Please run the following commands in the link below to change the TiDB version and restart the cluster and then re-scan with Qualys and share the results:\\n\\nhttps://docs.pingcap.com/tidb/stable/high-reliability-faq#does-tidb-support-modifyin g-the-mysql-version-string-of-the-server-to-a-specific-one-that-is-required-by-the-s ecurity-vulnerability-scanning-tool\\n\\nDirect Reporting: Users are encouraged to report any suspected vulnerabilities directly to the TiDB security team. This allows for prompt investigation and resolution of potential security issues.\\n\\nConclusion:\\n\\nTiDB's compatibility with MySQL should not be misconstrued as a dependency on MySQL libraries.The independent development process ensures that TiDB is not subject to vulnerabilities inherent to ${ \\\\mathsf { C } } / { \\\\mathsf { C } } + +$ .We encourage users to adopt principle-based security scans and refer to our documented list of fixed vulnerabilities for further information.By understanding the true relationship between TiDB and MySQL, users can make informed decisions and maintain a secure environment.\",\n",
       "  'source_link': 'https://docs.google.com/document/d/19L_LG4XokKIBVVnS_yZAxsoPGsj9ty5h2agZgPsTsfg/edit?usp=drivesdk'},\n",
       " '3afc7b8a-092f-424a-a862-f50050b0ebea': {'source_id': '3afc7b8a-092f-424a-a862-f50050b0ebea',\n",
       "  'source_name': '1 - Account Discovery Capture Sheet (with instruction)',\n",
       "  'source_content': '# Account Name:\\n\\n# What the company Does:\\n\\n# Verified requirement for Commercial Support 中 (Yes/No)\\n\\n<html><body><table><tr><td>Application | Project I Initiative Name:</td><td colspan=\"4\"></td></tr><tr><td>Circle the Sales Motion:</td><td>Build</td><td>Select</td><td>Replace</td><td>Migrate</td></tr><tr><td>Name of person(s) we\\'re meeting with & role(s):</td><td colspan=\"4\"></td></tr><tr><td>Who is the economic buyer?</td><td colspan=\"4\"></td></tr></table></body></html>\\n\\n<html><body><table><tr><td>Why do Anything(Value Drivers)?</td><td></td></tr><tr><td>Timeline/Deadline/Milestone Date? Why?</td><td>(No date, no deal)</td></tr></table></body></html>\\n\\n<html><body><table><tr><td>Current State (people, process, technology):</td><td>Future State:</td></tr><tr><td>Example:</td><td>Example:</td></tr><tr><td>Application is 5 years old</td><td>Expand the scope of spaces they cover both in terms of geographies</td></tr><tr><td>SaaS/DaaS platform</td><td>and drug categories</td></tr><tr><td>Collects clinical drug trial data They bring together data, from multiple disparate sources</td><td></td></tr><tr><td>Sold by modules - people buy only what they need by modules</td><td></td></tr><tr><td>Provides multi-tenant environment with customized module stack each tenant</td><td></td></tr></table></body></html>\\n\\n<html><body><table><tr><td>Required Capabilities (people, process, technology):</td><td>Other Key Information(people, timelines, competing priorities, etc.):</td></tr><tr><td>Example:</td><td>Example:</td></tr><tr><td>Scale horizontally in a flexible way Provide multi-tenant isolation</td><td>Their customers are using the platform to collect data for</td></tr><tr><td></td><td>Market research Conference presentations</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></table></body></html>\\n\\n# Discovery Workshop Questions\\n\\n<html><body><table><tr><td>Account name</td><td>The name of the organization</td></tr><tr><td>Replace / Migrate</td><td>. What are the pain points for the existing data platform?</td></tr><tr><td>.</td><td>What are the key requirements for the new data platform?</td></tr><tr><td></td><td>What other options are they looking at?</td></tr><tr><td></td><td>What happens if they do nothing?</td></tr><tr><td></td><td>Is there a compelling event in the future (something that creates a deadline)?</td></tr><tr><td></td><td>Have they been given a specific deadline to meet for finding another platform, complete the project,or</td></tr><tr><td></td><td>otherwise fixing the problem on the current platform? If yes, who gave the deadline?</td></tr><tr><td></td><td>Who else in the company knows about the pain points with the current platform?</td></tr><tr><td></td><td>Does company leadership know?</td></tr><tr><td></td><td></td></tr><tr><td></td><td>O If so, are they motivated to get the problem solved?</td></tr><tr><td></td><td>Is there alarger project for any necessary application work forconverting to the new platform (ex.converting</td></tr><tr><td></td><td>from NoSQL to SQL)?</td></tr><tr><td></td><td> Does this project have funding?</td></tr><tr><td></td><td>What are the timelines and milestones dates for the project?</td></tr><tr><td></td><td>0 Who manages the project?</td></tr><tr><td></td><td>What cross-functional teams are involved (developers,infrastructure,architects,etc.) 。</td></tr><tr><td></td><td>How would they test TiDB Cloud to make sure it meets their requirements?</td></tr><tr><td></td><td>What would be their success criteria?</td></tr><tr><td></td><td>Who would validate that the success criteria as satisfied? Is a PoC necessary to validate the success criteria?</td></tr></table></body></html>\\n\\n<html><body><table><tr><td></td><td>Who is the economic buyer?</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td>What is the role of our primary contact?</td></tr><tr><td></td><td></td><td></td></tr></table></body></html>\\n\\n<html><body><table><tr><td>Application |</td><td></td><td></td></tr><tr><td rowspan=\"8\">Project | Initiative</td><td></td><td>What does the application do?</td></tr><tr><td></td><td>What is the priority of the success of the application to the business?</td></tr><tr><td></td><td>Who is the primary audience for the application?</td></tr><tr><td></td><td>What type(s) of data is processed by the application? (structured user input, unstructured user</td></tr><tr><td></td><td>content, streaming loT, etc.)</td></tr><tr><td></td><td>How is the application / data used internally?</td></tr><tr><td></td><td>What type of reporting and analytics are needed on the data</td></tr><tr><td></td><td></td></tr><tr><td rowspan=\"8\">Key Players</td><td></td><td>What is the role of our primary contact with respect to the project? (beware the trap of just</td></tr><tr><td></td><td>accepting the individual\\'s title). Which team builds the application?</td></tr><tr><td></td><td>Who is leading the team for this project?</td></tr><tr><td></td><td>What\\'s their position in the org chart?</td></tr><tr><td></td><td>0 Who do they report to?</td></tr><tr><td></td><td>Who reports to them?</td></tr><tr><td>.</td><td>Who is ultimately responsible for the success or failure of the application?</td></tr><tr><td>Why do Anything</td><td></td></tr><tr><td rowspan=\"8\"></td><td></td><td>What is the business reason for creating the application?</td></tr><tr><td></td><td>Why has the business decided that now is the right time to take action?</td></tr><tr><td>.</td><td>What is the value driver behind the decision?</td></tr><tr><td></td><td>Compete and make money = Drive growth</td></tr><tr><td></td><td>Save money = Lower TCO 。</td></tr><tr><td></td><td>0 Ensure the safety of data and the business = Reduce risk</td></tr><tr><td></td><td>0 Drive development velocity = Accelerate time to value</td></tr><tr><td></td><td></td></tr><tr><td rowspan=\"5\">Timeline / Deadline / Milestone Dates No date, no deal</td><td colspan=\"2\">We need to find out the exact go live date the customer is targeting.</td></tr><tr><td colspan=\"2\">What if they could get there faster?</td></tr><tr><td colspan=\"2\">What happens if they miss the deadline</td></tr><tr><td colspan=\"2\">We need to find out the compelling event that is driving the urgency to take action. We also must establish the date beyond which failing to change willcause tangible damage to</td></tr><tr><td colspan=\"2\">the application or to the business</td></tr></table></body></html>',\n",
       "  'source_link': 'https://docs.google.com/document/d/1RUV6-5dZ0Is3sp40jGQyeELRluxeMncDL4yGcygGT2c/edit?usp=drivesdk'},\n",
       " '8637134f-3072-4698-8c31-da72c54b8a66': {'source_id': '8637134f-3072-4698-8c31-da72c54b8a66',\n",
       "  'source_name': 'Louis FINAL 10102023',\n",
       "  'source_content': '# TiDB Overview for Apple\\n\\nNabil Nawaz Principal Solution Engineer\\n\\nLouis Fahrberger Account Executive\\n\\n# Agenda\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant model   \\nSecurity   \\nNext steps\\n\\n# Safe Harbor Statement\\n\\nThis presentation has been prepared for general informational purposes only. All information contained in this presentation is provided in good faith, however we make no representation or warranty of any kind, express or implied, regarding the accuracy, adequacy， completeness ofany information.The information may not be incorporated into any contract. The development, release and timing of any features or functionality described for PingCAP products remains at the sole discretion of PingCAP.\\n\\n# Agenda\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant model   \\nSecurity   \\nNext steps\\n\\n# Topics\\n\\nTiDBArchitecture   \\nActive-Active setup with in region and across the   \\nregion   \\nBackup/Restore   \\nBenchmarking results - competitive，standard TPC   \\nOLTP/OLAP/Mixed   \\nObservability and performance insight   \\nData Migration Tools   \\nMulti Tenant model   \\nSecurity (MTLS， Encryption， roles,authentication,   \\nauthorization ，auditing and so on)\\n\\n# TiDB Introduction\\n\\n# Introductions\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant model   \\nSecurity   \\nNext steps\\n\\n![](images/eb89f83eeed4fb65b953b5f55613c8887efbbbf2108bc36898daeea6660a6898.jpg)\\n\\n![](images/c2f2f6262a8dbbdad5a4fe35d1716f7584072ba40aaf910686b41c6eb32ddbe4.jpg)\\n\\n![](images/1f642dcebbf30ad2c5be9dfb3f759acbc7ef826da72e1e07eb89027e98636ac0.jpg)\\n\\nLouis Fahrberger Sr. Enterprise Account Executive\\n\\nStephan Debray VP Sales North America\\n\\nNabil Nawaz   \\nPrincipal Solutions   \\nEngineer\\n\\n![](images/bc70c23991148b07c941997d64130ebd0b09b107b9077c50a7c56f4a97e48036.jpg)\\n\\n![](images/b62d238abef187bb17bc58fa1424bd45d50feee4b81118f0bbad7c93dd5f4f79.jpg)\\n\\n![](images/76e2f5f2114503f964b5e98735e72de8334f55192a897c2598bfce441ad3e534.jpg)\\n\\nYu Dong Sr Director Engineering\\n\\nJingpeng Zhang Director Engineering\\n\\nKevin Liu Architect and Director Engineering\\n\\n# PingCAP\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant model   \\nSecurity   \\nNext steps\\n\\nFunded by:\\n\\n![](images/6eca5b06d0a7f8db79cb0bffe018ecccf0fb35c100a5377e17d6282418067e72.jpg)  \\nConfidential and Proprietary. Do not distribute without PingCAP consent. $\\\\circledcirc$ PingCAP2023.All rights reserved.\\n\\nSEQUOIAI GGVCAPITAL COATUE\\n\\nm GIC\\n\\n# PingCAP TiDB World-Leading Distributed SQL Database\\n\\nOpen Source from day one\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant model   \\nSecurity   \\nNext steps\\n\\nOGitHub星\\n\\n![](images/6c856d775b353fcfee9a2793f989b785ace9594f246ee61351a9d2466cc64110.jpg)\\n\\n33,000+ Stars\\n\\n2,000+\\n\\nLarge Scale Adopters\\n\\n1,500+\\n\\nEcosystem Contributors\\n\\n# PingCAP\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant model   \\nSecurity   \\nNext steps\\n\\n# Trusted and Verified by Innovation Leaders\\n\\n（20 $1 5 0 0 +$ production adopters\\n\\nNIANTIC ? Pinterest databricks SShopee Catalyst □Square  Streak PING AN dailymotion @colopl m xiaomi Finance ·Technology\\n\\nAnd...\\n\\n# Travel&Rental SocialMedia Online Gaming Banking\\n\\n# Cryptocurrency\\n\\nA US industry leader\\n\\nA US industry leader\\n\\nA US industry leader\\n\\nA global top 3 industry leader\\n\\n•A global exchange market\\n\\n# TiDB\\n\\n# What is TiDB ?\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant model   \\nSecurity   \\nNext steps\\n\\nThe most advanced, open source, distributed SQL database for building modern applications that are relentlessly Scalable，Reliable,and Versatile.\\n\\nHTAP/Hybrid data engine for OLTP and OLAP processing\\n\\nAutomatically distributes and replicates data in the cluster\\n\\nCloud native，MySQL compatible,and ACID guarantees for queries\\n\\nTiDB\\n\\n# TiDBBenefits\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant model   \\nSecurity   \\nNext steps\\n\\n![](images/e6eb4b028fcf750f82012a54cdcfdedcb922a75a6d56c4585b80dd676b81c643.jpg)\\n\\n# Exploring TiDB\\'s Architecture\\n\\n# Performance at Scale\\n\\nIntroduction\\n\\nLegacy, monolithic databases are difficult and expensive to scale Modern, distributed databases effortlessly scale horizontally\\n\\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant model   \\nSecurity   \\nNext steps\\n\\n![](images/ca57f5cc6901a05dd5494f0b4464a2cf0ce214ea96e9f4178224fef5a727592f.jpg)  \\nDatabase Size /Concurrency\\n\\n# TiDBArchitecture\\n\\n# Introduction\\n\\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant model   \\nSecurity   \\nNext steps\\n\\n![](images/0573aaf2bf101812b4e5cd4ff91e37bbd21f9cbc0153ff5596b0c21cfa35ccaf.jpg)\\n\\n# The power of both OLTP and OLAP\\n\\nIntroduction\\n\\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant model   \\nSecurity   \\nNext steps\\n\\n![](images/6a391478d84bfd5e50dcd38c44ab2aa3fbb4cd089c0e6faf4e2d38c709f548d0.jpg)\\n\\nSmart Optimizer retrieves data from from TiKV/TiFlash nodes, Transparent to the user\\n\\n# TiDBReplication\\n\\n# What is Change Data Capture?\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant mode   \\nSecurity   \\nNext steps\\n\\nChange data capture (CDC) refers to the process of identifying and capturing changes made to data in a database and then delivering & applying those changes in real-time to a downstream process or system.\\n\\nChanges involve\\n\\nDML such as insert, update, delete DDL such as create table, drop table,alter table\\n\\n# CDC Diagram\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant model   \\nSecurity   \\nNext steps\\n\\n![](images/d3320327bf161609eb24ac914fbe01a39d7144998571e5d623675994ceb54e1d.jpg)\\n\\nTransaction Logs\\n\\n# CDC Diagram\\n\\nIntroduction Architecture Replication Backup/Restore Monitoring Benchmarks Multi-tenant model Security 。 Next steps\\n\\n1. Multiple TiCDC processes pull data changes from TiKV nodes   \\n2. Data changes sorted and merged internally.   \\n3. Data changes are replicated to multiple downstream systems\\n\\n![](images/3efcac9e68364dc142d82419113603f78f88554b03f85e0fce46916c50edf12d.jpg)\\n\\n# CDC Supported Scenarios\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant model   \\nSecurity   \\nNext steps\\n\\nSelf-Managed\\n\\nTiDB $\\\\mathrm { - } \\\\mathrm { > }$ TiCDC $\\\\displaystyle - >$ Kafka (one way replication) TiDB $\\\\mathrm { - } \\\\mathrm { > }$ TiCDC $\\\\displaystyle - >$ MySQL (one way replication) TiDB $\\\\mathrm { - - } >$ TiCDC $\\\\displaystyle - >$ TiDB (bi-directional) . Files written on the local filesystem or on the Amazon S3-compatible storage.\\n\\n![](images/8f12afa3c30a97df46d9eec091521bea2cc79126435cb4093e11bb925b10908c.jpg)\\n\\n# TiDB Backup/Restore\\n\\n# Backup and Restore\\n\\nBR (Backup and Restore)\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant mode   \\nSecurity   \\nNext steps   \\nBR is a command-line tool for fast distributed backup and   \\nrestore (1GB/s\\\\*)   \\nBR pushes down backup and restore tasks to each TiKV   \\ninstance for execution   \\nBackup/Restore from S3 or Google Storage   \\nIncremental Backup/Restore   \\nSupports PITR   \\nFlashback Table & Database\\n\\n![](images/c8205558e37c1e54567db0c99701942bd3dd2b48532b2b7c5ec9d1e48a02a917.jpg)\\n\\n# TiDB Backup/Migration tools\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant mode   \\nSecurity   \\nNext steps\\n\\nDumpling\\n\\nDumping data to SQL or CSV in parallel.   \\nCan also dump data from MySQL.\\n\\n# Lightning\\n\\nRestore a dump that was made with dumpling   \\n$\\\\bullet$ Can import CSV made by other tools.   \\n$\\\\bullet$ Can import directly to TiKV, bypassing the SQL layer.   \\n$\\\\bullet$ Can also import via SQL statements to TiDB. Main use case is to migrate data from MySQL\\n\\n# TiDB Backup /Migration tools\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant mode   \\nSecurity   \\nNext steps\\n\\nDM (Data Migration)\\n\\nSupports Point in time & Continous Realtime Replication Source Databases: MySQL compatible Databases Target: TiDB\\n\\n# TiDB Monitoring\\n\\n# Grafana + Prometheus\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant mode   \\nSecurity   \\nNext steps\\n\\nData from clusters pulled by Prometheus Performance metrics- servers,CPU, storage,query performance, etc.... Grafana displays data from Prometheus - Charts\\n\\n![](images/ca3dea67ec59462a6d460b02475a71b87a484da5fe65c2216b1a7b4cacf06bf8.jpg)\\n\\n![](images/6dd07e7543a2a16e745867046d666918df356c5a3cfc312b92d7718bad5a3bc9.jpg)\\n\\n![](images/d70cbb6ffdb01868dcac00a3e0f8af7a4f7f9a652075c495254ecfd453315fd0.jpg)\\n\\n# TiDB Dashboard\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant model   \\nSecurity   \\nNext steps Overview of the entire TiDB cluster   \\nLatency - QPS/TPS   \\nThe SQL statements causing database bottlenecks and query plans The slow queries exceeding execution time thresholds.   \\nThe node status   \\nMonitor and alert message\\n\\n![](images/8c2fd2ed28d85ae3eccd6a4ffc1513f2160948851e9378d0b749111d89633018.jpg)\\n\\n# Events monitored\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant mod   \\nSecurity   \\nNext steps   \\nNode/Cluster Up/Down   \\nMemory, Disk, CPU load/usage   \\nWaiting sessions   \\nSpace thresholds   \\nSlow queries   \\nRead/Write & Disk Latency\\n\\n<html><body><table><tr><td>Severity level</td><td>Description</td></tr><tr><td>Emergency-level</td><td>The highest severity level at which the service is unavailable. Emergency-level alerts are often caused by a service or node failure. Manual intervention is required immediately.</td></tr><tr><td>Critical-level</td><td>Decreased service availability.For the critical-level alerts,a close watch on the abnormal metrics is required.</td></tr><tr><td>Warning-level</td><td>Warning-level alertsarea reminder foran issue or error.</td></tr></table></body></html>\\n\\n# 3rd Party Monitoring Integrations\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant mode   \\nSecurity   \\nNext steps\\n\\nTiDB offers third-party integrations with a number of monitoring tools, including:\\n\\nDatadog   \\nGrafana   \\nPrometheus   \\nNew Relic   \\nDynatrace   \\nAppDynamics\\n\\nThese integrations allow you to send TiDB metrics to your preferred monitoring tool, so that you can monitor the performance and health of your TiDB cluster\\n\\n# TiDB Benchmarks\\n\\n# TPC Benchmark\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant model   \\nSecurity   \\nNext steps\\n\\nThese results demonstrate that TiDB is a high-performance database that can handle large and complex workloads.\\n\\nTPC-C: 0 1.8 million tps on a cluster of four nodes with 32 cores and 128 GB of RAM per node. 0 281.2 million tps on a cluster of 1,024 nodes with 32 cores and 128 GB of RAM per node.   \\nTPC-H: ○ 2.5 million queries per second (QPS) on a cluster of four nodes with 32 cores and 128 GB of RAM per node. 0 14.6 million QPS on a cluster of 1,024 nodes with 32 cores and 128 GB of RAM per node.   \\nTPC-DS: ○ 1.5 billion tps on a cluster of 1,024 nodes with 32 cores and 128 GB of RAM per node.\\n\\n# Real-world production example, at scale\\n\\n# Tweets from Xiaoguang Sun, Head of Infrastructure $@$ Zhihu.com\\n\\n![](images/3cf94b44349a0d32025bfedfb82d92c0d181e17eaa6e9b3251dad7e34ae76758.jpg)\\n\\nXiaoguang Sun @xgsun\\n\\nThis is perhaps the largest single TiDB cluster in the world:168TiKVinstances/21TiDB instances/1,820 billion rows of records/318TB of data/peak read100 million rows persecond/peakwrite 87,OoO rows per second.\\n\\nItwouldn\\'thavebeenpossibleifTiDBhadn\\'tbeen there\\n\\nTranslateTweet\\n\\n![](images/2715c8e606c960ae13b55d4d32348561c580e139b0418f9cf764109b0a197ab8.jpg)\\n\\nMaintained by two engineers\\n\\n# Customer Benchmark\\n\\n# TiDB Scales to 1 Million QPS with FlipKart\\n\\n# Customer Benchmark - Online eCommerce\\n\\nJD.com\\n\\n![](images/e781f30519aa857bc7c8a1d5263be44fb6073904f5818c4e82ccef632441e379.jpg)\\n\\nJD.COM\\n\\nLogistics platform handles over 1 million orders per day in TiDB Japanese e-commerce company\\n\\n![](images/b8abe953b59054f962d8c3beff5d7daa46b85594deeec3ebcbfe3bd5b9fd3de8.jpg)\\n\\n![](images/eae15b0702e5ffc0d6307d8d9a1db4b14946cc4c5501b70f09d7fb01d8a97440.jpg)\\n\\n$\\\\$ 1.5 B$ annual revenue   \\nTiDBScales 2M QPSwith 99TiDB $^ +$ 72   \\nTiKV\\n\\n# Multi-Tenant Model\\n\\n# Multi-Tenancy\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant model   \\nSecurity   \\nNext steps\\n\\nTiDB supports multi-tenancy in a number of ways:\\n\\nSchema isolation: TiDB allows you to create multiple schemas in a single database instance. This allows you to isolate the data and schema for each tenant.   \\nResource control: TiDB allows you to control the resources that each tenant can use.This can be useful for preventing one tenant from consuming too many resources and impacting the performance of other tenants.   \\nAuditing: TiDB provides auditing features that can be used to track the activity of each tenant. This can be useful for security and   \\ncompliance purposes.\\n\\n# TiDB Security\\n\\n# Security 1st\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant mode   \\nSecurity   \\nNext steps\\n\\nSupport provided locally:\\n\\nPingCAP provides local Technical Support per customer request via Jira/Zendesk system, a US company whose product is developed and operated by their US team. The data in Zendesk system is stored in the US.\\n\\nTransparency:\\n\\nThe TiDB source code is transparent to any customer, at any time, who needs to examine it to meet strict security compliance requirements. We publish TiDB security reports including bugs and vulnerabilities on Github. https://github.com/pingcap/tidb/security\\n\\n# Engineering:\\n\\nPingCAP Security Committee leads and reviews intensive internal security testing. All vulnerabilities are fixed in a prompt manner. Our main code repositories are scanned by internal self-developed tools and external third-party tools such as snyk.io and NcC Group\\n\\n# TiDB Compliance\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant model   \\nSecurity   \\nNext steps\\n\\n![](images/2c528b8bb0de9f0eb60f9cd3a95ab6a036c1c1c0769182cc890d22c963179f4c.jpg)\\n\\nPci DSS FE HIPAA COMPLIANT COMPLIANT\\n\\n# TiDB Security Functions\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant mode   \\nSecurity   \\nNext steps\\n\\nPasswordManagement PasswordPolices-Complexity, Expiration,PasswordStrength, Reuse/Tracking\\n\\non L\\n\\nEncryptionin Flight   \\nTiDBClient--TLS   \\nTiDBinter-node--TLS   \\nmTLS\\n\\nA Authentication LDAP Support\\n\\n![](images/78b7883b4d3642e6efb13a091efacd621aaf3e31ad1a753a31b308478cca0326.jpg)\\n\\nRBAC\\n\\nRole&PrivilegeBasedAccess Control\\n\\nZ22\\n\\nEncryptionat Rest AES128，AES192,AES256\\n\\n![](images/b3703d8e985aae6ce99b1a7b2851ab54f1e049867da02d889e74f473187fd3df.jpg)\\n\\nLoad Balacing ProxySQL,HAProxy,MySQLRouter\\n\\n![](images/9dd273a9b3d4189b18dde6de00196362933458cb43246b7ffcef306e4ac53373.jpg)\\n\\n# Audit\\n\\nTrackingsource&impactofdatabase operations to prevent illegal data theftor tampering\\n\\n# Next Steps\\n\\n# Nest Steps - TiDB PoC\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant mode   \\nSecurity   \\nNext steps\\n\\nNDA/Paper Process Follow On Technical Discussion\\n\\nO Topics   \\nO Identify MySQL Use Case New? Migration?\\n\\nTiDB PoC Planning Session\\n\\nO Requirement for success O Environement Sizing Excersize O Joint Execution Plan $0$ Resource Allocation\\n\\nTiDBPoC\\n\\nO Feature Functionality O Performance O Scale\\n\\nNext Steps\\n\\nQA\\n\\nThank you\\n\\n# The most advanced open source, distributed SQL database for modern applications.\\n\\nScalable. Versatile. Titanium (Ti) Reliable.\\n\\n□Square ERTIK Catalyst dailymotion NIANTIC 5 CAPCOM Bolt MI xiaomi S Shopee ninjavan H NETSCAN T Streak\\n\\n# Combine the power of both OLTP and OLAP\\n\\nSELECT AVG(s.price) FROM prod p, sales s WHERE p.pid $\\\\mathbf { \\\\tau } = \\\\mathbf { \\\\tau }$   \\n\\'B1328\\'   \\nGROUP BY s.price;\\n\\n![](images/564df550902aa23f428761d2b8aa5b248fb96f52535fbbed72de1e9027a18081.jpg)\\n\\n![](images/5397398ccedfa6b143a74738d150dae3e2cf8512992934d40041da83bfa6e1a3.jpg)\\n\\n# TiDB Security Functions\\n\\n![](images/3f840ebdd8198b3aed086be22de4f6618752f05aab5e75b167039ce8e8c08cd5.jpg)\\n\\n# Password Management\\n\\nPassword Polices -Complexity, Expiration, Password Strength, Reuse/Tracking\\n\\nAuthentication LDAP Support\\n\\nProxySQL,HA Proxy, MySQL Router\\n\\n![](images/bdbe4b3753ed4da535debf05dfdc0931964bd4d9a2fed2e893a032068f5f5343.jpg)\\n\\n电\\n\\nRBAC\\n\\nEncryption in Flight   \\nTiDB Client -- TLS   \\nTiDB inter-node -- TLS   \\nmTLS\\n\\nRole&Privilege Based Access Control\\n\\np\\n\\n# Audit\\n\\nTrackingsource&impactofdatabase operationsto prevent illegal datatheft or tampering\\n\\nEncryption at Rest AES128，AES192，AES256\\n\\n# Traditional Architecture HTAP ArchiteCture\"DB\\n\\n![](images/14798f34869f2591e1689381692c334a9e38f3123927d03a3a91c417c7e09b0a.jpg)\\n\\n# TPC Benchmark\\n\\nIntroduction   \\nArchitecture   \\nReplication   \\nBackup/Restore   \\nSecurity   \\nMonitoring   \\nBenchmarks   \\nMulti-tenant mode   \\nNext steps\\n\\nTiDB has performed well in a number of TPC benchmarks,including:\\n\\nTPC-C: TiDB achieved a new world record for TPC-C throughput in 2022, handling 281.2 million transactions per second (tps) on a cluster of 1,024 nodes.   \\nTPC-H: TiDB achieved a new world record for TPC-H   \\nprice-to-earnings query in 2022, completing the query in 0.12 seconds on a cluster of 1,024 nodes.   \\nTPC-DS: TiDB achieved a new world record for TPC-DS   \\nthroughput in 2023, handling 1.5 billion transactions per second (tps) on a cluster of 1,024 nodes.\\n\\n# TiCDC Bi-Directional Replication\\n\\nBi-directional replication clusters cannot detect write conflicts\\n\\nCascade Replication not supported TiDB A $- >$ TiDBB $\\\\phantom { 0 } \\\\mathbf { - } >$ TiDB C $\\\\phantom { 0 } \\\\mathbf { - } >$ TiDB A. In such a topology, if one cluster fails, the whole data replication will be affected.\\n\\nTherefore, to enable bi-directional replication among multiple clusters,you need to connect each cluster with every other clusters, for example,\\n\\nTiDB A <-> TiDB B,TiDB B <-> TiDB C,TiDB C <-> TiDB A\\n\\n# CDC Technical Requirements\\n\\nTo use TiCDC in disaster recovery scenarios, you need to configure redo log. TiCDC replication network latency between the clusters should not be higher than 100 ms\\n\\nTiCDC only replicates the table that has at least one valid index. Avalid index is defined as follows:\\n\\nA primary key(PRIMARY KEY) is a valid index. A unique index (UNIQUE INDEX)\\n\\nWhen you replicate a wide table with a large single row (greater than 1K),it is recommended that you configure per-table-memory-quota so that per-table-memory-quota $\\\\mathbf { \\\\tau } = \\\\mathbf { \\\\tau }$ ticdcTotalMemory/(tableCount \\\\* 2). ticdcTotalMemory is the memory of a TiCDC node, and tableCount is the number of target tables that a TiCDC node replicates.',\n",
       "  'source_link': 'docs/Apple/Copy of Apple TiDB Overview- Nabil/Louis FINAL 10102023/Copy of Apple TiDB Overview- Nabil/Louis FINAL 10102023_magic_pdf/Louis FINAL 10102023.md'},\n",
       " 'c0aa6372-0a15-471b-9ada-7f47499e5dd8': {'source_id': 'c0aa6372-0a15-471b-9ada-7f47499e5dd8',\n",
       "  'source_name': 'Apple Relevant Features',\n",
       "  'source_content': '# TiDB: Apple PoC Success Criteria Relevant Features\\n\\nNabil Nawaz Principal Solution Engineer\\n\\nLouis Fahrberger Account Executive\\n\\n# Agenda\\n\\nActive/Active Architecture Performance Multi-tenant Security Next steps\\n\\n# Safe Harbor Statement\\n\\nThis presentation has been prepared for general informational purposes only. All information contained in this presentation is provided in good faith, however we make no representation or warranty of any kind, express or implied, regarding the accuracy, adequacy， completeness ofany information.The information may not be incorporated into any contract. The development, release and timing of any features or functionality described for PingCAP products remains at the sole discretion of PingCAP.\\n\\n# TiDB Active/Active Clustering by Default\\n\\n# TiDB Active/Active Architecture\\n\\nActive/Active Architecture Performance Multi-tenant Security Next steps\\n\\n![](images/bab9070615d4325b0ff4578d57ae5be20ad1d905376608ccfb4a5560113c8664.jpg)\\n\\n# Performance\\n\\n# Performance at Scale\\n\\nActive/Active Architecture Performance Multi-tenant Security Next steps\\n\\nLegacy, monolithic databases are difficult and expensive to scale Modern, distributed databases effortlessly scale horizontally\\n\\n![](images/09b0d615a7eca0f5f79fd06174631a192347098ca653587619ea7af591bb9d6c.jpg)\\n\\n# Performance at Scale\\n\\nActive/Active Architecture Performance Multi-tenant Security Next steps\\n\\nJD.com eCommerce Platform: Achieved an 8x System Performance Boost “Why We Migrated from MySQL to TiDB\"\\n\\n\"TiDB helps scale out our databases and increases our large parcel sorting system\\'s performance by $8 \\\\times$ “\\n\\n\"We estimate that in the next two years it can reduce our IT costs of our logistics billing system by $67 \\\\%$ ”.\\n\\n# Performance at Scale\\n\\n# CustomerBenchmark\\n\\nActive/Active Architecture Performance Multi-tenant Security Next steps\\n\\nTiDB Scales to 1 Million QPS with FlipKart\\n\\n# Performance at Scale\\n\\n# TiDBPublic Benchmark Results\\n\\nActive/Active Architecture Performance Multi-tenant Security Next steps\\n\\nhttps://qithub.com/pingcap/docs/tree/master/benchmark\\n\\n# Performance at Scale\\n\\n# Linkedin TiDB Competitive Results\\n\\nActive/Active Architecture Performance Multi-tenant Security Next steps\\n\\nCockroachDB Yugabyte Vitess\\n\\nhttps://www.youtube.com/watch?v=DzZ-l8WL2jM\\n\\n# Multi-Tenant Model\\n\\n# Why Database Consolidation with TiDB?\\n\\nIdeal for Multi-tenant applications\\n\\nActive/Active Architecture Performance Multi-tenant Security Next steps\\n\\nConsolidate multiple MySQL instances across multiple applications into one or more distributed databases\\n\\nCost Reduction - Reduce overall TCO\\n\\nEase of Operations - Standardized platform\\n\\nTechnical Benefits\\n\\nIsolation Resource control Multi-tenancy\\n\\n# Multi-Tenancy\\n\\n# TiDB supports Multi-tenancy in a number of ways:\\n\\nActive/Active Architecture Performance Multi-tenant Security Next steps\\n\\nSchema isolation: TiDB allows you to create multiple schemas in a single database instance. This allows you to isolate the data and schema for each tenant securly.\\n\\nResource control: TiDB allows you to control the resources that each tenant can use. This can be useful for preventing one tenant from consuming too many resources and impacting the performance of other tenants.\\n\\nAuditing: TiDB provides auditing features that can be used to track the activity of each tenant. This can be useful for security and compliance purposes.\\n\\n# Resource Control\\n\\nActive/Active Architecture Performance Multi-tenant Security Next steps\\n\\nManage shared resources in the TiDB Cluster (CPU, IOPS,and IO bandwidth metrics)\\n\\nCritical applications always get necessary resources Non-Critical applications can have limits set\\n\\nCreate resource groups Set quotas for resource groups Assign users to those groups\\n\\n# TiDB Security\\n\\n# Security 1st\\n\\nActive/Active Architecture Performance Multi-tenant Security Next steps\\n\\nSupport provided locally:\\n\\nPingCAP provides local Technical Support per customer request via Jira/Zendesk system, a US company whose product is developed and operated by their US team. The data in Zendesk system is stored in the US.\\n\\nTransparency:\\n\\nThe TiDB source code is transparent to any customer, at any time, who needs to examine it to meet strict security compliance requirements. We publish TiDB security reports including bugs and vulnerabilities on Github. https://github.com/pingcap/tidb/security\\n\\n# Engineering:\\n\\nPingCAP Security Committee leads and reviews intensive internal security testing. All vulnerabilities are fixed in a prompt manner. Our main code repositories are scanned by internal self-developed tools and external third-party tools such as snyk.io and NcC Group\\n\\n# TiDB Compliance\\n\\nActive/Active Architecture Performance Multi-tenant Security Next steps\\n\\n![](images/8e58bd9b09efb32bbb8fc6e381dd44791af1367d1245fb7fa2f0bfb214b2c835.jpg)\\n\\nFEeE HIPAA Pci DSS COMPLIANT\\n\\n# TiDB Security Functions\\n\\nActive/Active Architecture Performance Multi-tenant Security Next steps\\n\\nPassword Management PasswordPolices-Complexity, Expiration,PasswordStrength, Reuse/Tracking\\n\\non L\\n\\nEncryptionin Flight   \\nTiDBClient--TLS   \\nTiDBinter-node--TLS   \\nmTLS\\n\\nA Authentication LDAP Support\\n\\n![](images/61291cd6705584eb1942af85eeffdfd30b7ed05589acd13e4e77e0029e724090.jpg)\\n\\nRBAC\\n\\nRole & Privilege Based Access Control\\n\\nZ22\\n\\nEncryptionat Rest AES128，AES192,AES256\\n\\n![](images/7398959e3807ffac13685035bedc15b73051ed88b024f11d32682abe809c53a6.jpg)\\n\\nLoad Balacing ProxySQL,HAProxy,MySQLRouter\\n\\n![](images/40a7bcd46c6249477b4b1560bbc0c0c1b652e59c822a043c0a24f9973771a744.jpg)\\n\\n# Audit\\n\\nTrackingsource&impactofdatabase operations to prevent illegal data theftor tampering\\n\\n# Next Steps\\n\\n# Next Steps - TiDB PoC\\n\\nActive/Active Architecture Performance Multi-tenant Security Next steps\\n\\nIdentify Use Case\\n\\nO New? O Migration?\\n\\nTiDB PoC Planning Session\\n\\nO Requirement for success O Environment Sizing Excersize O Joint Execution Plan\\n\\nTiDBPoC\\n\\nO Feature Functionality O Performance O Scale\\n\\nNext Steps\\n\\nQA\\n\\n# Appendix\\n\\n# TiDB Replication\\n\\n# CDC Diagram\\n\\n1. Multiple TiCDC processes pull data changes from TiKV nodes   \\n2. Data changes sorted and merged internally.   \\n3. Data changes are replicated to multiple downstream systems\\n\\n![](images/cfdb020122e79c4266fa276cb273120f17445b4dcf79848a3b70bcaa4efd31a7.jpg)\\n\\n# CDC Supported Scenarios\\n\\nSelf-Managed\\n\\nTiDB $\\\\mathrm { - } \\\\mathrm { > }$ TiCDC $\\\\displaystyle - >$ Kafka (one way replication) TiDB $\\\\mathrm { - } \\\\mathrm { > }$ TiCDC $\\\\displaystyle - >$ MySQL (one way replication) TiDB $\\\\mathrm { - - } >$ TiCDC $\\\\displaystyle - >$ TiDB (bi-directional) C Files written on the local filesystem or on the Amazon S3-compatible storage.\\n\\n![](images/df3347545dd1ea91264f23325c38c5bf8855272461269d1511eb5f0cf7856f57.jpg)\\n\\n# Active/Active Replication\\n\\nPerformance Replication Multi-tenant model Security Next steps\\n\\n![](images/7972137c339d3445df14d8ca27c3e34169dfa3aafdcd2d6b09e1ed7c57dcd4f7.jpg)\\n\\nTiCDC Bidirectional replication\\n\\nWrite conflicts are not detected, needs to be verified on application side   \\nNetwork lag should be under 100ms   \\nNeed to have Primary/Unique Keys on tables to be replicated   \\nSome DDL limitations   \\nhttps://docs.pingcap.com/tidb/stable/ticdc-bidirectional-replication#execute-ddl\\n\\nThank you\\n\\n# TiDB Architecture\\n\\n# Replication by Stripe Region w/ Raft\\n\\n![](images/4282a60b0b85acf1033d1aee89a9503f1efd3bf0d5c702667abbbd1a9c928c35.jpg)\\n\\n# Fun Fact:\\n\\n\"Raft\" was introduced to   \\nthe world in 2014 as an   \\n\"understandable\"   \\nspecification for   \\nconsensus and   \\nreplication through log   \\nshipping\\n\\nTiDB breaks table data into small stipes that we call Regions (default: 96MB) Each Region has a primary and two secondary copies\\n\\n![](images/3d4aec51232fa6d1a28421e02314609783c104333a787be68037310a2429290d.jpg)\\n\\n# Products that use the Raft or other basic log replication today:\\n\\nNon-distributed - MySQL, MariaDB, PostgreSQL, MongoDB Distributed - Google Spanner, TiDB, CockroachDB, YugabyteDB, Kafka\\n\\n# Data Distribution by Stripe Region w/ Raft\\n\\n![](images/1b699c82783b9e54fd8383be8b713efabc6a39d097eca4fe19d2867d37bc9109.jpg)\\n\\n# Fun Fact:\\n\\nCollectively, all stripe copies of a region are known as a Raft Group\\n\\nIndividual copies are Raft members\\n\\n![](images/bec9fedbeb6326f7bb6cc99a15182d9ce7d858ec62cd0eb8d270885224f6671a.jpg)\\n\\n# TiDB with Columnar Extension\\n\\n![](images/5431da713b424a968132a059ca6af3a9e43e4aebc0f976c66acfaae6e40c6d1b.jpg)\\n\\n# The most advanced, open source, distributed SQL database for modern applications.\\n\\nScalable. Versatile. Titanium (Ti) Reliable.\\n\\n□Square ERTIK Catalyst dailymotion NIANTIC 5 CAPCOM Bolt MI xiaomi S Shopee ninjavan H NETSCAN T Streak',\n",
       "  'source_link': 'https://docs.google.com/presentation/d/1HC91c9eh0-bMspRkUlRnt3gC0UfPwz9lmxNKgpOxZLk/edit?usp=drivesdk'},\n",
       " '7fe32fcd-2a5a-48ea-aec1-cb58a458e915': {'source_id': '7fe32fcd-2a5a-48ea-aec1-cb58a458e915',\n",
       "  'source_name': 'Apple POC Joint Execution Plan',\n",
       "  'source_content': '<html><body><table><tr><td>ID</td><td>Title</td><td>Creation Date</td><td>Owner</td><td>Status</td><td>notes</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>\\n\\n![](images/6bdd8a44bbe31ef910dc59be8e9a5091517e1de0e1923e3a512ab27fafcff5f2.jpg)\\n\\n![](images/35ff730c8815311a2988123de964a6643d6504f731189e869a6fd6ee25980a4a.jpg)\\n\\n<html><body><table><tr><td>Information required on existing platform</td><td></td><td>Customer Facing</td><td></td></tr><tr><td>Function</td><td>Important</td><td>Questions</td><td>Customer Answer</td></tr><tr><td>Data Sources</td><td></td><td>For Data Stores,whattypeofdataisexpected?Isitstructuredor structured + unstructured?</td><td></td></tr><tr><td>Data Sources</td><td></td><td>If unstructured data is expected, what is the nature of unstructured data?</td><td></td></tr><tr><td>Data Sources</td><td></td><td>Do these data stores have capability of pushing data to file formats (for Data Warehouse consumption)?File formats could be text, csv, json etc</td><td></td></tr><tr><td></td><td></td><td>How many users are expected to use this system? Power Users-create ad-hoc reports Mid-level Users-consume reports via active portals e.g. Bl Tools</td><td></td></tr><tr><td>Users</td><td></td><td>Basic Users-consume static output What are the indicative data volumes 0 0-6months?</td><td></td></tr><tr><td></td><td></td><td>7-12 months? 12-24 months? It is also OK to give an approximate daily data volume and a quarterly</td><td></td></tr><tr><td>Data Volume Data Volume</td><td>Y</td><td>growth rate tobe used for calculation Num of rows in biggest table?Avg size of row?</td><td></td></tr><tr><td></td><td></td><td>Are there any existing licenses for ETL tool? If so,which tool and what are the license terms?</td><td></td></tr><tr><td>Existing Tools (ETL)</td><td></td><td>If not,would you likeus to propose oneand do you have any preferences? Are there any existing licenses for any Bl/ Data Presentation tools?</td><td></td></tr><tr><td>Existing Tools (BI)</td><td></td><td>If so, which tool and what are the license terms? If not,would you likeus to propose oneand do you have any preferences?</td><td></td></tr><tr><td>Backup & Recovery</td><td></td><td>Does any external backup storage solution exists? Please provide details</td><td></td></tr><tr><td>Code</td><td></td><td>Are you currently using MySQL? If not,please provide details on your currentdatabase</td><td></td></tr><tr><td>DDLand Top Queries</td><td>Y</td><td>Can you provide your existing database DDLand provide you top 20 SQL queries?</td><td></td></tr><tr><td>Benchmarks</td><td>Y</td><td>Do you have any benchmarks youcan provide on your current environment?</td><td></td></tr><tr><td>Use Cases</td><td>Y</td><td>What arethe existing use cases on the existing data platform? Are there any new use cases which needs to be created? Please help to elaborate</td><td></td></tr><tr><td>Long termgoal &expectations</td><td></td><td></td><td></td></tr><tr><td>Business Objective</td><td></td><td>Questions</td><td></td></tr><tr><td>Value</td><td>Y</td><td>In your own words what is your ideal outcome of leveraging TiDB? What wil it mean to your teamand the business?</td><td></td></tr><tr><td>Implication</td><td>Y</td><td>Whathappensiftebusiness is notableto leverageanewDBlike TiDB?</td><td></td></tr><tr><td>Timeline</td><td>Y</td><td>What is the timeline of going live if the POC is successful? What happens if the timeline is not met?</td><td></td></tr><tr><td>Stakeholders</td><td>Y</td><td>Whoneedstobepartof thisPOCtomakesureeveryoneisonthesame page? Who needs to approve next steps post POC if successful?</td><td></td></tr><tr><td>Next Steps</td><td>Y</td><td>Will your teambe ready to go live and leverage oursupport if the POC meetsall successcriteris?</td><td></td></tr><tr><td>Technical Expectations fromnew</td><td></td><td></td><td></td></tr><tr><td>platform Function</td><td></td><td></td><td></td></tr><tr><td>Performance (Latency Requirements)</td><td>Y</td><td>Questions Queriesper Second (QPS) expectations?</td><td></td></tr><tr><td>Performance (Latency Requirements)</td><td>Y</td><td>Expected latencypertransaction?</td><td></td></tr><tr><td>Workload Type</td><td></td><td>Pointselect/batch update?</td><td></td></tr><tr><td>High Availability</td><td>Y</td><td>RPOand RTO?</td><td></td></tr><tr><td>Analytics</td><td></td><td>Is there any requirement on real-time analytics?</td><td></td></tr><tr><td></td><td></td><td>Is Disaster Recovery required within the first year of operations?</td><td></td></tr><tr><td>DR</td><td></td><td>If so,which type of DR is required?And can you also indicate what would be youracceptable RTO and RPO?</td><td></td></tr><tr><td>Real-Time</td><td></td><td>Isthereany \"Real-time\"data expected?</td><td></td></tr><tr><td>Real-Time</td><td></td><td>Ifreal-timedtsedsptatid data?If yes,which source will generate data streaming?</td><td></td></tr><tr><td></td><td></td><td>Is thereaplantohave separate environments for Development,I,UAT</td><td></td></tr><tr><td>Environments</td><td></td><td>& Production for thedataplatform?</td><td></td></tr></table></body></html>\\n\\n![](images/4485517d677bd35e146ac1b530df5166535c83ae498e68187ec8b6463888701f.jpg)\\n\\n<html><body><table><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td colspan=\"2\"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>App Name</td><td colspan=\"2\">CPU\\'s Ram</td><td>Used Storage Connection Counts Users Query\\'s/s Selects/s DML/s</td><td></td><td></td><td></td><td>Growth Storage (Annually)</td><td>Growth WorkloadDr Solution # of replicas</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td colspan=\"2\">426GB10GB</td><td></td><td></td><td>20</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>credit</td><td colspan=\"2\"></td><td>500</td><td>3</td><td>15</td><td>5</td><td>20%</td><td>20%Yes</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>async</td><td colspan=\"2\">1626GB2TB</td><td>2,000</td><td>20 1000</td><td>800</td><td>200</td><td>0%</td><td>50%No</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>events</td><td colspan=\"2\">426GB3.35TB</td><td>600</td><td>20</td><td>200</td><td>20 180</td><td>40%</td><td>10%No</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>customcheckout-master</td><td colspan=\"2\">32208GB14TB</td><td>5000</td><td>405k-20k</td><td></td><td>4k-18k200-4000</td><td>40%</td><td>120% Yes</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td colspan=\"2\"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>18000</td><td>4000</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>Ratio:</td><td>0.82</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>total</td><td>22000</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td colspan=\"2\"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td colspan=\"2\">Cluster Configuration Options</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>PROD-14TB2TB(Node Size)</td><td>4TB (Node Size)</td><td></td><td></td><td></td><td>PROD-2TB Nodes</td><td></td><td>yearly</td><td></td><td>PROD-4TB Nodes</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>2x Tidb-8c32G</td><td>2xTidb-8c32G</td><td>compute</td><td></td><td></td><td></td><td>monthly cluster</td><td>13,292.74</td><td>159,512.87</td><td></td><td></td><td>monthly</td><td>yearly</td></tr><tr><td></td><td></td><td>12 xTikv-8c 32G</td><td>6xTikv-8c32G</td><td>storage</td><td></td><td></td><td>* backup& transfers</td><td></td><td>705.60</td><td>8,467.16</td><td>* backup&transfers</td><td>cluster</td><td>9,379.65</td><td>112,555.77 6,119.31</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>enterprise support</td><td></td><td>2.799.67</td><td>33,596.01</td><td>enterprise support</td><td></td><td>509.94</td><td>23,735.02</td></tr><tr><td></td><td>POC-2TB</td><td>1TB(NodeSize)</td><td></td><td></td><td></td><td></td><td></td><td>Total</td><td>16,798.01</td><td>201,576.04</td><td></td><td>Total</td><td>1,977.92 11,867.51</td><td>142,410.09</td></tr><tr><td></td><td></td><td>2xTidb-8c 32G</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>3x Tikv-8c 32G</td><td></td><td></td><td></td><td></td><td>POC-1TBNodes</td><td>monthly</td><td>yearly</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>cluster</td><td>3.781.18</td><td>45,374.21</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>backup & transfers</td><td></td><td>230.02</td><td>2.760.23</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>enterprise support</td><td></td><td>802.24</td><td>9,626.89</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>Total</td><td>4,813.44</td><td>57,761.32</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>* Backup size may need to be adjusted this uses 2TB for calculation</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></table></body></html>\\n\\n<html><body><table><tr><td>#</td><td>UseCase</td><td>Importance</td><td>Need Test Plan (Y/N)</td><td>Status (Pass/Fail)</td><td>Verified By</td><td>Notes</td></tr><tr><td>1</td><td>Online DDL</td><td>2 - High</td><td>Y</td><td>Pass</td><td>Customer Name</td><td>Works great!</td></tr><tr><td>2</td><td>Backup/PITR</td><td>1 - Very High</td><td>Y</td><td>Pass</td><td>Customer Name</td><td>PITR PATR</td></tr></table></body></html>\\n\\n<html><body><table><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td colspan=\"3\">For each High Priority Success Criteria There should be a Test Plan</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SC #</td><td>Steps</td><td>Description</td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>do this first</td><td></td><td></td><td></td></tr><tr><td>1</td><td>2</td><td>then this</td><td></td><td></td><td></td></tr><tr><td>1</td><td>3</td><td>look for this</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>\\n\\n<html><body><table><tr><td>Query</td><td>Count</td><td></td><td></td><td></td><td></td></tr><tr><td>select *frommytable where item_id=?and type=？</td><td>99999</td><td></td><td></td><td></td><td></td></tr><tr><td>select *from users where id =？</td><td>8888</td><td></td><td>Period Observed: Jan 2022 - Nov 2022 (example)</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>',\n",
       "  'source_link': 'https://docs.google.com/spreadsheets/d/1SoWsfW5TyFzFt8AMSFgx8j8s_f3dgFu9YOsXBAv0cyI/edit?usp=drivesdk'},\n",
       " 'e695ddf0-07cc-4120-88c8-bd6829dca2d5': {'source_id': 'e695ddf0-07cc-4120-88c8-bd6829dca2d5',\n",
       "  'source_name': ' Technical Discovery Capture Form ',\n",
       "  'source_content': '# TECHNICAL DISCOVERY CAPTURE FORM\\n\\n# OvERVIEW:\\n\\nThis document helps us better understand your goals and objectives, use cases, performance requirements and timeline. Please feel free to add any additional questions or information that might help accelerate your evaluation progress. The information in the document will be used to help build an execution plan for the next step in the evaluation process.\\n\\n# BUSINESS /TECHNICAL CONTEXT:\\n\\n1.What is the line of business name and the application name for the first evaluation use case? a.   \\n2.What use cases are you considering TiDB Cloud as a fit for (ex: loT, e-commerce, SaaS platform, general OLTP, HTAP, etc. )? a.   \\n3. Does the current use case make use of database resident application logic (stored procedures,user defined functions, triggers)? a.   \\n4.What is the desired business outcome (increase availability， lower cost， better scalability, etc)? a.   \\n5. Who is the executive sponsor who will care most about identifying the right solution (please provide a name)? a.   \\n6. In your own words, what is your ideal outcome of leveraging TiDB? What willit mean to your team and the business? a.   \\n7.Name of the person responsible for making the technical decision? a.   \\n8. Is a PoC necessary to validate the success criteria (or will a demo / workshop also work)? Please elaborate. a.\\n\\n# EVALUATION PLAN:\\n\\n1. If the evaluation requires a PoC: a. Who willbe part of the PoC execution team (please provide names and roles\\n\\nof all necessary parties)?\\n\\nb. Who will define the PoC success criteria (please provide a name(s))?   \\nc. Who will approve the PoC outcomes against the success criteria (please provide a name)? ?   \\nd. Do you have an exception process for test cases that may not fully pass the success criteria? If yes,who will approve any necessary exceptions (please provide a name)?\\n\\n2. Do you already have a workload identified / defined for testing? If so, please specify a.\\n\\n3. Have you defined the test cases for the evaluation? If so, please describe.\\n\\na.   \\nb. If available: please provide a shared link to the test case documentation   \\nc. If success criteria is in a separate document please provide a link to this document at well\\n\\n4. If you have not defined test cases and success criteria, may we assist you with this process? a.\\n\\n5. Is this a competitive evaluation? If so，what other database platforms are you considering /evaluating? a.\\n\\n6. What is the duration of the evaluation you are expecting?\\n\\na.\\n\\n# DATA SERVICES ENVIRONMENT:\\n\\n1. Do you plan to run TiDB Cloud on AWS or GCP? Please specify the region that wil be used to deploy the cluster. a.   \\n2. Where will the application or user connect to the TiDB cluster/services from? a.   \\n3. Test data: a. How will the data be ingested (initial load)?   \\nb. What format will be used to stage the data prior to loading (csv, gzip, json, avro or other please specify)?   \\nc. Where will the data be staged (EBS, S3, GCP Cloud Storage, other)?   \\nd. How much data do you plan to run for the initial load to ingest into the cluster?   \\ne. How will incremental data be ingested?   \\nf. How much incremental data do you plan to stream into the cluster over the course of the evaluation? □\\n\\n# TEST WORKLOAD PROFILE:\\n\\n1. Does the test workload have max I/O throughput requirements (if yes，please specify)? a.\\n\\n2. What is the read:write ratio of the workload you are planning to test? a.\\n\\n3. What are the RTO and RPO requirements?\\n\\na. RTO: b. RPO:\\n\\n4. How many tables will the schema include?\\n\\na.\\n\\n5. What is the data volume of the biggest table?\\n\\na.\\n\\n6. And what is the average size of a single row? a.\\n\\n# TRANSACTIONAL WORKLOAD DETAILS:\\n\\n1. Are there semi-structured data types that will be tested (if yes, please specify? a.   \\n2. Does the test workload have max TPS/QPS requirements (if yes, please specify)? a.\\n\\n3. Does the test workload have max concurrency requirements (if yes, please specify)? a.\\n\\n4. Does the test workload have max latency requirements (if yes, please specify)? Read:\\n\\nP50: P90: P99: Write: P50: P90: P99:\\n\\n5. What is the workload type? Is it point select or batch update? Please provide some sample queries.\\n\\na.\\n\\n# ANALYTICAL WORKLOAD DETAILS (IF ANY):\\n\\n1. Can you provide analytical queries with mixed levels of complexity? a.\\n\\n2. How“fresh\" (close to realtime) does the data need to be for the analytical queries? a.\\n\\n3.What is the query origination source (ad-hoc，applications，Bl tools，or other sql tools, please specify)? a.\\n\\n# OTHERS:\\n\\nPlease specify other questions or considerations you would like to address during this PoC.',\n",
       "  'source_link': 'https://docs.google.com/document/d/1w30BZFSm8q5bpdsY-KKbcY7rRxlX_XIqSHpEyRCH7iA/edit?usp=drivesdk'},\n",
       " 'dc5dde77-495a-4b16-a57b-957a93d4660c': {'source_id': 'dc5dde77-495a-4b16-a57b-957a93d4660c',\n",
       "  'source_name': 'Copy of  TiDB 7.0.0 Testing Report',\n",
       "  'source_content': '# TiDB7.0.0 Testing Report\\n\\nProduct Name: TiDB Product Version: 7.0.0 Report Time : 2023-03-30\\n\\nSQL Test 3   \\nBasic Table Test 3   \\nQuery Test 3   \\nJoin opeator Test 4   \\nAGG Test 4   \\nOperator Test 4   \\nSQL Randgen Test 5   \\nPlugln Regression Test 5   \\nUTF Regression Test 5   \\nSQL Integration Test 7   \\nTransaction Test 7   \\nJepsen Test 7   \\nTP-Test Test 8   \\n$T X N + D D L$ Test 8   \\nTransaction Test 8   \\nOLTP Test 9   \\nOLTP Regression Test 9   \\nOLTP Integration Test 9   \\nHTAP Test 10   \\nHTAP Regression Test 10   \\nHTAP Integration Test 10   \\nDP System Test 11   \\nTiCDC System Test 12   \\nBinlog System Test 13   \\nDM System Test 14   \\nBRIE System Test 15   \\nHA Test 15   \\nUpgrade & Compatibility Test 17   \\nTiDB Upgrade Test 17   \\nPerformance Test 18   \\nSummary 18   \\nTest results 19   \\nMultiRocksdb 22   \\nKnown issues 22   \\nTest report 22   \\n7-day long stability test with MultiRocksDB Enabled 23   \\nLongrun System Test 23   \\nKernel on DBaas 23   \\nDbaas kernel Testing 23   \\nDBaaS Hot-upgrade Test - (version path) 24   \\nPerformance testing on DBaas 24   \\nARM Compatibility Test 25   \\nSecurity 26   \\nGolang 26   \\nConclusion 26\\n\\n# SQL Test\\n\\nSQL test which generates multiple tables and queries,covering the tests of index and query, operators and other related features,as well as the utf case of some sql features since 4.0.\\n\\nBasic Table Test   \\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>Full coverage of Primary key types with single column</td><td>PASS</td></tr><tr><td>Full coverage of Primary key types with multi column</td><td>PASS</td></tr><tr><td>Exist other index on Primary key</td><td>PASS</td></tr><tr><td>hash partition table</td><td>PASS</td></tr><tr><td>range partition table</td><td>PASS</td></tr><tr><td>range columns table</td><td>PASS</td></tr></table></body></html>\\n\\n# Query Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>Query with prepare</td><td></td></tr><tr><td>Normal Query</td><td></td></tr><tr><td></td><td>PASS</td></tr></table></body></html>\\n\\nJoin opeator Test   \\n\\n<html><body><table><tr><td>Projection</td><td>PASS</td></tr><tr><td>Union</td><td>PASS</td></tr><tr><td>WindowFunction</td><td>PASS</td></tr></table></body></html>\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>right join</td><td>PASS</td></tr><tr><td>left join</td><td>PASS</td></tr><tr><td>inner join</td><td>PASS</td></tr><tr><td>HashJoin</td><td>PASS</td></tr><tr><td>MergeJoin</td><td>PASS</td></tr><tr><td>IndexHashJoin</td><td>PASS</td></tr><tr><td>IndexMergeJoin</td><td>PASS</td></tr></table></body></html>\\n\\n# AGG Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>StreamAgg</td><td>PASS</td></tr><tr><td>HashAgg</td><td>PASS</td></tr></table></body></html>\\n\\n# Operator Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>TableScan</td><td>PASS</td></tr><tr><td>IndexScan</td><td>PASS</td></tr><tr><td>IndexMerge</td><td>PASS</td></tr><tr><td>IndexLookUp</td><td>PASS</td></tr><tr><td>PointGet</td><td>PASS</td></tr></table></body></html>\\n\\n<html><body><table><tr><td>BatchPointGet</td><td>PASS</td></tr><tr><td></td><td></td></tr></table></body></html>\\n\\n# SQL Randgen Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>Collation</td><td>PASS</td></tr><tr><td>prepared plan cache</td><td>PASS</td></tr><tr><td>copr_cache</td><td>PASS</td></tr><tr><td>partition pruning</td><td>PASS</td></tr></table></body></html>\\n\\n# Plugln Regression Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>deploy-plugin-tiup</td><td>PASS</td></tr><tr><td>deploy-plugin-operator</td><td>PASS</td></tr></table></body></html>\\n\\n# UTF Regression Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>utf-admin-show-ddl</td><td>PASS</td></tr><tr><td>utf-approx-percentile</td><td>PASS</td></tr><tr><td>utf-autorandom</td><td>PASS</td></tr><tr><td>utf-chunkrpc</td><td>PASS</td></tr><tr><td>utf-clustered-index</td><td>PASS</td></tr><tr><td>utf-collation</td><td>PASS</td></tr><tr><td>utf-collation2</td><td>PASS</td></tr><tr><td>utf-column-type-change</td><td>PASS</td></tr><tr><td>utf-coprcache</td><td>PASS</td></tr><tr><td>utf-cte-inline</td><td>PASS</td></tr><tr><td>utf-except-intersect</td><td>PASS</td></tr><tr><td>utf-expression-index</td><td>PASS</td></tr><tr><td>utf-flashback</td><td>PASS</td></tr></table></body></html>\\n\\n<html><body><table><tr><td>utt-gbk</td><td>PASS</td></tr><tr><td>utf-gcworker</td><td>PASS</td></tr><tr><td>utf-hint</td><td>PASS</td></tr><tr><td>utf-indexjoin-atuo</td><td>PASS</td></tr><tr><td>utf-indexmerge</td><td>PASS</td></tr><tr><td>utf-invisible-index</td><td>PASS</td></tr><tr><td>utf-join</td><td>PASS</td></tr><tr><td>utf-memory-quota</td><td>PASS</td></tr><tr><td>utf-partition</td><td>PASS</td></tr><tr><td>utf-plancache</td><td>PASS</td></tr><tr><td>utf-ratelimit</td><td>PASS</td></tr><tr><td>utf-rbac</td><td>PASS</td></tr><tr><td>utf-rowformat</td><td>PASS</td></tr><tr><td>utf-sequence</td><td>PASS</td></tr><tr><td>utf-spm</td><td>PASS</td></tr><tr><td>utf-statements</td><td>PASS</td></tr><tr><td>utf-table-partition</td><td>PASS</td></tr><tr><td>utf-temporary-table</td><td>PASS</td></tr><tr><td>utf-unionscan</td><td>PASS</td></tr><tr><td>utf-view</td><td>PASS</td></tr><tr><td>topsql</td><td>PASS</td></tr><tr><td>tidb-mysql-client</td><td>PASS</td></tr><tr><td>placement-in-sql</td><td>PASS</td></tr><tr><td>utf-once</td><td>PASS</td></tr><tr><td>utf-regression</td><td>PASS</td></tr><tr><td>utf-kill-auto-analyze</td><td>PASS</td></tr><tr><td>utf-only-full-group-by</td><td>PASS</td></tr><tr><td>utf-fk-auto</td><td>PASS</td></tr><tr><td>dm-upgrade-with-fk</td><td>PASS</td></tr><tr><td>multi-valued-index</td><td>PASS</td></tr><tr><td>flashback-ha-test-fault-inject</td><td>PASS</td></tr><tr><td>flashback-ha-test-2tidb-3pd-3tikv-sysbench-random</td><td>PASS</td></tr><tr><td>flashback-ha-test-2tidb-3pd-3tikv-tpcc-random</td><td>PASS</td></tr><tr><td>flashback-ha-test-with-tiflash-self-run-tpcc-random</td><td>PASS</td></tr></table></body></html>\\n\\n<html><body><table><tr><td>flashback-with-pitr</td><td>PASS</td></tr><tr><td>utf-flash-back-auto</td><td>PASS</td></tr><tr><td>add-index-ingest</td><td>PASS</td></tr><tr><td>utf-json-tiflash-auto</td><td>PASS</td></tr><tr><td>sm3-auth</td><td>PASS</td></tr><tr><td>utf-multi-schema-change-auto</td><td>PASS</td></tr><tr><td>utf-named-lock-auto</td><td>PASS</td></tr><tr><td>utf-optimize-count-distinct-auto</td><td>PASS</td></tr><tr><td>metadata-lock</td><td>PASS</td></tr><tr><td>utf-async-commit</td><td>PASS</td></tr><tr><td>tidb-ttl-common</td><td>PASS</td></tr><tr><td>tidb-ttl-cdc-sync-mysql</td><td>PASS</td></tr><tr><td>tidb-ttl-br-and-lightning</td><td>PASS</td></tr><tr><td>tidb-ttl-cdc-sync-tidb</td><td>PASS</td></tr><tr><td>utf-reorganize-partition-compatibility</td><td>PASS</td></tr><tr><td>utf-reorganize-partition-ha</td><td>PASS</td></tr></table></body></html>\\n\\n# SQL Integration Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>tidb_ghpr_common_test</td><td>PASS</td></tr><tr><td>tidb_ghpr_integration_common_test</td><td>PASS</td></tr><tr><td>tidb_ghpr_sqllogic_test</td><td>PASS</td></tr><tr><td>tidb_ghpr_integration_ddl_test</td><td>PASS</td></tr></table></body></html>\\n\\n# Transaction Test\\n\\nThis test includes random correctness tests based on randgen, correctness tests based on concurrent transactions,and random tests including transaction fault injection, al of these test passed.\\n\\nJepsen Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>bank</td><td>PASS</td></tr><tr><td>bank-multitable</td><td>PASS</td></tr><tr><td>set</td><td>PASS</td></tr><tr><td>set-cas</td><td>PASS</td></tr><tr><td>append</td><td>PASS</td></tr><tr><td>register</td><td>PASS</td></tr><tr><td>comments</td><td>PASS</td></tr><tr><td>long-fork</td><td>PASS</td></tr><tr><td>txn-cycle</td><td>PASS</td></tr><tr><td>monotonic</td><td>PASS</td></tr><tr><td>sequential</td><td>PASS</td></tr></table></body></html>\\n\\n# TP-Test Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>clustered vs nonclustered</td><td>PASS</td></tr><tr><td>default config, tidb vs mysql</td><td>PASS</td></tr><tr><td>new collation enabled</td><td>PASS</td></tr></table></body></html>\\n\\n# TXN $^ +$ DDL Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>txn-with-ddl</td><td>PASS</td></tr><tr><td>stmtflow/ddl</td><td>PASS</td></tr></table></body></html>\\n\\n# Transaction Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>UTF Transaction</td><td>PASS</td></tr></table></body></html>\\n\\n# OLTP Test\\n\\nThis test includes TiKV& PD integration test, system acceptance case and new feature system test.\\n\\n# OLTP Regression Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>tikv_ghpr_integration_common_test</td><td>PASS</td></tr><tr><td>tikv_ghpr_integration-copr-test</td><td>PASS</td></tr><tr><td>tikv_ghpr_integration_compatibility_test</td><td>PASS</td></tr><tr><td>tikv_ghpr_integration_ddl_test</td><td>PASS</td></tr><tr><td>tikv_ghpr_integration_br_test</td><td>PASS</td></tr></table></body></html>\\n\\n# OLTP Integration Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>oltp-diskfull-stability-fun</td><td>PASS</td></tr><tr><td>oltp-pd-tso-scalibility</td><td>PASS</td></tr><tr><td>oltp-scale-stability-all</td><td>PASS</td></tr><tr><td>oltp-stale-read-and-hot-region-history</td><td>PASS</td></tr><tr><td>oltp-stale-read-fun.daily</td><td>PASS</td></tr><tr><td>oltp-stale-read-fun.upgrade</td><td>PASS</td></tr><tr><td>oltp-tiup-tls.daily</td><td>PASS</td></tr><tr><td>oltp-tpcc-large-fun</td><td>PASS</td></tr><tr><td>oltp-upgrade</td><td>PASS</td></tr><tr><td>pd-regression</td><td>PASS</td></tr><tr><td>oltp-sysbench-oltp-write-only-stability</td><td>PASS</td></tr><tr><td>oltp-tpcc-stability.large</td><td>PASS</td></tr><tr><td>oltp-4core-quota-limiter-fun-001</td><td>PASS</td></tr></table></body></html>\\n\\n<html><body><table><tr><td>oltp-data-recovery</td><td>PASS</td></tr><tr><td>oltp-load-base-split</td><td>PASS</td></tr><tr><td>oltp-range-partition-sysbench-oltp-read-write</td><td>PASS</td></tr><tr><td>oltp-sysbench-oltp-random-workload-fun</td><td>PASS</td></tr><tr><td>oltp-sysbench-oltp-random-workload-fun2</td><td>PASS</td></tr><tr><td>oltp-titan-tpcc</td><td>PASS</td></tr><tr><td>oltp-ycsb-all</td><td>PASS</td></tr></table></body></html>\\n\\n# HTAP Test\\n\\nThis test include HTAP Operation and maintenance test, limit test, fault injection test, MPP integration test and oncall test and regression test case.\\n\\n# HTAP Regression Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>tiflash-ghpr-integration-tests</td><td>PASS</td></tr><tr><td>tiflash_regression_test_daily</td><td>PASS</td></tr></table></body></html>\\n\\n# HTAP Integration Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>tiflash-compaction</td><td>PASS</td></tr><tr><td>tiflash-ddl</td><td>PASS</td></tr><tr><td>tiflash-encryption</td><td>PASS</td></tr><tr><td>tiflash-fast-mode</td><td>PASS</td></tr><tr><td>tiflash-gc</td><td>PASS</td></tr><tr><td>tiflash-misc</td><td>PASS</td></tr><tr><td>tiflash-mutiple-disks</td><td>PASS</td></tr><tr><td>tiflash-storage-version</td><td>PASS</td></tr><tr><td>tiflash-tools-compatibility</td><td>PASS</td></tr><tr><td>tiflash-window-func-version</td><td>PASS</td></tr></table></body></html>\\n\\n<html><body><table><tr><td>tiflash-window-func</td><td>PASS</td></tr><tr><td>auto-reload-cert</td><td>PASS</td></tr><tr><td>bigdata-consistency</td><td>PASS</td></tr><tr><td>bigdata-consistency3</td><td>PASS</td></tr><tr><td>bigdata-consistency2</td><td>PASS</td></tr><tr><td>ch-tls.release</td><td>PASS</td></tr><tr><td>ch.daily</td><td>PASS</td></tr><tr><td>ch_apply_snapshot</td><td>PASS</td></tr><tr><td>ch_big_region_apply_snapshot</td><td>PASS</td></tr><tr><td>dt-performance</td><td>PASS</td></tr><tr><td>feature-matrix-stability</td><td>PASS</td></tr><tr><td>high-currency</td><td>PASS</td></tr><tr><td>memory-tracker-accuracy</td><td>PASS</td></tr><tr><td>memory-tracker</td><td>PASS</td></tr><tr><td>mpp-kill</td><td>PASS</td></tr><tr><td>mpp-randgen</td><td>PASS</td></tr><tr><td>pagestorage-performance</td><td>PASS</td></tr><tr><td>partition-high-currency</td><td>PASS</td></tr><tr><td>partition360</td><td>PASS</td></tr><tr><td>random-failpoint</td><td>PASS</td></tr><tr><td>stale-read</td><td>PASS</td></tr><tr><td>upgrade</td><td>PASS</td></tr></table></body></html>\\n\\n# DP System Test\\n\\nThis test include ticdc/br/lightning/dumpling/binlog system acceptance test, upgrade test, compatibility test, important scenarios test and regression test.DP Regression Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>br_ghpr_integration_test</td><td>PASS</td></tr><tr><td>binlog_ghpr_integration</td><td>PASS</td></tr></table></body></html>\\n\\n<html><body><table><tr><td>tools_ghpr_integration</td><td>PASS</td></tr><tr><td>cdc_ghpr_integration_test</td><td>PASS</td></tr><tr><td>cdc_ghpr_kafka_integration_test</td><td>PASS</td></tr><tr><td>dm_ghpr_integration_test</td><td>PASS</td></tr><tr><td>dm_ghpr_compatibility_test</td><td>PASS</td></tr></table></body></html>\\n\\n# TiCDC System Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>cdc_kafka_basic_functionality</td><td>PASS</td></tr><tr><td>cdc_kafka_scale_big_table</td><td>PASS</td></tr><tr><td>cdc_kafka_scale_big_table_longrun</td><td>PASS</td></tr><tr><td>cdc_kafka_scale_big_table_ops</td><td>PASS</td></tr><tr><td>cdc_lightning_comp_kafka</td><td>PASS</td></tr><tr><td>cdc_upgrade_test_v6.1.5-v7.0.0-pre</td><td>PASS</td></tr><tr><td>cdc_upgrade_test_v6.5.1-v7.0.0-pre</td><td>PASS</td></tr><tr><td>cdc_upgrade_test_v6.6.0-v7.0.0-pre</td><td>PASS</td></tr><tr><td>cdc-br-lightning-tidb-sync</td><td>PASS</td></tr><tr><td>cdc-failure-chaos-plan</td><td>PASS</td></tr><tr><td>cdc-flashback-plan</td><td>PASS</td></tr><tr><td>cdc-kafka-broker-failure</td><td>PASS</td></tr><tr><td>cdc-kafka-controller-failure</td><td>PASS</td></tr><tr><td>cdc-kafka-mysql-sync</td><td>PASS</td></tr><tr><td>cdc-kafka-mysql-sync-gcttl</td><td>PASS</td></tr><tr><td>cdc-max-incremental-single-table-sync</td><td>PASS</td></tr><tr><td>cdc-minor-components-unavaible</td><td>PASS</td></tr><tr><td>cdc-network-chaos-plan</td><td>PASS</td></tr><tr><td>cdc-new-collation-sync</td><td>PASS</td></tr></table></body></html>\\n\\n<html><body><table><tr><td>cdc-node-restart-numerous-changefeed-tidb-sync</td><td>PASS</td></tr><tr><td>cdc-only-upgrade-sync</td><td>PASS</td></tr><tr><td>cdc-pd-leader-switch-sync</td><td>PASS</td></tr><tr><td>cdc-qiho0-360-sync</td><td>PASS</td></tr><tr><td>cdc-redo-bench</td><td>PASS</td></tr><tr><td>cdc-redo-long-duration</td><td>PASS</td></tr><tr><td>cdc-redo-related</td><td>PASS</td></tr><tr><td>cdc-redo-syncpoint</td><td>PASS</td></tr><tr><td>cdc-scale-sync</td><td>PASS</td></tr><tr><td>cdc-scheduler-sync</td><td>PASS</td></tr><tr><td>cdc-storage-sink</td><td>PASS (storage sink consumer with issue Issue #8666)</td></tr><tr><td>cdc-tikv-chaos-sync</td><td>PASS</td></tr><tr><td>cdc-tikv-scale-sync</td><td>PASS</td></tr><tr><td>cdc-tiup-cluster</td><td>PASS</td></tr><tr><td>cdc-tiup-longtime</td><td>PASS</td></tr><tr><td>cdc-tiup-plan</td><td>PASS</td></tr><tr><td>cdc-truncate-table-sync</td><td>PASS</td></tr><tr><td>cdc-truncate-table-tidb-sync</td><td>PASS</td></tr><tr><td>cdc-upstream-chaos-kafka-sync</td><td>PASS</td></tr></table></body></html>\\n\\n# Binlog System Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>binlog-tiup-scale-out</td><td>PASS</td></tr><tr><td>binlog-tiup-scale-in</td><td>PASS</td></tr></table></body></html>\\n\\n<html><body><table><tr><td>binlog-tiup-deploy</td><td>PASS</td></tr><tr><td>binlog-kafka</td><td>PASS</td></tr><tr><td>binlog-file-basic</td><td>PASS</td></tr><tr><td>binlog-acceptance-test</td><td>PASS</td></tr><tr><td>binlog-long-duration</td><td>PASS</td></tr><tr><td>binlog-tiup-upgrade</td><td>PASS</td></tr><tr><td>binlog-tiup-long-duration</td><td>PASS</td></tr></table></body></html>\\n\\n# DM System Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>dm-downstream-version-compatibility</td><td>PASS</td></tr><tr><td>dm-sharding-all</td><td>PASS</td></tr><tr><td>dm-upgrade-with-fk</td><td>PASS</td></tr><tr><td>dm_lighting_physical_big</td><td>PASS</td></tr><tr><td>dm-migrate-incremental-ddl</td><td>PASS</td></tr><tr><td>dm-long-duration</td><td>PASS</td></tr><tr><td>dm-migrate-mysql</td><td>PASS</td></tr><tr><td>dm-tiup-upgrade</td><td>PASS</td></tr><tr><td>dm-tiup-long-duration</td><td>PASS</td></tr><tr><td>dm-tiup-testing</td><td>PASS</td></tr><tr><td>dm_upgrade_operator_v5.4.3-v7.0.0-pre</td><td>PASS</td></tr><tr><td>dm_upgrade_operator_v6.5.1-v7.0.0-pre</td><td>PASS</td></tr><tr><td>dm_upgrade_operator_v6.6.0-v7.0.0-pre</td><td>PASS</td></tr><tr><td>dm_upgrade_tiup_v5.4.3-v7.0.0</td><td>PASS</td></tr><tr><td>dm_upgrade_tiup_v6.5.1-v7.0.0</td><td>PASS</td></tr><tr><td>dm_upgrade_tiup_v6.6.0-v7.0.0</td><td>PASS</td></tr></table></body></html>\\n\\n# BRIE System Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>brie-acceptance</td><td>PASS</td></tr><tr><td>brie-acceptance-long-time</td><td>PASS</td></tr><tr><td>brie-acceptance-robust</td><td>PASS</td></tr><tr><td>brie-acceptance-pitr</td><td>PASS</td></tr><tr><td>brie-acceptance-pitr-robust</td><td>PASS</td></tr><tr><td>brie-acceptance-pitr-long-time</td><td>PASS</td></tr><tr><td>brie-tiup-acceptance</td><td>PASS</td></tr></table></body></html>\\n\\n# HA Test\\n\\nWorkload:TPCC、CHDPtest:PiTR、TICDC\\n\\n<html><body><table><tr><td>用例ID</td><td>Test Items</td><td>Result</td></tr><tr><td>ha_tidb(random)_restart</td><td>Simulate tidb restart</td><td>PASS</td></tr><tr><td>ha_all_comp_kill</td><td>Simulate all comp kill</td><td>PASS</td></tr><tr><td>ha_pdleader_data_io_ha ng</td><td>Simulate pd leader io hang</td><td>PASS</td></tr><tr><td>ha_pdleader_io_delay(10 ms)</td><td>Simulate pd leader io delay</td><td>PASS</td></tr><tr><td>ha_tikv(random)_data_io _hang</td><td> Simulate tikv io hang</td><td>PASS</td></tr><tr><td>ha_tikv(random)_io_dela y(10ms)</td><td>Simulate tikv io delay</td><td>PASS</td></tr><tr><td>ha_az(random)_failure</td><td>Simulate one az failure</td><td>PASS</td></tr></table></body></html>\\n\\n<html><body><table><tr><td>ha_az(random)_to_az(all )_network_latency(50ms</td><td> Simulate one az network delay</td><td>PASS</td></tr><tr><td>ha_az(random)_to_az(all )_network_partition</td><td> Simulate one az network partition</td><td>PASS</td></tr><tr><td>ition</td><td>ha_two_az_network_part Simulate network partitio between two az</td><td>PASS</td></tr><tr><td>ha_tidb(random)_kill</td><td>Simulate one tidb kill</td><td>PASS</td></tr><tr><td>ha_tidb(random)_to_all_ network_partition</td><td>Simulate one tidb network partition</td><td>PASS</td></tr><tr><td>ha_tikv(random)_kill</td><td>Simulate one tikv kill</td><td>PASS</td></tr><tr><td>ha_tikv(random)_to_tidb (random)_network_partit and one tidb ion</td><td>Simulate network partitio between one tikv</td><td>PASS</td></tr><tr><td>ha_tikv(random)_to_pdle ader_network_partition</td><td>Simulate network partitio between one tikv and one pd leadr</td><td>PASS</td></tr><tr><td>ha_tikv(random)_failure_ 10m</td><td>Simulate one tikv failure for 10m</td><td>PASS</td></tr><tr><td>ha_tikv(random)_to_all_ network_latency(50ms)</td><td>Simulate one tikv network latency</td><td>PASS</td></tr><tr><td>ha_tikv(random)_to_tikv( all)_network_partition_1 0m</td><td>Simulate one tikv network partition to other tikv</td><td>tikv/tikv/issues/13758</td></tr><tr><td>ha_tikv(random)_to_all_ network_partition_50m</td><td>Simulate one tikv network partition to other pods</td><td>tikv/tikv/issues/12259</td></tr><tr><td>ha_pdleader_to_tidb(ran dom)_network_partition _10</td><td>Simulate network partitio between pd leader and one tidb</td><td>PASS</td></tr></table></body></html>\\n\\n<html><body><table><tr><td>ha_pdleader_time_offset (+5m)</td><td> Simulate pd leader time_offset(+5m)</td><td>tidb/issues/38248</td></tr><tr><td>ha_pdleader_time_offset (+1s)</td><td> Simulate pd leader time_offset(+1s)</td><td>PASS</td></tr><tr><td>ha_pdleader_time_offset (-1s)</td><td> Simulate pd leader time_offset(-1s)</td><td>PASS</td></tr><tr><td>ha_pdfollower(random)_ restart</td><td>Simulate one pd follower restart</td><td>PASS</td></tr><tr><td>ha_pdleader_kill</td><td>Simulate pd leader kill</td><td>PASS</td></tr><tr><td>ha_pdleader_to_all_netw ork_latency(50ms)</td><td> Simulate pd leader network latency</td><td>PASS</td></tr><tr><td>ha_pdleader_to_all_netw ork_partition</td><td>Simulate pd leader network partition</td><td>tikv/tikv/issues/14241</td></tr><tr><td>ha-tiflash-random-to-all- network-partition-last-fo r-10m</td><td>Simulate one tiflash network partition</td><td>PASS</td></tr><tr><td>ha-tiflash-random-restar t</td><td>Simulate one tiflash restart</td><td>PASS</td></tr><tr><td>ha-tiflash-random-failure -50m</td><td>Simulate one tiflash failure for 50m</td><td>PASS</td></tr><tr><td>ha-tiflash-random-io-del ay-10ms-last-for-10m</td><td>Simulate one tiflash io delay</td><td>PASS</td></tr><tr><td>ha-tiflash-random-data-i o-hang-last-for-20m</td><td>Simulate one tiflash io hang</td><td>PASS</td></tr></table></body></html>\\n\\n# Upgrade & Compatibility Test\\n\\nTiDB Upgrade Test\\n\\nUpgrade path include from v5.0.6,v5.1.5, v5.2.4,v5.3.4,v5.4.3,v6.1.5, v6.5.1, v6.6.0 to v7.0.0\\n\\nUpgrade Tools TiUP，Operator\\n\\nWorkload BANK   \\nClusterr Topology 3\\\\*PD, $2 ^ { \\\\star }$ TiDB, $4 ^ { \\\\star }$ TiKV, $2 ^ { \\\\star }$ TiFlash，1\\\\*TiCDC   \\nConfig default(Operator), self-define(TiUP,link)   \\nSource Version v5.0.6,v5.1.5,v5.2.4,v5.3.4,v5.4.3,v6.1.5,v6.5.1,v6.6.0   \\nResult PASS\\n\\nConfig and System Variable Change List change list: link Compatibility Change List: link\\n\\n# Performance Test\\n\\n# Summary\\n\\nCompared with version 6.6.0, the performance testing results of version 7.0.0 are as follows:\\n\\n1.The performance in OLTP/OLAP/HTAP scenarios remains basically the same.   \\n2.The performance in TiCDC/BR/Lightning/Dumpling scenarios remains basically the same.   \\n3．The performance in the Ossinsight - DDL Add index scenario has been improved by $1 1 \\\\%$ due to adjustments made in pull/42326.   \\n4． The performance in the Ossinsight - Analyze scenario has been improved by optimizing the analyze performance for large tables (tikv task part), resulting in a decrease in execution time from 851s to 509s,a reduction of $40 \\\\%$ ：   \\n5. As for metrics, TiKV IO throughput and IOPS have been greatly optimized due to the default enablement of log_recycling, resulting in a decrease of $3 . 4 8 \\\\%$ to $3 1 . 3 4 \\\\%$   \\n6. MultiRocksDB Feature Testing Summary a. Summary i. MultiRocksDB shows great performance improvement for pure insert workload. b．6 hours stress testing i. For sysbench oltp_insert, there is $133 \\\\%$ QPS improvement. ii. For tpcc,there is $6 . 5 \\\\%$ decrease in QPS c. tpcc 7 days stability testing i. QPS decreases much faster and performance is unstable compared to baseline #14460 d.There are 5 critical issues and 3 major issues opened from Performance Testing.\\n\\ni. Disk space usage from raft-engine could increase from \\\\~20GB to over 100GB.\\n\\ne.Functionality and compatibility i. Restart TiKV is Slow #14481 14472 ii. Raft-engine Prefill does not work #14468 i. dynamically change the value raftstore.store-io-pool-size does not work #14485\\n\\nTest results\\n\\nHardware configurations\\n\\nOLTP: 3TiDB $^ +$ 3TiKV $^ +$ 1 PD: each with 16c 48g   \\nOLAP TiKV: 1TiDE $3 + 3 \\\\intercal$ iKV $^ +$ 1 PD: TiDB: 16c 80g TiKV 16c 48g   \\nOLAP TiFlash: 1TiDB $^ +$ 3TiKV $^ +$ 1/2TiFlash $^ +$ 1 PD: TiDB/TiKV each with 16c 48g,   \\nTiFlash 16c 48g   \\nHTAP TiFlash: 3TiDB $^ +$ 3TiKV $^ +$ 2TiFlash $^ +$ 1 PD: TiDB/TiKV each with 16c 48g,   \\nTiFlash 16c 64g\\n\\nDetail performance test report:$\\\\mathsf { v } 1 = \\\\mathsf { v } 6 . 6 . 0$ ， $\\\\mathsf { v } 2 \\\\mathsf { = v } 7 . 0 . 0$\\n\\n# OLTP\\n\\n<html><body><table><tr><td>benchmark_id ~ OLTP</td><td>6070000-idc-v6.6.0-v7.0.0 </td><td></td><td>version1</td><td>6.6.0< version2</td><td>7.0.0</td><td colspan=\"5\"></td></tr><tr><td colspan=\"11\"></td></tr><tr><td colspan=\"11\"></td></tr><tr><td></td><td>#bench_type </td><td> bench_sub_type </td><td></td><td> thread </td><td> v1_qps </td><td>v2_qps </td><td></td><td>v1_avg_latency (ms) </td><td>v2_avg_latency (ms) </td><td>qps_diff(%) </td></tr><tr><td></td><td>1bank</td><td>bank-50m</td><td></td><td>50</td><td></td><td>56734</td><td>55374</td><td>0.799</td><td>0.816</td><td>-2.40</td></tr><tr><td></td><td>2bank</td><td>bank-50m</td><td></td><td>200</td><td></td><td>83414</td><td>83762</td><td>2.27</td><td>2.25</td><td>0.42</td></tr><tr><td></td><td>3 benchmarksql</td><td>benchmarksql-1k</td><td></td><td></td><td>200</td><td>65022</td><td>63933</td><td>2.95</td><td>2.96</td><td>-1.68</td></tr><tr><td></td><td>4 booking</td><td>booking</td><td></td><td></td><td>2</td><td>16339</td><td>16586</td><td>2.35</td><td>2.28</td><td>1.51</td></tr><tr><td></td><td>5 corebanking</td><td></td><td>hzbank_simulation</td><td></td><td>5</td><td>37093</td><td>37051</td><td>2.00</td><td>1.86</td><td>-0.11</td></tr><tr><td></td><td>6 hzbank</td><td>hzbank_poc</td><td></td><td></td><td>5</td><td>78453</td><td>78979</td><td>0.797</td><td>0.796</td><td>0.67</td></tr><tr><td></td><td>7 shenma</td><td></td><td>shenma-2x-speed-5-threads</td><td></td><td>5</td><td>16197</td><td>16204</td><td>0.649</td><td>0.556</td><td>0.04</td></tr><tr><td></td><td>8 sysbench</td><td>oltp_insert</td><td></td><td></td><td>100</td><td>34227</td><td>35123</td><td>2.80</td><td>2.72</td><td>2.62</td></tr><tr><td></td><td>9sysbench</td><td>oltp_point_select</td><td></td><td></td><td>100</td><td>238926</td><td>239941</td><td>0.343</td><td>0.343</td><td>0.42</td></tr><tr><td></td><td>10sysbench</td><td>oltp_read_write</td><td></td><td></td><td>100</td><td>82100</td><td>81912</td><td>1.08</td><td>1.08</td><td>-0.23</td></tr><tr><td></td><td>11sysbench</td><td></td><td>oltp_update_index</td><td></td><td>100</td><td>21174</td><td>21449</td><td>4.56</td><td>4.52</td><td>1.30</td></tr><tr><td></td><td>12 sysbench</td><td></td><td>oltp_update_non_index</td><td></td><td>100</td><td>43367</td><td>44964</td><td>2.08</td><td>2.12</td><td>3.68</td></tr><tr><td></td><td>13tpcc</td><td>tpcc1k</td><td></td><td></td><td>400</td><td>78293</td><td>77563</td><td>4.97</td><td>5.00</td><td>-0.93</td></tr><tr><td></td><td>14ycsb</td><td>workloada</td><td></td><td></td><td>100</td><td>37823</td><td>38155</td><td>2.55</td><td>2.53</td><td>0.88</td></tr><tr><td></td><td>15ycsb</td><td>workloadb</td><td></td><td></td><td>100</td><td>86081</td><td>86749</td><td>1.05</td><td>1.05</td><td>0.78</td></tr><tr><td></td><td>16ycsb</td><td></td><td>workloadc</td><td></td><td>100</td><td>94704</td><td>96258</td><td>0.935</td><td>0.921</td><td>1.64</td></tr><tr><td></td><td>17ycsb</td><td>workloadd</td><td></td><td></td><td>100</td><td>81075</td><td>82166</td><td>1.12</td><td>1.11</td><td>1.35</td></tr></table></body></html>\\n\\n# OLAP\\n\\nv OLAP\\n\\ni\\n\\nOLAPRelea!   \\n\\n<html><body><table><tr><td>#</td><td>bench_type </td><td>bench_sub_type </td><td>v1_qpm </td><td>v2_qpm </td><td>v1_total_avg_latency (s) </td><td>v2_total_avg_latency (s)</td><td>Diff(%) </td></tr><tr><td>1</td><td>job</td><td>job-tikv</td><td>17.6</td><td>17.3</td><td>303</td><td>306</td><td>1.02</td></tr><tr><td>2</td><td>ossinsight</td><td>ossinsight</td><td>3.01</td><td>3.00</td><td>1863</td><td>1900</td><td>2.00</td></tr><tr><td>3</td><td>tpcds</td><td>tpcds-tiflash-50g</td><td>11.5</td><td>11.4</td><td>323</td><td>325</td><td>0.37</td></tr><tr><td>4</td><td>tpcds</td><td>tpcds-tikv-50g</td><td>3.58</td><td>3.57</td><td>1176</td><td>1183</td><td>0.66</td></tr><tr><td>5</td><td>tpch</td><td>tpch-2tiflash-50g</td><td>16.1</td><td>16.2</td><td>73.6</td><td>72.7</td><td>-1.22</td></tr><tr><td>6</td><td>tpch</td><td>tpch-tiflash-50g</td><td>9.78</td><td>10.1</td><td>123</td><td>120</td><td>-1.79</td></tr><tr><td></td><td>7tpch</td><td>tpch-tikv-50g</td><td>1.96</td><td>1.96</td><td>669</td><td>671</td><td>0.19</td></tr></table></body></html>\\n\\n# 6 hours Stress testing\\n\\n<html><body><table><tr><td colspan=\"8\">OLTP 6 Hours Stress Test Results</td></tr><tr><td># bench_type</td><td>bench_sub_type </td><td>thread</td><td>v1_qps</td><td></td><td>v2_qps v1_avg_latency (ms) v2_avg_latency (ms) </td><td></td><td>qps_diff(%) </td></tr><tr><td>1 sysbench</td><td>oltp_insert</td><td>100</td><td>22858</td><td>22192</td><td>4.28</td><td>4.41</td><td>-2.9</td></tr><tr><td>2tpcc</td><td>tpcc1k</td><td>200</td><td>64840</td><td>64388</td><td>2.96</td><td>2.98</td><td>-0.70</td></tr></table></body></html>\\n\\n# HTAP\\n\\n<html><body><table><tr><td colspan=\"10\"> HTAP</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>HTAPRelease Test Results- TP</td><td></td></tr><tr><td># bench_type</td><td>bench_sub_type </td><td>thread</td><td>v1_qps </td><td></td><td></td><td>v2_qps  v1_avg_latency (ms) |v2_avg_latency (ms)</td><td></td><td>qps_diff(%) </td><td>avg_qps_d</td></tr><tr><td>1ch</td><td>ch-1k</td><td>200</td><td>53523</td><td>53158</td><td></td><td>3.66</td><td>3.69</td><td>-0.68</td><td></td></tr><tr><td colspan=\"10\"></td></tr><tr><td>i</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>HTAP Release Test Results - AP</td><td></td></tr><tr><td># bench_type</td><td>bench_sub_type </td><td>v1_qpm </td><td></td><td>v2_qpm </td><td></td><td>v1_total_avg_latency (s)  v2_total_avg_latency (s) </td><td></td><td>Diff(%)</td><td></td></tr><tr><td>1ch</td><td>ch-1k</td><td></td><td>2.85</td><td>2.91</td><td></td><td>428</td><td>423-1.0</td><td></td><td></td></tr></table></body></html>\\n\\nTools\\n\\n<html><body><table><tr><td colspan=\"8\">TiCDCRelease Test Resultsv</td></tr><tr><td>#description </td><td>bench_type</td><td>bench_sub_type </td><td>thread</td><td>v1_cdc_changefeed_checkpoint_lag(s)  v2_cdc_changefeed_checkpoint_lag(s)</td><td></td><td>cdc_ccl_diff(s) </td><td>cdc_ccl_diff(%)v1</td></tr><tr><td>1kafka-big-single-table</td><td>cdc_workload</td><td>bank</td><td>128</td><td>12.6</td><td>11.3</td><td>-1.36</td><td>-11</td></tr><tr><td>2mysql-big-single-table</td><td>cdc_workload</td><td>bank</td><td>128</td><td>18.8</td><td>17.9</td><td>-0.92</td><td>-4.9</td></tr><tr><td>3mysql-sysbench</td><td>sysbench</td><td>oltp_write_only</td><td>100</td><td>1.12</td><td>1.01</td><td>-0.11</td><td>-9.6</td></tr><tr><td>4tidb-big-single-table</td><td>cdc_workload</td><td>bank</td><td>128</td><td>15.1</td><td>16.0</td><td>0.91</td><td>6.01</td></tr><tr><td>5tidb-sysbench</td><td>sysbench</td><td>oltp_write_only</td><td>100</td><td>3.42</td><td>3.43</td><td>0.01</td><td>0.260</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>bench_type</td><td>bench_sub_type </td><td></td><td></td><td>BR Restore</td><td></td><td></td></tr><tr><td># description </td><td></td><td></td><td></td><td>v1_restore_duration(s) </td><td>v2_restore_duration(s) </td><td></td><td>dif%)V</td></tr><tr><td>1</td><td>ossinsight</td><td>ossinsight</td><td></td><td>2905</td><td>2877</td><td></td><td>-0.96</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>BR Backup</td><td></td><td></td></tr><tr><td>#description </td><td>bench_type </td><td>bench_sub_type </td><td></td><td>v1_backup_duration(s) </td><td>v2_backup_duration(s) </td><td></td><td>diff%)v</td></tr><tr><td>1</td><td>ossinsight</td><td>ossinsight</td><td></td><td>10556</td><td>10314</td><td></td><td>-2.29</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>#description </td><td>bench_type </td><td>bench_sub_type </td><td></td><td></td><td>Dumpling</td><td></td><td></td></tr><tr><td>1</td><td>ycsb</td><td>workloada</td><td>960</td><td>v1_dumpling_duration v2_dumpling_durationi 950</td><td>dif(%) v1_monitoring -1.02https://clinic.pingcap..</td><td>v2_monitoring </td><td>v1_log </td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>https://linic.pingcap.c...</td><td>https://clinic</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>Lightning</td><td></td><td></td></tr><tr><td># description </td><td>bench_type </td><td>bench_sub_type </td><td></td><td>v1_lightning_duration( v2_lightning_duration(:</td><td>diff(%) v1_monitoring </td><td>v2_monitoring </td><td>v1_log </td></tr><tr><td>1</td><td>ycsb</td><td>workloada</td><td>669</td><td>673</td><td>0.54https://clinic.pingcap.c..</td><td>https://clinic.pingcap...</td><td>https://clinic</td></tr></table></body></html>\\n\\n# DDL & Analyze\\n\\n<html><body><table><tr><td colspan=\"9\">~ DDL</td></tr><tr><td colspan=\"9\"></td></tr><tr><td>#</td><td>description </td><td>bench_type </td><td></td><td>bench_sub_type v1_add_index_durat  v2_add_index_dural </td><td></td><td></td><td>dif(%) commands </td><td>Add Index <</td></tr><tr><td>1</td><td></td><td>ossinsight</td><td>ossinsight-ddl</td><td>5128</td><td>4558</td><td></td><td>-11.12</td><td>alter table github_ev...</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan=\"9\">v Analyze</td></tr><tr><td colspan=\"9\"></td></tr><tr><td>#</td><td>description </td><td>bench_type </td><td>bench_sub_type </td><td>v1_analyze_duration(s) </td><td>v2_analyze_duration(s) </td><td></td><td>Analyze diff(%) </td><td>ana</td></tr><tr><td>1</td><td></td><td>ossinsight</td><td>ossinsight</td><td>851</td><td></td><td>509</td><td>-40.11</td><td>set</td></tr><tr><td>2</td><td></td><td>ossinsight</td><td>ossinsight-ddl</td><td>918</td><td></td><td>622</td><td>-32.32</td><td>set</td></tr></table></body></html>\\n\\nMetrics   \\n\\n<html><body><table><tr><td></td><td></td><td></td><td></td><td>oltp_metries_diff</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>bench_type ♀</td><td>bench_sub_type ♀ bank-50m</td><td>thread ♀avg_latency_diff(r 50 </td><td>avg_latency_diff[% °</td><td></td><td>patenc</td><td>-2.3</td><td>20</td><td></td><td></td><td></td><td></td><td></td><td>tratf ♀tikjothougputdift(MBps) ♀tikvjo_throughptdif(%)</td><td></td></tr><tr><td>bank bank </td><td>bank-50m</td><td>200</td><td></td><td>0.738</td><td>2.86</td><td>-1.3</td><td>33</td><td></td><td></td><td></td><td>8.74</td><td>3.60</td><td>-45</td><td></td></tr><tr><td>benchmarksql</td><td>benchmarksq-1k</td><td>200</td><td></td><td>-0.59</td><td>-1.7</td><td>5.39</td><td>8.66</td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.7</td></tr><tr><td>booking</td><td>booking</td><td>2</td><td></td><td></td><td></td><td>2.70</td><td>8.94</td><td></td><td></td><td></td><td></td><td></td><td></td><td>13</td></tr><tr><td>ch </td><td>ch-1k</td><td>200</td><td></td><td></td><td></td><td>-3.9</td><td>4.1</td><td></td><td></td><td></td><td></td><td></td><td></td><td>6</td></tr><tr><td>corebanking</td><td>hzbenk_simulation</td><td>5</td><td></td><td></td><td></td><td>-84</td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td><td>15</td></tr><tr><td>hzbank</td><td>hzbank_poc</td><td>5</td><td></td><td></td><td></td><td></td><td>-21</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>shenma</td><td>shenma-2x-speed-5-threa</td><td></td><td></td><td></td><td></td><td>-1.1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>-7.2</td></tr><tr><td>sysbench</td><td>oltp_insert</td><td></td><td></td><td></td><td></td><td></td><td>-19</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>100</td><td></td><td>1.3</td><td></td><td>-4.0</td><td></td><td>2.33</td><td>8.08</td><td></td><td>15.3</td><td>7.99</td><td>128</td><td>-20</td></tr><tr><td>sysbench sysbench</td><td>oltp_pointselet</td><td>100</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>8.57</td><td>6.34</td><td></td><td></td></tr><tr><td></td><td>oltpred_wite</td><td>100</td><td></td><td></td><td></td><td>-1.2</td><td>-8.0</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>sysbeneh</td><td>oltp_update_index oltp_update, onindex</td><td>100</td><td></td><td></td><td></td><td>59</td><td></td><td></td><td></td><td></td><td>7.11</td><td>3.53</td><td></td><td>24</td></tr><tr><td>sysbench</td><td>tpec1k </td><td>100</td><td></td><td></td><td></td><td>-3.3</td><td>7.38</td><td></td><td></td><td></td><td>7.24</td><td>5.52</td><td></td><td>41</td></tr><tr><td>tpec</td><td>workloada</td><td>400 100</td><td></td><td>2.90</td><td>3.67</td><td>10.1</td><td></td><td></td><td></td><td></td><td></td><td>2.95</td><td></td><td>-4.8</td></tr><tr><td>yesb</td><td>workloadb</td><td>100</td><td></td><td></td><td></td><td>-1.8</td><td></td><td></td><td></td><td></td><td>5.87</td><td></td><td></td><td>-3.5</td></tr><tr><td>yesb</td><td>workloadc</td><td>100</td><td></td><td></td><td></td><td>-1.7</td><td></td><td></td><td></td><td></td><td>7.52</td><td>4.03 5.41</td><td>3.8</td><td>1.9 2.22</td></tr><tr><td>yesb</td><td>workloadd</td><td>100</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>9.25</td><td>4.56</td><td></td><td>5.15</td></tr><tr><td>yesb</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>8.58</td><td></td><td></td><td>5.69</td></tr></table></body></html>\\n\\n# 7-day long stability testbaseline\\n\\n![](images/d2836852dd9ab3bf99facb8242539bc760a65ce805343c55704cb3de1fb9bf5d.jpg)\\n\\nMultiRocksdb\\n\\nKnown issues\\n\\nCritical   \\n[Dynamic Regions] TPCC 1K 7d stability test: QPS drops dramatically when   \\npartitioned-raft-kv is enabled compared to baseline #14460   \\n[Dynamic Regionsl improve the network trafic spikes when partitioned-raft-kv is enabled   \\n#14458   \\n[Dynamic Regions] max _prefill count conflicts with large purge-threshhold #14468   \\n[Dynamic Regions] raft-engine with v2 is slower than baseline due to append flow on GCP   \\npd-ssd disk #14487   \\n[Dynamic Regions] repaly raft log after restart is too slow (> 5hour in some cases) #14481\\n\\nMajor [Dynamic Reqions] qps and latency is unstable for oltp insert #14470 [Dynamic Regions] upgrade from v6.6. to v7.0 is slow due to Recovering raft logs #14472 [Dynamic Regions] raftstore.store-io-pool-size can not be changed dynamically #14485\\n\\nDue to incorrect configurations [Dynamic Regions] TiKV oom with TPCC workload and partitioned-raft-kv enabled #14457 [Dynamic Regions] raft-engine with v2 is much slower on GCP pd-ssd disk #14483\\n\\nTest report   \\nhttp:/perf.pingcap.net/d/BDUK3KJ7k/release-test-report-idc-environment?orgld=1&var-benc   \\nhmark_id=6070000&var-version1=7.0.0-v1&var-version2=7.0.0-v2\\n\\n7-day long stability test with MultiRocksDB Enabled https://clinic.pingcap.com.cn/portal/#/orgs/33/clusters/7214338157988119735\\n\\n![](images/a70029341aa030c1f3cc11e07d200fdbd54e71d82fb25d0a75dd786c3c2096f8.jpg)\\n\\n# Longrun System Test\\n\\n7.0.0 longrun test report\\n\\n# Kernel on DBaas\\n\\nDbaas kernel Testing TiDB v7.0.0 for DBaaS Testing Report\\n\\n<html><body><table><tr><td>Case summary</td><td>Result</td></tr><tr><td>dbaas-smoke-ch-stable-run</td><td>PASS</td></tr><tr><td>dbaas-smoke-ch-random-down-tidb</td><td>PASS</td></tr></table></body></html>\\n\\n<html><body><table><tr><td>dbaas-smoke-ch-random-down-pd</td><td>PASS</td></tr><tr><td>dbaas-smoke-ch-random-down-tikv</td><td>PASS</td></tr><tr><td>dbaas-smoke-ch-random-down-tiflash</td><td>PASS</td></tr><tr><td>dbaas-smoke-ch-backup-and-restore</td><td>PASS</td></tr><tr><td>dbaas-smoke-ch-lightning-import</td><td>PASS</td></tr><tr><td>dbaas-smoke-ch-update-config</td><td>PASS</td></tr><tr><td>dbaas-smoke-ch-scale-tidb</td><td>PASS</td></tr><tr><td>dbaas-smoke-ch-scale-tikv</td><td>PASS</td></tr></table></body></html>\\n\\n# DBaaS Hot-upgrade Test - (version path)\\n\\n<html><body><table><tr><td>Source version</td><td>Target Version</td><td>Result</td></tr><tr><td>v6.1.5</td><td>v7.0.0</td><td>PASS</td></tr><tr><td>v6.5.1</td><td>v7.0.0</td><td>PASS issues/37548</td></tr><tr><td>v6.6.0</td><td>v7.0.0</td><td>PASS issues/37548</td></tr></table></body></html>\\n\\n# Performance testing on DBaas\\n\\nStarting from version 7.0.0,the device model has been changed to ARM,and the connection method has been switched from VPC peering to PrivateLink.\\n\\nCompared to v6.6.0,there is some performance regression, but the magnitude is less than $5 \\\\%$ （204号\\n\\n# sysbench\\n\\ntpcc   \\n\\n<html><body><table><tr><td colspan=\"9\">OLIP</td></tr><tr><td colspan=\"9\">Sysbench Test Results </td></tr><tr><td>#</td><td>bench_type </td><td>bench_sub_type </td><td> thread </td><td>v1_tps </td><td>v2_tps </td><td>v1_qps </td><td>v2_qps </td><td>qps_diff(%) </td></tr><tr><td>1</td><td>sysbench</td><td>oltp_insert</td><td>100</td><td>9469</td><td>9209</td><td>9468.86</td><td>9209.19</td><td>-2.7</td></tr><tr><td>2</td><td>sysbench</td><td>oltp_point_select</td><td>100</td><td>54275</td><td>53250</td><td>54275.43</td><td>53250.05</td><td>-1.9</td></tr><tr><td>3</td><td>sysbench</td><td>oltp_read_write</td><td>100</td><td>1656</td><td>1632</td><td>33109.89</td><td>32640.95</td><td>-1.4</td></tr><tr><td></td><td>4sysbench</td><td>oltp_update_index</td><td>100</td><td>7072</td><td>6840</td><td>7071.79</td><td>6840.22</td><td>-3.3</td></tr><tr><td>5</td><td>sysbench</td><td>oltp_update_non_index</td><td>100</td><td>9181</td><td>8860</td><td>9181.39</td><td>8859.93</td><td>-3.5</td></tr></table></body></html>\\n\\nTPCCTestResults   \\n\\n<html><body><table><tr><td># bench_type</td><td>bench_sub_type </td><td>thread</td><td>v1_tpmC </td><td>v2_tpmC tpm_diff(%)</td><td></td><td>avg_tpm_diff(%) </td><td>duration(s) </td></tr><tr><td>1 tpcc</td><td>tpcc-1k</td><td>100</td><td>49292</td><td>49151-0.29</td><td></td><td>-0.29</td><td>7201</td></tr></table></body></html>\\n\\n# ARM Compatibility Test\\n\\n<html><body><table><tr><td>Test Items</td><td>Result</td></tr><tr><td>deploy-plugin-tiup-arm64</td><td>PASS</td></tr><tr><td>deploy-plugin-operator-arm64</td><td>PASS</td></tr><tr><td>e2e-htap-misc-arm64</td><td>PASS</td></tr><tr><td>e2e-htap-encryption</td><td>PASS</td></tr><tr><td>endless-oltp-tiup-tls-arm64</td><td>PASS</td></tr><tr><td>cdc-pd-leader-switch-sync-arm64</td><td>PASS</td></tr><tr><td>brie-acceptance-robust-arm64</td><td>PASS</td></tr><tr><td>binlog-acceptance-test-arm64</td><td>PASS</td></tr><tr><td>dm-migrate-incremental-dml-arm64</td><td>PASS</td></tr><tr><td>conprof-arm64</td><td>PASS</td></tr><tr><td>tiup_upgrade_test_v5.1.4-v7.0.0-arm64</td><td>PASS</td></tr><tr><td>tiup_upgrade_test_v6.1.5-v7.0.0-arm64</td><td>PASS</td></tr><tr><td>tiup_upgrade_test_v6.5.1-v7.0.0-arm64</td><td>PASS</td></tr><tr><td>upgrade_test_v5.1.4-v7.0.0-pre-arm64</td><td>PASS</td></tr><tr><td>upgrade_test_v6.1.5-v7.0.0-pre-arm64</td><td>PASS</td></tr><tr><td>upgrade_test_v6.5.1-v7.0.0-pre-arm64</td><td>PASS</td></tr><tr><td>endless-oltp-tpcc-stability-large-arm64</td><td>PASS</td></tr></table></body></html>\\n\\n# Security\\n\\nTiDB 7.0.0-DMR Release Security test upgrade grafana to last v7.5.11 Fix CVE-2021-39226\\n\\n# Golang\\n\\nGo has been upgraded to version 1.20.2， some of the garbage collector\\'s internal data structures were reorganized to be both more space and CPU efficient. Go_1.20 Release Notes\\n\\n# Conclusion\\n\\nCompared to v6.6.0,we have added more than 68 test cases for   \\nCompute/OLTP/HTAP/DP   \\nIn v7.0.0,we have verified the upgrade compatibility from versions 4.0, 5.0, 5.1, 5.2, 5.3, 5.4, 6.1, 6.2.0, 6.3.0, 6.4.0, 6.5.0, 6.6.0 to v7.0.0, on both TiUP and Operator environments.   \\nWe conducted user journey testing based on the CC and CH workloads in long-run clusters, as well as stability testing for TP, AP, CDC,and PiTR.   \\nWe conducted kernel user journey testing, high availability testing,and hot-upgrade testing in the DBaaS environment.   \\nIn terms of performance, compared to v6.6.0, there has been an $1 1 \\\\%$ improvement in \"Add index\" and a $40 \\\\%$ improvement in \"Analyze\",while the performance in other aspects remained stable.   \\n$7 ^ { \\\\star } 2 4$ hours stable testing on the default configuration cluster runs smoothly, but on a cluster with mulitirocksdb enabled, the QPS will gradually decrease.We have identified this issue and it will be resolved in subsequent versions.   \\nWe also conducted compatibility testing on ARM environment, including test cases for enterprise plugins, OLTP, HTAP and DP.   \\nThe security department conducted a security check on v7.0.0 and upgraded the Grafana version to v7.5.11.   \\nGolang was upgraded to 1.20.2,with improved GC for better memory and CPU utilization.',\n",
       "  'source_link': 'https://docs.google.com/document/d/1APn1S3EVj6v19iF2h_txk39EdcDLk0WhlZoTRzIVoDE/edit?usp=drivesdk'},\n",
       " '863ab62a-490b-4f02-bf0c-411fd54a51d8': {'source_id': '863ab62a-490b-4f02-bf0c-411fd54a51d8',\n",
       "  'source_name': 'Apple TiDB Meeting Jan 22 2024',\n",
       "  'source_content': '# TiDB Meeting Jan 22nd for Apple\\n\\nYu Dong Cong Wang Senior Director of Engineering Manager of TiDB SQL Infra team\\n\\nNabil Nawaz Principal Solution Engineer\\n\\nLouis Fahrberger Account Executive\\n\\n# Safe Harbor Statement\\n\\nThis presentation has been prepared for general informational purposes only. All information contained in this presentation is provided in good faith, however we make no representation or warranty of any kind,express or implied, regarding the accuracy,adequacy, completeness of any information.The information may not be incorporated into any contract. The development, release and timing of any features or functionality described for PingCAP products remains at the sole discretion of PingCAP.\\n\\n![](images/615d00ce13338b463edfe54f37d47246dec7a8b4b2118ecb9241bbda0226eb2e.jpg)\\n\\n# TiDB Security Updates\\n\\n<html><body><table><tr><td>Severity Title</td><td></td><td>File</td><td>Line</td><td>Actions</td><td>Estimated Fix Date</td></tr><tr><td rowspan=\"9\">Max Max</td><td rowspan=\"9\">DeprenatedTLS/SSL</td><td>br/pkg/lightning/config/config.go</td><td>190</td><td rowspan=\"9\">Wecan remove the support pfTLs1.0/1by 8ome</td><td rowspan=\"9\">12th,Jan.2023</td></tr><tr><td>dumpling/export/config.go pkg/server/server.go</td><td>282 728</td></tr><tr><td></td><td></td></tr><tr><td>pkg/server/server.go</td><td>730</td></tr><tr><td>pkg/sesaionctx/variable/statusvar.go</td><td>126</td></tr><tr><td>pkg/sessionctx/variable/statusvar.go</td><td>127</td></tr><tr><td>pkg/util/miac.go</td><td>488</td></tr><tr><td>pkg/util/miac.go</td><td>491</td></tr><tr><td>pkg/util/miac.go</td><td>493</td></tr><tr><td rowspan=\"3\"></td><td>pkg/util/security.go pkg/util/security.go</td><td></td><td>97</td></tr><tr><td></td><td>pkg/privilege/privileges/ldap/ldap_co mmon.go</td><td>186 123</td><td>Refine the code following this instruction: however, when setting the‘min TLS version\\',we\\'llface the \\'deprecated TLS version\\'</td></tr><tr><td>Insecure TLS Configuration</td><td>pkg/privilege/privileges/ldap/ldap_co mmon.go</td><td>136</td><td>issue so the first issue should be addressed.</td></tr><tr><td rowspan=\"9\">Max</td><td rowspan=\"9\">SQL Injection Risk-Format String</td><td>br/pkg/lightning/checkpoints/checkpoi nts.go</td><td>1614</td><td rowspan=\"9\">Refine the code following this instruction. 12,Jan,2024</td></tr><tr><td>br/pkg/lightning/checkpointa/checkpoi nts.go</td><td>1619</td></tr><tr><td>br/pkg/lightning/checkpoints/checkpoi nts.go</td><td>1665</td></tr><tr><td>br/pkg/lightning/checkpointa/checkpoi1670 nts.go</td><td></td></tr><tr><td>br/pkg/lightning/checkpoints/checkpoi nts.go</td><td>1675</td></tr><tr><td>br/pkg/lightning/importer/meta_manag|274 er.go</td><td></td></tr><tr><td>br/pkg/lightning/importer/meta_manag|463 er.go br/pkg/lightning/importer/meta_manag</td><td>691</td></tr><tr><td>er.go br/pkg/lightning/importer/meta_manag800</td><td></td></tr><tr><td>er.go br/pkg/lightning/importer/meta_manag 918</td><td></td></tr><tr><td>er.go</td><td>dumpling/teats/s3/import.go</td><td>69</td></tr></table></body></html>\\n\\n# Resolved.. No ActionNeeded\\n\\n<html><body><table><tr><td rowspan=\"8\">High Poor Secrets Control</td><td rowspan=\"8\"></td><td>br/compatibility/credentials/applicatio n_default_credentials.json</td><td>3)</td><td rowspan=\"13\">(NO ACTION NEEDED) Used for internal teating tool,not released in the product.</td><td rowspan=\"8\">NA</td><td rowspan=\"8\"></td></tr><tr><td>br/tests/br_gcs/run.sh</td><td>62</td></tr><tr><td>br/tests/docker_compatible_gcs/_run. sh</td><td>26</td></tr><tr><td>br/tests/lightning_gca/run.sh</td><td>60</td></tr><tr><td>pkg/config/config_test.go</td><td>952</td></tr><tr><td>pkg/privilege/privileges/tidb_auth_toke|37 n_test.go</td><td></td></tr><tr><td>pkg/privilege/privileges/tidb_auth_toke63 n_test.go</td><td></td></tr><tr><td>pkg/privilege/privileges/tidb_auth_toke|90 n_test.go</td><td></td></tr></table></body></html>\\n\\n<html><body><table><tr><td rowspan=\"2\">Max</td><td rowspan=\"2\">Profiling Endpoint</td><td>cmd/benchkv/main.go</td><td>23</td><td rowspan=\"2\">(NO ACTION NEEDED) Used for internal</td><td rowspan=\"2\">NA</td></tr><tr><td>cmd/benchraw/main.go</td><td>22</td></tr><tr><td></td><td>Automatically Exposed</td><td></td><td></td><td>benchmark tool,not released in the product.</td><td></td></tr></table></body></html>\\n\\n# TiDB Replication\\n\\n# Replication Supported Scenarios\\n\\nSelf-Managed\\n\\nTiDB $\\\\mathrm { - - } >$ TiCDC $\\\\displaystyle - >$ Kafka (one way replication) TiDB $\\\\mathrm { - - } >$ TiCDC $\\\\displaystyle - >$ MySQL (one way replication) . TiCDC Files written on the local filesystem or on the Amazon S3-compatible storage.\\n\\nDR use case TiDB $\\\\displaystyle - >$ TiCDC $\\\\displaystyle - >$ TiDB (one way & bi-directional) Migration use case DM Replication from MySQL Compatible DB to TiDB\\n\\n![](images/9be02314217d457025768739b8348b99ddee3cd57406c97297a515dc0c25511f.jpg)\\n\\n# Replication Supported Scenarios\\n\\nSource\\n\\nChange Data Capture\\n\\nTargets\\n\\n![](images/02ef56022ff38cfbc485f1ce39e0528d7db5d3ad9adf4feca7724d3ab8297c3a.jpg)\\n\\n# Flipkart TiCDC Conflict Detection\\n\\nFlipkart uses the following monitoring metrics to detect TiCDC data conflicts -\\n\\nMySQL sink conflict detect duration: The histogram of the time spent on detecting MySQL sink conflicts\\n\\nMySQL sink conflict detect duration percentile:The time (P95,P99,and P999) spent on detecting MySQL sink conflicts within one second\\n\\nSync Diff Inspector is used to identified the tables out of sync\\n\\nhttps://docs.pingcap.com/tidb/stable/sync-diff-inspector-overview\\n\\n![](images/b04888fc5b634bd8527691410f33e02151d962d069fbd1500c66b0c10914e7e5.jpg)\\n\\n# Unsupported Scenarios\\n\\nBlock-level data replication across two TiDB clusters,assuming each TiDB cluster would be in separate regions.\\n\\nSingle TiDB cluster deployed across regions (Multi-region cluster).\\n\\n![](images/d8db1b9dede3f2941f653a4e3cd5f7c66d796047b604d96393e63ffb91ee5f19.jpg)\\n\\n# FRM -Feature Request Management\\n\\nFRM Process\\n\\n1. Proposal\\n\\nRegional Engineering resource assigned   \\nGlobal Product Enabling Team - Dir Global Support Team Oversight   \\nTracked in internal JIRA system\\n\\n2. Initiating Request\\n\\nDescription - Customer Requirements Current Solution - How Customer handles today Customer Expectation - Goal and Time Business Impact on Customers and PingCAP 0 Customer - Business associated to delivery 0 PingCAP- Effect on Current TiDB Product Road Map\\n\\n# 3. Approve/Decline\\n\\nExamples\\n\\nPITR   \\nBackup &Recovery ie.TiDB Flashback   \\nTTL - Time To Live   \\nMulti-Tenant   \\nMulti Value Index - In Progress\\n\\n# TiDB Roadmap\\n\\n# OdIDo 5 Contributors\\n\\nThisroadmapprovidesalookintotheproposedfuture.Thiswillbecontinuallyupdatedaswereleaseong-termstable(LTS)versions.The purposeistoprovidvisibilityintatiomingsotatyouanmoreoselyfollowhproges,aabouttheyilestonsone way, and give feedback as the development work goes on.\\n\\nInthecourseofdevelopment,thisoadapsecttoangebasedsereedsandfedback.Asexpeted,asthecmseig theitemsunderthemarelesscommitted.Ifyouhaveafeaturerequestorwanttopriorieafeature,pleasefileanisseonGitHub.\\n\\nRolling roadmap highlights   \\n\\n<html><body><table><tr><td>Category</td><td>End of CY23 LTS release</td><td>Mid of CY24 LTS release</td><td>Future releases</td></tr><tr><td>Scalability and Performance Enhance horsepower</td><td>Distributed eXecution Framework (DXF) In v7.1.0,TDBintroducedthe DXF fortasks (such as DDL and analyze). This is the foundation for parallelizing these tasksacross compute nodes. v7.4.0 introduces global sorting in distributed re-organization tasks (such as DDL and import), which greatly mitigatesextra resourceconsumption instorage. Optionally, external shared storage can be leveraged for simplicity and cost savings.</td><td>•Enhancements to performance and generalizability of plan cache ·Dynamic node scaling via the DXF Automaticallyadjust node allocation tomeet resource</td><td>·Unlimited transaction size ·Federated query TiDB query planner support for multiple storage engines in HTAP use cases.</td></tr><tr><td>Reliability and Availability Enhance dependability</td><td>Resource control for background tasks Control over how background tasks,such as imports, DDL, TTL, auto-analye, d compactions,canafect foregroundtraffic Runawayquery control Anoperator-controlled wayto greatlyenhance performance stability for workloads with</td><td>·Disaggregationof Placement Driver (PD) Enhance cluster scalability andresilience</td><td>·Multi-tenancy Resource isolation on top of resource control</td></tr></table></body></html>\\n\\n# The most advanced, open source, distributed SQL database for modern applications.\\n\\nScalable. Versatile. Titanium (Ti) Reliable.\\n\\n□Square ERTIK Catalyst dailymotion NIANTIC 5 CAPCOM Bolt MI xiaomi S Shopee ninjavan H NETSCAN T Streak\\n\\n# PoC to Simulate a Critical Workload\\n\\nWe will recommend an ideal sized TiDB Cluster based on the following criteria.\\n\\nProvide raw data size   \\nPeak QPS (Reads/Writes/Mixed %)   \\nPeak concurrent connections   \\nSchema scripts (Table,PK, Index definitions) for review   \\nSQL scripts that will run for review   \\nWorkload type OLTP/Analytical or both (HTAP) O If Analytical please provide the sizing of tables to be stored in TiFlash\\n\\n# TiDB Benchmarking Tools\\n\\nTiup benchmark tool for performance testing https://docs.pingcap.com/tidb/dev/benchmark-tidb-using-tpcc\\n\\nTiDBwith sysbench https://docs.pingcap.com/tidb/stable/benchmark-tidb-using-sysbench\\n\\nThank you\\n\\nQA',\n",
       "  'source_link': 'https://docs.google.com/presentation/d/1oc8ghjERj785A6a5OYucaZ_LtO7VdpCvHgFsXcbkMmI/edit?usp=drivesdk'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_name = \"Apple\"\n",
    "docs = client_docs[client_name]\n",
    "\n",
    "topic_name = f\"Customer Tracking for {client_name}\"\n",
    "\n",
    "print(\"step 1: upload docs to knowledge base\")\n",
    "topic_docs = {}\n",
    "for doc in docs:\n",
    "    file_path = doc['path']\n",
    "    try:\n",
    "        res = kb_builder.extract_knowledge(\n",
    "            file_path, \n",
    "            doc\n",
    "        )\n",
    "        if res['status'] == 'success':\n",
    "            topic_docs[res['source_id']] = {\n",
    "                \"source_id\": res['source_id'],\n",
    "                \"source_name\": res['source_name'],\n",
    "                \"source_content\": res['source_content'],\n",
    "                \"source_link\": res['source_link'],\n",
    "            } \n",
    "        else:\n",
    "            print(f\"process index {file_path} failed, {res['error']}\", exc_info=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"process index {file_path} failed, {e}\", exc_info=True)\n",
    "\n",
    "topic_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-12 14:32:52,246] INFO in graph_builder: Building iterative knowledge graph for topic: Customer Apple Tracking: 9 documents\n",
      "[2025-06-12 14:32:52,247] INFO in graph_builder: === Stage 0: Generating document summaries ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2: add to graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-12 14:32:54,646] INFO in summarizer: Generating summary for document: Apple Customer Questions\n",
      "[2025-06-12 14:34:19,986] INFO in _client: HTTP Request: POST http://192.168.206.101:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:34:22,553] INFO in summarizer: Using cached summary for document: TiDB's Relationship with MySQL and Security Vulnerabilities\n",
      "[2025-06-12 14:34:23,697] INFO in summarizer: Using cached summary for document: 1 - Account Discovery Capture Sheet (with instruction)\n",
      "[2025-06-12 14:34:25,010] INFO in summarizer: Using cached summary for document: Louis FINAL 10102023\n",
      "[2025-06-12 14:34:26,223] INFO in summarizer: Using cached summary for document: Apple Relevant Features\n",
      "[2025-06-12 14:34:27,569] INFO in summarizer: Using cached summary for document: Apple POC Joint Execution Plan\n",
      "[2025-06-12 14:34:28,820] INFO in summarizer: Using cached summary for document:  Technical Discovery Capture Form \n",
      "[2025-06-12 14:34:30,189] INFO in summarizer: Using cached summary for document: Copy of  TiDB 7.0.0 Testing Report\n",
      "[2025-06-12 14:34:31,445] INFO in summarizer: Using cached summary for document: Apple TiDB Meeting Jan 22 2024\n",
      "[2025-06-12 14:34:32,288] INFO in summarizer: Generated 9 summaries for topic: Customer Apple Tracking\n",
      "[2025-06-12 14:34:32,289] INFO in graph_builder: \n",
      "=== Stage 1: Generating skeletal graph ===\n",
      "[2025-06-12 14:34:32,291] INFO in graph: Generating skeletal graph for Customer Apple Tracking\n",
      "[2025-06-12 14:34:33,217] INFO in graph: Using cached skeletal graph for Customer Apple Tracking\n",
      "[2025-06-12 14:34:33,218] INFO in graph_builder: \n",
      "=== Stage 2: Generating analysis blueprint ===\n",
      "[2025-06-12 14:34:33,630] INFO in graph: Using existing analysis blueprint for Customer Apple Tracking\n",
      "[2025-06-12 14:34:34,057] INFO in graph_builder: \n",
      "=== Stage 2.5: Converting skeletal graph to entities and relationships ===\n",
      "[2025-06-12 14:34:34,059] INFO in graph: Converting skeletal entity {'description': 'Customer entity evaluating TiDB for tracking systems', 'entity_type': 'Customer', 'name': 'Apple', 'role': 'central_topic'} to actual entity\n",
      "[2025-06-12 14:34:35,430] INFO in graph: Converting skeletal entity {'description': \"Distributed database solution evaluated for Apple's tracking needs\", 'entity_type': 'Technical Component', 'name': 'TiDB Cloud', 'role': 'key_aspect'} to actual entity\n",
      "[2025-06-12 14:34:36,250] INFO in graph: Converting skeletal entity {'description': 'Change Data Capture tool for replication and synchronization in TiDB', 'entity_type': 'Technical Component', 'name': 'TiCDC', 'role': 'supporting_element'} to actual entity\n",
      "[2025-06-12 14:34:37,089] INFO in graph: Converting skeletal entity {'description': 'Requirements for encryption, TLS standards, and vulnerability management', 'entity_type': 'Requirement', 'name': 'Security Compliance', 'role': 'key_aspect'} to actual entity\n",
      "[2025-06-12 14:34:37,937] INFO in graph: Converting skeletal entity {'description': 'Need for horizontal scaling, multi-tenancy, and global low-latency operations', 'entity_type': 'Requirement', 'name': 'Scalability Requirements', 'role': 'key_aspect'} to actual entity\n",
      "[2025-06-12 14:34:38,795] INFO in graph: Converting skeletal entity {'description': 'Cloud infrastructure providers for TiDB deployment', 'entity_type': 'Technical Component', 'name': 'AWS/GCP', 'role': 'supporting_element'} to actual entity\n",
      "[2025-06-12 14:34:39,625] INFO in graph: Converting skeletal entity {'description': 'Stakeholder responsible for validating TiDB technical suitability', 'entity_type': 'Key Personnel', 'name': 'Technical Decision-Maker', 'role': 'supporting_element'} to actual entity\n",
      "[2025-06-12 14:34:40,437] INFO in graph: Converting skeletal entity {'description': 'High-level stakeholder driving the TiDB evaluation initiative', 'entity_type': 'Key Personnel', 'name': 'Executive Sponsor', 'role': 'supporting_element'} to actual entity\n",
      "[2025-06-12 14:34:41,284] INFO in graph: Converting skeletal entity {'description': 'Apple hardware compatibility requirement for TiDB deployment', 'entity_type': 'Technical Component', 'name': 'ARM Architecture', 'role': 'supporting_element'} to actual entity\n",
      "[2025-06-12 14:34:42,124] INFO in graph: Converting skeletal entity {'description': \"Critical requirement for data resilience in Apple's tracking systems\", 'entity_type': 'Requirement', 'name': 'Disaster Recovery (DR)', 'role': 'key_aspect'} to actual entity\n",
      "[2025-06-12 14:34:42,971] INFO in graph: Converting skeletal relationship {'relationship': 'evaluates for customer tracking system replacement', 'source_entity': 'Apple', 'target_entity': 'TiDB Cloud'} to actual relationship\n",
      "[2025-06-12 14:34:43,818] INFO in graph: Converting skeletal relationship {'relationship': 'provides features to meet horizontal scaling and multi-tenancy needs', 'source_entity': 'TiDB Cloud', 'target_entity': 'Scalability Requirements'} to actual relationship\n",
      "[2025-06-12 14:34:44,632] INFO in graph: Converting skeletal relationship {'relationship': 'must align with encryption, TLS, and vulnerability standards', 'source_entity': 'TiDB Cloud', 'target_entity': 'Security Compliance'} to actual relationship\n",
      "[2025-06-12 14:34:45,483] INFO in graph: Converting skeletal relationship {'relationship': 'requires infrastructure on cloud providers for TiDB deployment', 'source_entity': 'Apple', 'target_entity': 'AWS/GCP'} to actual relationship\n",
      "[2025-06-12 14:34:46,326] INFO in graph: Converting skeletal relationship {'relationship': 'validates technical suitability for tracking workloads', 'source_entity': 'Technical Decision-Maker', 'target_entity': 'TiDB Cloud'} to actual relationship\n",
      "[2025-06-12 14:34:47,167] INFO in graph: Converting skeletal relationship {'relationship': \"must support Apple's M1/M2 chip compatibility\", 'source_entity': 'TiDB Cloud', 'target_entity': 'ARM Architecture'} to actual relationship\n",
      "[2025-06-12 14:34:48,012] INFO in graph: Converting skeletal relationship {'relationship': 'requires DR planning for business continuity', 'source_entity': 'Apple', 'target_entity': 'Disaster Recovery (DR)'} to actual relationship\n",
      "[2025-06-12 14:34:48,827] INFO in graph: Converting skeletal relationship {'relationship': 'enables high-availability replication for tracking data consistency', 'source_entity': 'TiCDC', 'target_entity': 'Apple'} to actual relationship\n",
      "[2025-06-12 14:34:49,937] INFO in graph: Converting skeletal relationship {'relationship': 'drives alignment with business objectives for adoption', 'source_entity': 'Executive Sponsor', 'target_entity': 'TiDB Cloud'} to actual relationship\n",
      "[2025-06-12 14:34:50,831] INFO in graph: Converting skeletal relationship {'relationship': 'addresses MySQL compatibility and vulnerability management', 'source_entity': 'TiDB Cloud', 'target_entity': 'Security Compliance'} to actual relationship\n",
      "[2025-06-12 14:34:51,686] INFO in graph: Skeletal graph converted: 0 entities, 0 relationships\n",
      "[2025-06-12 14:34:51,687] INFO in graph_builder: \n",
      "=== Stage 3: Extracting narrative triplets to enrich skeletal graph ===\n",
      "[2025-06-12 14:34:51,687] INFO in graph: Processing document to extract triplets: Apple Customer Questions\n",
      "[2025-06-12 14:39:35,229] INFO in _client: HTTP Request: POST http://192.168.206.101:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:39:35,236] INFO in graph: semantic triplet: {'subject': {'name': 'TiCDC', 'description': 'Change Data Capture tool for replication and synchronization in TiDB, capable of handling node failures through leader election and data resynchronization', 'attributes': {'entity_type': 'Technical Component'}}, 'predicate': \"Enables high-availability replication for Apple's tracking data consistency by detecting node failures via heartbeat mechanisms, electing new leaders, and synchronizing data to minimize downtime\", 'object': {'name': 'Apple', 'description': 'Customer entity evaluating TiDB for tracking systems with critical requirements for fault-tolerant data replication', 'attributes': {'entity_type': 'Customer'}}, 'relationship_attributes': {'timestamp': 'Not specified', 'sentiment': 'positive'}, 'topic_name': 'Customer Apple Tracking', 'category': 'narrative'}\n",
      "[2025-06-12 14:39:35,236] INFO in graph: semantic triplet: {'subject': {'name': 'TiDB Cloud', 'description': \"Distributed database solution evaluated for Apple's tracking needs, supporting multiple replication streams and horizontal scaling\", 'attributes': {'entity_type': 'Technical Component'}}, 'predicate': \"Addresses Apple's scalability requirements through support for multiple replication streams and horizontal scaling, enabling multi-tenancy and global low-latency operations\", 'object': {'name': 'Scalability Requirements', 'description': \"Need for horizontal scaling, multi-tenancy, and global low-latency operations in Apple's tracking systems\", 'attributes': {'entity_type': 'Requirement'}}, 'relationship_attributes': {'timestamp': 'Not specified', 'sentiment': 'positive'}, 'topic_name': 'Customer Apple Tracking', 'category': 'narrative'}\n",
      "[2025-06-12 14:39:35,236] INFO in graph: semantic triplet: {'subject': {'name': 'TiDB Cloud', 'description': \"Distributed database solution evaluated for Apple's tracking needs, requiring compliance with MySQL 8.x libraries and TLS standards\", 'attributes': {'entity_type': 'Technical Component'}}, 'predicate': \"Must align with Apple's security compliance requirements by avoiding MySQL 5.7 EOL libraries and adopting modern TLS standards to mitigate vulnerability risks\", 'object': {'name': 'Security Compliance', 'description': \"Requirements for encryption, TLS standards, and vulnerability management in Apple's tracking systems\", 'attributes': {'entity_type': 'Requirement'}}, 'relationship_attributes': {'timestamp': 'Not specified', 'sentiment': 'positive'}, 'topic_name': 'Customer Apple Tracking', 'category': 'narrative'}\n",
      "[2025-06-12 14:39:35,236] INFO in graph: semantic triplet: {'subject': {'name': 'TiDB Cloud', 'description': \"Distributed database solution evaluated for Apple's tracking needs, requiring ARM architecture compatibility\", 'attributes': {'entity_type': 'Technical Component'}}, 'predicate': \"Must support Apple's M1/M2 chip compatibility (ARM Architecture) to ensure alignment with the company's ecosystem and hardware integration requirements\", 'object': {'name': 'ARM Architecture', 'description': 'Apple hardware compatibility requirement for TiDB deployment in M1/M2 chip environments', 'attributes': {'entity_type': 'Technical Component'}}, 'relationship_attributes': {'timestamp': 'Not specified', 'sentiment': 'positive'}, 'topic_name': 'Customer Apple Tracking', 'category': 'narrative'}\n",
      "[2025-06-12 14:39:35,237] INFO in graph: semantic triplet: {'subject': {'name': 'Apple', 'description': 'Customer entity evaluating TiDB for tracking systems with disaster recovery planning needs', 'attributes': {'entity_type': 'Customer'}}, 'predicate': 'Requires disaster recovery planning for business continuity by implementing TiCDC-based failover from primary to DR clusters, including steps like stopping replication, starting DR cluster processes, and updating application connections', 'object': {'name': 'Disaster Recovery (DR)', 'description': \"Critical requirement for data resilience in Apple's tracking systems\", 'attributes': {'entity_type': 'Requirement'}}, 'relationship_attributes': {'timestamp': 'Jan 30, 2024', 'sentiment': 'positive'}, 'topic_name': 'Customer Apple Tracking', 'category': 'narrative'}\n",
      "[2025-06-12 14:39:35,237] INFO in graph: semantic triplet: {'subject': {'name': 'PingAn Group', 'description': 'Customer who replaced Oracle Exadata with TiDB to achieve 40% TCO reduction and eliminate scalability bottlenecks', 'attributes': {'entity_type': 'Core Projects'}}, 'predicate': \"Serves as a reference case for Apple's evaluation of TiDB Cloud, demonstrating its ability to replace Oracle with horizontal scalability and cost efficiency in core business systems\", 'object': {'name': 'Apple', 'description': 'Customer entity evaluating TiDB for tracking systems with similar scalability and cost-reduction objectives', 'attributes': {'entity_type': 'Customer'}}, 'relationship_attributes': {'timestamp': 'Not specified', 'sentiment': 'positive'}, 'topic_name': 'Customer Apple Tracking', 'category': 'narrative'}\n",
      "[2025-06-12 14:39:35,237] INFO in graph: semantic triplet: {'subject': {'name': 'TiCDC', 'description': 'Change Data Capture tool for replication and synchronization in TiDB, supporting cross-cloud deployment', 'attributes': {'entity_type': 'Technical Component'}}, 'predicate': \"Requires deployment on cloud infrastructure providers (AWS/GCP) to align with Apple's cross-cloud deployment strategy for TiDB\", 'object': {'name': 'AWS/GCP', 'description': \"Cloud infrastructure providers for TiDB deployment in Apple's tracking systems\", 'attributes': {'entity_type': 'Technical Component'}}, 'relationship_attributes': {'timestamp': 'Not specified', 'sentiment': 'positive'}, 'topic_name': 'Customer Apple Tracking', 'category': 'narrative'}\n",
      "[2025-06-12 14:39:35,237] INFO in graph: semantic triplet: {'subject': {'name': 'Executive Sponsor', 'description': \"High-level stakeholder driving the TiDB evaluation initiative for Apple's tracking systems\", 'attributes': {'entity_type': 'Key Personnel'}}, 'predicate': \"Drives alignment with business objectives for TiDB adoption by prioritizing cost reduction and global scalability, referencing PingAn Group's Oracle replacement success\", 'object': {'name': 'TiDB Cloud', 'description': \"Distributed database solution evaluated for Apple's tracking needs with business outcomes tied to cost and scalability\", 'attributes': {'entity_type': 'Technical Component'}}, 'relationship_attributes': {'timestamp': 'Not specified', 'sentiment': 'positive'}, 'topic_name': 'Customer Apple Tracking', 'category': 'narrative'}\n",
      "[2025-06-12 14:39:35,238] INFO in graph: semantic triplet: {'subject': {'name': 'Technical Decision-Maker', 'description': \"Stakeholder responsible for validating TiDB technical suitability in Apple's tracking systems\", 'attributes': {'entity_type': 'Key Personnel'}}, 'predicate': \"Validates TiDB Cloud's technical fit through monitoring metrics for sink errors and replication task interruptions, using Grafana dashboards and TiCDC CLI tools\", 'object': {'name': 'TiDB Cloud', 'description': \"Distributed database solution evaluated for Apple's tracking needs with technical validation requirements\", 'attributes': {'entity_type': 'Technical Component'}}, 'relationship_attributes': {'timestamp': 'Not specified', 'sentiment': 'positive'}, 'topic_name': 'Customer Apple Tracking', 'category': 'narrative'}\n",
      "[2025-06-12 14:39:35,238] INFO in graph: semantic triplet: {'subject': {'name': 'TiCDC', 'description': 'Change Data Capture tool for replication and synchronization in TiDB, currently lacking built-in write conflict resolution', 'attributes': {'entity_type': 'Technical Component'}}, 'predicate': \"Relies on application-level conflict resolution for write conflicts in multi-master replication scenarios, as demonstrated by Flipkart and LinkedIn's approaches\", 'object': {'name': 'Application-Level Conflict Handling', 'description': \"Workaround for TiCDC's lack of built-in write conflict resolution in Apple's multi-region replication strategy\", 'attributes': {'entity_type': 'Challenges'}}, 'relationship_attributes': {'timestamp': 'Not specified', 'sentiment': 'neutral'}, 'topic_name': 'Customer Apple Tracking', 'category': 'narrative'}\n",
      "[2025-06-12 14:43:24,018] INFO in _client: HTTP Request: POST http://192.168.206.101:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:43:24,022] INFO in graph: structural triplet: {'subject': {'name': 'TiCDC', 'description': 'Change Data Capture tool for replication and synchronization in TiDB', 'attributes': {'entity_type': 'Technical Component'}}, 'predicate': 'handles node failure recovery by detecting failures, electing new leaders, synchronizing data, and resuming replication', 'object': {'name': 'Disaster Recovery (DR)', 'description': \"Critical requirement for data resilience in Apple's tracking systems\", 'attributes': {'entity_type': 'Requirement'}}, 'relationship_attributes': {'timestamp': None, 'sentiment': 'neutral', 'hierarchy_level': 'component_to_detail'}, 'topic_name': 'Customer Apple Tracking', 'category': 'skeletal'}\n",
      "[2025-06-12 14:43:24,023] INFO in graph: structural triplet: {'subject': {'name': 'TiDB Cloud', 'description': \"Distributed database solution evaluated for Apple's tracking needs\", 'attributes': {'entity_type': 'Technical Component'}}, 'predicate': 'supports multiple replication streams for real-time data synchronization', 'object': {'name': 'Scalability Requirements', 'description': 'Need for horizontal scaling, multi-tenancy, and global low-latency operations', 'attributes': {'entity_type': 'Requirement'}}, 'relationship_attributes': {'timestamp': None, 'sentiment': 'positive', 'hierarchy_level': 'aspect_to_component'}, 'topic_name': 'Customer Apple Tracking', 'category': 'skeletal'}\n",
      "[2025-06-12 14:43:24,023] INFO in graph: structural triplet: {'subject': {'name': 'TiDB Data Migration (DM)', 'description': 'Tool for full data migration and incremental replication from MySQL to TiDB', 'attributes': {'entity_type': 'Technical Component'}}, 'predicate': 'enables one-way replication from MySQL to TiDB but lacks bidirectional support', 'object': {'name': 'TiDB Cloud', 'description': \"Distributed database solution evaluated for Apple's tracking needs\", 'attributes': {'entity_type': 'Technical Component'}}, 'relationship_attributes': {'timestamp': None, 'sentiment': 'neutral', 'hierarchy_level': 'component_to_detail'}, 'topic_name': 'Customer Apple Tracking', 'category': 'skeletal'}\n",
      "[2025-06-12 14:43:24,023] INFO in graph: structural triplet: {'subject': {'name': 'PingAn Group', 'description': 'Customer case study replacing Oracle with TiDB for core business systems', 'attributes': {'entity_type': 'Customer'}}, 'predicate': \"demonstrates TiDB's scalability and cost-effectiveness compared to Oracle\", 'object': {'name': 'Scalability Requirements', 'description': 'Need for horizontal scaling, multi-tenancy, and global low-latency operations', 'attributes': {'entity_type': 'Requirement'}}, 'relationship_attributes': {'timestamp': None, 'sentiment': 'positive', 'hierarchy_level': 'topic_to_aspect'}, 'topic_name': 'Customer Apple Tracking', 'category': 'skeletal'}\n",
      "[2025-06-12 14:43:24,024] INFO in graph: structural triplet: {'subject': {'name': 'TiCDC', 'description': 'Change Data Capture tool for replication and synchronization in TiDB', 'attributes': {'entity_type': 'Technical Component'}}, 'predicate': 'enables failover to a standby TiDB cluster by stopping primary processes and starting replication on DR', 'object': {'name': 'Disaster Recovery (DR)', 'description': \"Critical requirement for data resilience in Apple's tracking systems\", 'attributes': {'entity_type': 'Requirement'}}, 'relationship_attributes': {'timestamp': '2024-01-30', 'sentiment': 'neutral', 'hierarchy_level': 'component_to_detail'}, 'topic_name': 'Customer Apple Tracking', 'category': 'skeletal'}\n",
      "[2025-06-12 14:43:24,024] INFO in graph: structural triplet: {'subject': {'name': 'TiCDC', 'description': 'Change Data Capture tool for replication and synchronization in TiDB', 'attributes': {'entity_type': 'Technical Component'}}, 'predicate': 'provides monitoring metrics for sink errors and replication task status via Grafana', 'object': {'name': 'Security Compliance', 'description': 'Requirements for encryption, TLS standards, and vulnerability management', 'attributes': {'entity_type': 'Requirement'}}, 'relationship_attributes': {'timestamp': None, 'sentiment': 'neutral', 'hierarchy_level': 'component_to_detail'}, 'topic_name': 'Customer Apple Tracking', 'category': 'skeletal'}\n",
      "[2025-06-12 14:43:24,024] INFO in graph: structural triplet: {'subject': {'name': 'TiDB Cloud', 'description': \"Distributed database solution evaluated for Apple's tracking needs\", 'attributes': {'entity_type': 'Technical Component'}}, 'predicate': \"requires compatibility with Apple's ARM architecture (M1/M2 chips)\", 'object': {'name': 'ARM Architecture', 'description': 'Apple hardware compatibility requirement for TiDB deployment', 'attributes': {'entity_type': 'Technical Component'}}, 'relationship_attributes': {'timestamp': None, 'sentiment': 'neutral', 'hierarchy_level': 'aspect_to_component'}, 'topic_name': 'Customer Apple Tracking', 'category': 'skeletal'}\n",
      "[2025-06-12 14:43:24,025] INFO in graph: structural triplet: {'subject': {'name': 'TiCDC', 'description': 'Change Data Capture tool for replication and synchronization in TiDB', 'attributes': {'entity_type': 'Technical Component'}}, 'predicate': 'lacks built-in write conflict resolution for multi-master replication', 'object': {'name': 'Technical Decision-Maker', 'description': 'Stakeholder responsible for validating TiDB technical suitability', 'attributes': {'entity_type': 'Key Personnel'}}, 'relationship_attributes': {'timestamp': None, 'sentiment': 'negative', 'hierarchy_level': 'component_to_detail'}, 'topic_name': 'Customer Apple Tracking', 'category': 'skeletal'}\n",
      "[2025-06-12 14:43:24,025] INFO in graph: Document(Apple Customer Questions): 10 semantic + 8 structural triplets. Extracted 18 total triplets.\n",
      "[2025-06-12 14:43:24,026] INFO in graph_builder: Processing document Apple Customer Questions: 10 narrative + 8 skeletal triplets\n",
      "[2025-06-12 14:43:28,578] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:43:32,230] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:43:36,508] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:43:39,846] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:43:43,558] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:43:46,600] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:43:49,991] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:43:53,142] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:43:56,179] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:43:58,688] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:43:59,868] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:44:03,205] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:44:05,539] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:44:08,596] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:44:11,288] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:44:15,066] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:44:18,898] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:44:21,303] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:44:23,956] INFO in _client: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-06-12 14:44:25,124] INFO in graph_builder: Successfully processed: 1 entities, 18 relationships\n",
      "[2025-06-12 14:44:25,125] INFO in graph: Processing document to extract triplets: TiDB's Relationship with MySQL and Security Vulnerabilities\n",
      "[2025-06-12 14:44:25,563] INFO in graph: Document already exists in the database: TiDB's Relationship with MySQL and Security Vulnerabilities\n",
      "[2025-06-12 14:44:25,964] INFO in graph_builder: Processing document TiDB's Relationship with MySQL and Security Vulnerabilities: 0 narrative + 0 skeletal triplets\n",
      "[2025-06-12 14:44:25,966] INFO in graph_builder: Successfully processed: 0 entities, 0 relationships\n",
      "[2025-06-12 14:44:25,966] INFO in graph: Processing document to extract triplets: 1 - Account Discovery Capture Sheet (with instruction)\n",
      "[2025-06-12 14:44:26,409] INFO in graph: Document already exists in the database: 1 - Account Discovery Capture Sheet (with instruction)\n",
      "[2025-06-12 14:44:26,805] INFO in graph_builder: Processing document 1 - Account Discovery Capture Sheet (with instruction): 0 narrative + 0 skeletal triplets\n",
      "[2025-06-12 14:44:26,806] INFO in graph_builder: Successfully processed: 0 entities, 0 relationships\n",
      "[2025-06-12 14:44:26,807] INFO in graph: Processing document to extract triplets: Louis FINAL 10102023\n",
      "[2025-06-12 14:44:27,206] INFO in graph: Document already exists in the database: Louis FINAL 10102023\n",
      "[2025-06-12 14:44:27,656] INFO in graph_builder: Processing document Louis FINAL 10102023: 0 narrative + 0 skeletal triplets\n",
      "[2025-06-12 14:44:27,657] INFO in graph_builder: Successfully processed: 0 entities, 0 relationships\n",
      "[2025-06-12 14:44:27,658] INFO in graph: Processing document to extract triplets: Apple Relevant Features\n",
      "[2025-06-12 14:44:28,063] INFO in graph: Document already exists in the database: Apple Relevant Features\n",
      "[2025-06-12 14:44:28,505] INFO in graph_builder: Processing document Apple Relevant Features: 0 narrative + 0 skeletal triplets\n",
      "[2025-06-12 14:44:28,507] INFO in graph_builder: Successfully processed: 0 entities, 0 relationships\n",
      "[2025-06-12 14:44:28,507] INFO in graph: Processing document to extract triplets: Apple POC Joint Execution Plan\n",
      "[2025-06-12 14:44:28,907] INFO in graph: Document already exists in the database: Apple POC Joint Execution Plan\n",
      "[2025-06-12 14:44:29,304] INFO in graph_builder: Processing document Apple POC Joint Execution Plan: 0 narrative + 0 skeletal triplets\n",
      "[2025-06-12 14:44:29,306] INFO in graph_builder: Successfully processed: 0 entities, 0 relationships\n",
      "[2025-06-12 14:44:29,307] INFO in graph: Processing document to extract triplets:  Technical Discovery Capture Form \n",
      "[2025-06-12 14:44:29,760] INFO in graph: Document already exists in the database:  Technical Discovery Capture Form \n",
      "[2025-06-12 14:44:30,157] INFO in graph_builder: Processing document  Technical Discovery Capture Form : 0 narrative + 0 skeletal triplets\n",
      "[2025-06-12 14:44:30,158] INFO in graph_builder: Successfully processed: 0 entities, 0 relationships\n",
      "[2025-06-12 14:44:30,159] INFO in graph: Processing document to extract triplets: Copy of  TiDB 7.0.0 Testing Report\n",
      "[2025-06-12 14:44:30,603] INFO in graph: Document already exists in the database: Copy of  TiDB 7.0.0 Testing Report\n",
      "[2025-06-12 14:44:31,008] INFO in graph_builder: Processing document Copy of  TiDB 7.0.0 Testing Report: 0 narrative + 0 skeletal triplets\n",
      "[2025-06-12 14:44:31,009] INFO in graph_builder: Successfully processed: 0 entities, 0 relationships\n",
      "[2025-06-12 14:44:31,010] INFO in graph: Processing document to extract triplets: Apple TiDB Meeting Jan 22 2024\n",
      "[2025-06-12 14:44:31,413] INFO in graph: Document already exists in the database: Apple TiDB Meeting Jan 22 2024\n",
      "[2025-06-12 14:44:31,850] INFO in graph_builder: Processing document Apple TiDB Meeting Jan 22 2024: 0 narrative + 0 skeletal triplets\n",
      "[2025-06-12 14:44:31,851] INFO in graph_builder: Successfully processed: 0 entities, 0 relationships\n",
      "[2025-06-12 14:44:31,852] INFO in graph_builder: Total triplets extracted: 18 (10 semantic + 8 structural)\n",
      "[2025-06-12 14:44:31,853] INFO in graph_builder: Iterative knowledge graph construction completed! Results: {'topic_name': 'Customer Apple Tracking', 'blueprint_id': '3d4fad24-47d2-493c-a8a0-aef68879e545', 'documents_processed': 9, 'summaries_generated': 9, 'triplets_extracted': 18, 'semantic_triplets': 10, 'structural_triplets': 8, 'entities_created': 1, 'relationships_created': 18, 'skeletal_entities_created': 0, 'skeletal_relationships_created': 0, 'narrative_entities_created': 1, 'narrative_relationships_created': 18, 'skeletal_graph': {'entities_count': 10, 'relationships_count': 10, 'skeletal_entities': [{'description': 'Customer entity evaluating TiDB for tracking systems', 'entity_type': 'Customer', 'name': 'Apple', 'role': 'central_topic'}, {'description': \"Distributed database solution evaluated for Apple's tracking needs\", 'entity_type': 'Technical Component', 'name': 'TiDB Cloud', 'role': 'key_aspect'}, {'description': 'Change Data Capture tool for replication and synchronization in TiDB', 'entity_type': 'Technical Component', 'name': 'TiCDC', 'role': 'supporting_element'}, {'description': 'Requirements for encryption, TLS standards, and vulnerability management', 'entity_type': 'Requirement', 'name': 'Security Compliance', 'role': 'key_aspect'}, {'description': 'Need for horizontal scaling, multi-tenancy, and global low-latency operations', 'entity_type': 'Requirement', 'name': 'Scalability Requirements', 'role': 'key_aspect'}, {'description': 'Cloud infrastructure providers for TiDB deployment', 'entity_type': 'Technical Component', 'name': 'AWS/GCP', 'role': 'supporting_element'}, {'description': 'Stakeholder responsible for validating TiDB technical suitability', 'entity_type': 'Key Personnel', 'name': 'Technical Decision-Maker', 'role': 'supporting_element'}, {'description': 'High-level stakeholder driving the TiDB evaluation initiative', 'entity_type': 'Key Personnel', 'name': 'Executive Sponsor', 'role': 'supporting_element'}, {'description': 'Apple hardware compatibility requirement for TiDB deployment', 'entity_type': 'Technical Component', 'name': 'ARM Architecture', 'role': 'supporting_element'}, {'description': \"Critical requirement for data resilience in Apple's tracking systems\", 'entity_type': 'Requirement', 'name': 'Disaster Recovery (DR)', 'role': 'key_aspect'}], 'skeletal_relationships': [{'relationship': 'evaluates for customer tracking system replacement', 'source_entity': 'Apple', 'target_entity': 'TiDB Cloud'}, {'relationship': 'provides features to meet horizontal scaling and multi-tenancy needs', 'source_entity': 'TiDB Cloud', 'target_entity': 'Scalability Requirements'}, {'relationship': 'must align with encryption, TLS, and vulnerability standards', 'source_entity': 'TiDB Cloud', 'target_entity': 'Security Compliance'}, {'relationship': 'requires infrastructure on cloud providers for TiDB deployment', 'source_entity': 'Apple', 'target_entity': 'AWS/GCP'}, {'relationship': 'validates technical suitability for tracking workloads', 'source_entity': 'Technical Decision-Maker', 'target_entity': 'TiDB Cloud'}, {'relationship': \"must support Apple's M1/M2 chip compatibility\", 'source_entity': 'TiDB Cloud', 'target_entity': 'ARM Architecture'}, {'relationship': 'requires DR planning for business continuity', 'source_entity': 'Apple', 'target_entity': 'Disaster Recovery (DR)'}, {'relationship': 'enables high-availability replication for tracking data consistency', 'source_entity': 'TiCDC', 'target_entity': 'Apple'}, {'relationship': 'drives alignment with business objectives for adoption', 'source_entity': 'Executive Sponsor', 'target_entity': 'TiDB Cloud'}, {'relationship': 'addresses MySQL compatibility and vulnerability management', 'source_entity': 'TiDB Cloud', 'target_entity': 'Security Compliance'}]}, 'analysis_blueprint': {'suggested_entity_types': ['Customer', 'Technical Component', 'Requirement', 'Key Personnel', 'Core Projects (e.g., POCs)', 'Business Goals', 'Challenges', 'Compliance Standards'], 'key_narrative_themes': ['Problem-solution alignment (e.g., TiDB Cloud addressing scalability and security gaps)', 'Stakeholder decision-making dynamics (Executive Sponsor vs. Technical Decision-Maker priorities)', 'Technical validation processes (POC execution, TiCDC replication testing)', 'Security compliance requirements and risk mitigation strategies', 'ARM architecture compatibility for Apple hardware integration', 'Disaster recovery planning and business continuity assurance', 'Cross-cloud (AWS/GCP) deployment considerations'], 'processing_instructions': \"Focus on extracting how TiDB Cloud components (e.g., TiCDC, ARM support) directly address Apple's scalability, security, and compliance requirements. Map technical validation activities (POC timelines, benchmark testing) to business outcomes like cost reduction and global scalability. Highlight stakeholder roles in decision-making (e.g., Executive Sponsor driving adoption vs. Technical Decision-Maker validating technical fit). Track compliance-specific details like MySQL 8.x compatibility, TLS deprecation, and encryption standards. Ensure ARM architecture alignment is emphasized as a critical technical requirement for Apple's ecosystem.\"}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_name': 'Customer Apple Tracking', 'blueprint_id': '3d4fad24-47d2-493c-a8a0-aef68879e545', 'documents_processed': 9, 'summaries_generated': 9, 'triplets_extracted': 18, 'semantic_triplets': 10, 'structural_triplets': 8, 'entities_created': 1, 'relationships_created': 18, 'skeletal_entities_created': 0, 'skeletal_relationships_created': 0, 'narrative_entities_created': 1, 'narrative_relationships_created': 18, 'skeletal_graph': {'entities_count': 10, 'relationships_count': 10, 'skeletal_entities': [{'description': 'Customer entity evaluating TiDB for tracking systems', 'entity_type': 'Customer', 'name': 'Apple', 'role': 'central_topic'}, {'description': \"Distributed database solution evaluated for Apple's tracking needs\", 'entity_type': 'Technical Component', 'name': 'TiDB Cloud', 'role': 'key_aspect'}, {'description': 'Change Data Capture tool for replication and synchronization in TiDB', 'entity_type': 'Technical Component', 'name': 'TiCDC', 'role': 'supporting_element'}, {'description': 'Requirements for encryption, TLS standards, and vulnerability management', 'entity_type': 'Requirement', 'name': 'Security Compliance', 'role': 'key_aspect'}, {'description': 'Need for horizontal scaling, multi-tenancy, and global low-latency operations', 'entity_type': 'Requirement', 'name': 'Scalability Requirements', 'role': 'key_aspect'}, {'description': 'Cloud infrastructure providers for TiDB deployment', 'entity_type': 'Technical Component', 'name': 'AWS/GCP', 'role': 'supporting_element'}, {'description': 'Stakeholder responsible for validating TiDB technical suitability', 'entity_type': 'Key Personnel', 'name': 'Technical Decision-Maker', 'role': 'supporting_element'}, {'description': 'High-level stakeholder driving the TiDB evaluation initiative', 'entity_type': 'Key Personnel', 'name': 'Executive Sponsor', 'role': 'supporting_element'}, {'description': 'Apple hardware compatibility requirement for TiDB deployment', 'entity_type': 'Technical Component', 'name': 'ARM Architecture', 'role': 'supporting_element'}, {'description': \"Critical requirement for data resilience in Apple's tracking systems\", 'entity_type': 'Requirement', 'name': 'Disaster Recovery (DR)', 'role': 'key_aspect'}], 'skeletal_relationships': [{'relationship': 'evaluates for customer tracking system replacement', 'source_entity': 'Apple', 'target_entity': 'TiDB Cloud'}, {'relationship': 'provides features to meet horizontal scaling and multi-tenancy needs', 'source_entity': 'TiDB Cloud', 'target_entity': 'Scalability Requirements'}, {'relationship': 'must align with encryption, TLS, and vulnerability standards', 'source_entity': 'TiDB Cloud', 'target_entity': 'Security Compliance'}, {'relationship': 'requires infrastructure on cloud providers for TiDB deployment', 'source_entity': 'Apple', 'target_entity': 'AWS/GCP'}, {'relationship': 'validates technical suitability for tracking workloads', 'source_entity': 'Technical Decision-Maker', 'target_entity': 'TiDB Cloud'}, {'relationship': \"must support Apple's M1/M2 chip compatibility\", 'source_entity': 'TiDB Cloud', 'target_entity': 'ARM Architecture'}, {'relationship': 'requires DR planning for business continuity', 'source_entity': 'Apple', 'target_entity': 'Disaster Recovery (DR)'}, {'relationship': 'enables high-availability replication for tracking data consistency', 'source_entity': 'TiCDC', 'target_entity': 'Apple'}, {'relationship': 'drives alignment with business objectives for adoption', 'source_entity': 'Executive Sponsor', 'target_entity': 'TiDB Cloud'}, {'relationship': 'addresses MySQL compatibility and vulnerability management', 'source_entity': 'TiDB Cloud', 'target_entity': 'Security Compliance'}]}, 'analysis_blueprint': {'suggested_entity_types': ['Customer', 'Technical Component', 'Requirement', 'Key Personnel', 'Core Projects (e.g., POCs)', 'Business Goals', 'Challenges', 'Compliance Standards'], 'key_narrative_themes': ['Problem-solution alignment (e.g., TiDB Cloud addressing scalability and security gaps)', 'Stakeholder decision-making dynamics (Executive Sponsor vs. Technical Decision-Maker priorities)', 'Technical validation processes (POC execution, TiCDC replication testing)', 'Security compliance requirements and risk mitigation strategies', 'ARM architecture compatibility for Apple hardware integration', 'Disaster recovery planning and business continuity assurance', 'Cross-cloud (AWS/GCP) deployment considerations'], 'processing_instructions': \"Focus on extracting how TiDB Cloud components (e.g., TiCDC, ARM support) directly address Apple's scalability, security, and compliance requirements. Map technical validation activities (POC timelines, benchmark testing) to business outcomes like cost reduction and global scalability. Highlight stakeholder roles in decision-making (e.g., Executive Sponsor driving adoption vs. Technical Decision-Maker validating technical fit). Track compliance-specific details like MySQL 8.x compatibility, TLS deprecation, and encryption standards. Ensure ARM architecture alignment is emphasized as a critical technical requirement for Apple's ecosystem.\"}}\n"
     ]
    }
   ],
   "source": [
    "topic_name = f\"Customer {client_name} Tracking\"\n",
    "\n",
    "print(\"step 2: add to graph\")\n",
    "result = graph_builder.build_iterative_knowledge_graph(topic_name, list(topic_docs.values()))\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
