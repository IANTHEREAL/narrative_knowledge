{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from llm.factory import LLMInterface\n",
    "from llm.embedding import get_text_embedding\n",
    "from setting.db import db_manager\n",
    "from knowledge_graph.knowledge import KnowledgeBuilder\n",
    "from knowledge_graph.graph_builder import KnowledgeGraphBuilder\n",
    "\n",
    "llm_client = LLMInterface(\"ollama\", \"qwen3:32b-fp16\")\n",
    "session_factory = db_manager.get_session_factory(os.getenv(\"GRAPH_DATABASE_URI\"))\n",
    "kb_builder = KnowledgeBuilder(llm_client, get_text_embedding, session_factory)\n",
    "graph_builder = KnowledgeGraphBuilder(llm_client, get_text_embedding, session_factory)\n",
    "\n",
    "# Initialize logging module with a basic configuration for console output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s] %(levelname)s - %(filename)s:%(lineno)d: %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "categories = [\n",
    "    'tidbcloud/API/API Overview',\n",
    "    'tidbcloud/About TiDB Cloud',\n",
    "    'tidbcloud/Billing',\n",
    "    'tidbcloud/Data Service (Beta)',\n",
    "    'tidbcloud/Develop Applications/Connect to TiDB Cloud',\n",
    "    'tidbcloud/Develop Applications/Development Reference',\n",
    "    'tidbcloud/Develop Applications/Quick Start',\n",
    "    'tidbcloud/Develop Applications/Third-Party Support',\n",
    "    'tidbcloud/Develop Applications/overview',\n",
    "    'tidbcloud/Disaster Recovery',\n",
    "    'tidbcloud/Explore Data',\n",
    "    'tidbcloud/FAQs',\n",
    "    'tidbcloud/Get Started',\n",
    "    'tidbcloud/Integrations',\n",
    "    'tidbcloud/Integrations/Terraform',\n",
    "    'tidbcloud/Integrations/Vercel',\n",
    "    'tidbcloud/Integrations/Zapier',\n",
    "    'tidbcloud/Maintenance Notification',\n",
    "    'tidbcloud/Manage Cluster/Delete a TiDB Cluster',\n",
    "    'tidbcloud/Manage Cluster/Manage TiDB Cloud Dedicated Clusters',\n",
    "    'tidbcloud/Manage Cluster/Manage TiDB Cloud Serverless Clusters',\n",
    "    'tidbcloud/Manage Cluster/Monitor and Alert',\n",
    "    'tidbcloud/Manage Cluster/Plan Your Cluster',\n",
    "    'tidbcloud/Manage Cluster/TiDB CLOUD Tune Performance',\n",
    "    'tidbcloud/Manage Cluster/Upgrade a TiDB Cluster',\n",
    "    'tidbcloud/Manage Cluster/Use an HTAP Cluster with TiFlash',\n",
    "    'tidbcloud/Migrate or Import Data/Import Data into TiDB Cloud',\n",
    "    'tidbcloud/Migrate or Import Data/Migrate Data into TiDB Cloud',\n",
    "    'tidbcloud/Migrate or Import Data/Overview',\n",
    "    'tidbcloud/Migrate or Import Data/Reference',\n",
    "    'tidbcloud/Reference/Batch Processin',\n",
    "    'tidbcloud/Reference/Benchmarks',\n",
    "    'tidbcloud/Reference/CLI',\n",
    "    'tidbcloud/Reference/DDL Execution Principles and Best Practices',\n",
    "    'tidbcloud/Reference/Glossary',\n",
    "    'tidbcloud/Reference/Resource Manager',\n",
    "    'tidbcloud/Reference/Server Status Variables',\n",
    "    'tidbcloud/Reference/Storage Engines',\n",
    "    'tidbcloud/Reference/Support',\n",
    "    'tidbcloud/Reference/Table Filter',\n",
    "    'tidbcloud/Reference/TiDB Cloud Limitations',\n",
    "    'tidbcloud/Reference/TiDB Cluster Architecture',\n",
    "    'tidbcloud/Reference/TiDB Distributed eXecution Framework (DXF)',\n",
    "    'tidbcloud/Reference/Troubleshoot Inconsistency Between Data and Indexes',\n",
    "    'tidbcloud/Reference/URI Formats of External Storage Services',\n",
    "    'tidbcloud/Release Notes',\n",
    "    'tidbcloud/Security/Audit Management',\n",
    "    'tidbcloud/Security/Data Access Control',\n",
    "    'tidbcloud/Security/Database Access Control',\n",
    "    'tidbcloud/Security/Identity Access Control',\n",
    "    'tidbcloud/Security/Network Access Control',\n",
    "    'tidbcloud/Stream Data',\n",
    "    'tidbcloud/TiDB Cloud Partner Web Console',\n",
    "    'tidbcloud/Vector Search (Beta)/Changelogs',\n",
    "    'tidbcloud/Vector Search (Beta)/Get Started',\n",
    "    'tidbcloud/Vector Search (Beta)/Improve Performance',\n",
    "    'tidbcloud/Vector Search (Beta)/Integrations',\n",
    "    'tidbcloud/Vector Search (Beta)/Limitations',\n",
    "    'tidbcloud/Vector Search (Beta)/Overview',\n",
    "    'tidbcloud/Vector Search (Beta)/Reference'\n",
    "]\n",
    "\n",
    "# Define the path to the JSON configuration file\n",
    "config_file_path = '/Users/ian/Work/docs/toc_files_for_tidb_cloud.json'\n",
    "\n",
    "# Variable to store the loaded data\n",
    "loaded_docs = []\n",
    "\n",
    "# Read the JSON configuration file\n",
    "try:\n",
    "    with open(config_file_path, 'r', encoding='utf-8') as f:\n",
    "        loaded_docs = json.load(f)\n",
    "    print(f\"Successfully loaded configuration from: {config_file_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Configuration file not found at '{config_file_path}'\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Could not decode JSON from file '{config_file_path}'. Check file format.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while reading the file: {e}\")\n",
    "\n",
    "if len(loaded_docs) > 0:\n",
    "    print(\"\\nExample: Accessing first document data:\")\n",
    "    print(loaded_docs[0])\n",
    "else:\n",
    "    print(\"\\nConfiguration file is empty.\")\n",
    "\n",
    "\n",
    "tidb_product_docs = {}\n",
    "for category in categories:\n",
    "    topic_name = \"TiDBCloud Product Documentation - \" + category\n",
    "    tidb_product_docs[topic_name] = []\n",
    "    topic_docs = set()\n",
    "    for doc in loaded_docs:\n",
    "        if category == doc['category']:\n",
    "            topic_id = f\"{category}-{doc['web_view_link']}\"\n",
    "            if topic_id in topic_docs:\n",
    "                continue\n",
    "            topic_docs.add(topic_id)\n",
    "            tidb_product_docs[topic_name].append({\n",
    "                'topic_name': topic_name,\n",
    "                'path': doc['path'],  # required\n",
    "                'doc_link': doc['web_view_link'], # required\n",
    "                'category': category,\n",
    "                'updated_at': doc['modified_time'],\n",
    "                'mime_type': doc['mime_type'],\n",
    "                'version': \"2025-07-07\"\n",
    "            })\n",
    "    print(f\"Category: {topic_name}, Number of documents: {len(tidb_product_docs[topic_name])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"step 1: upload docs to knowledge base\")\n",
    "topic_docs = {}\n",
    "for topic_name in tidb_product_docs:\n",
    "    print(\"uploading docs for topic: \", topic_name)\n",
    "    docs = tidb_product_docs[topic_name]\n",
    "    uploaded_docs = {}\n",
    "    for doc in docs:\n",
    "        file_path = doc['path']\n",
    "        try:\n",
    "            res = kb_builder.extract_knowledge(\n",
    "                file_path, \n",
    "                doc\n",
    "            )\n",
    "            if res['status'] == 'success':\n",
    "                uploaded_docs[res['source_id']] = {\n",
    "                    \"source_id\": res['source_id'],\n",
    "                    \"source_name\": res['source_name'],\n",
    "                    \"source_content\": res['source_content'],\n",
    "                    \"source_link\": res['source_link'],\n",
    "                    \"source_attributes\": res['source_attributes']\n",
    "                } \n",
    "            else:\n",
    "                print(f\"process index {file_path} failed, {res['error']}\", exc_info=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"process index {file_path} failed, {e}\", exc_info=True)\n",
    "    \n",
    "    topic_docs[topic_name] = list(uploaded_docs.values())\n",
    "\n",
    "topic_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Build Graph \n",
    "\n",
    "Assuming that source data already uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from knowledge_graph.models import SourceData\n",
    "from setting.db import db_manager\n",
    "\n",
    "def get_documents_by_topic(database_uri, topic_docs_config):\n",
    "    \"\"\"\n",
    "    Query database to get all documents aggregated by topic_name.\n",
    "    Returns a dict where each topic has an array of documents.\n",
    "    \n",
    "    Args:\n",
    "        database_uri: Database connection URI\n",
    "        topic_docs_config: Dictionary with topic_name as keys and list of doc configs as values\n",
    "                          Each doc config should have 'doc_link' field to match against SourceData.link\n",
    "    \"\"\"\n",
    "    from sqlalchemy.orm import joinedload\n",
    "    \n",
    "    result_topic_docs = defaultdict(list)\n",
    "    session_factory = db_manager.get_session_factory(database_uri)\n",
    "    \n",
    "    # Step 1: Collect all unique doc_links from all topics\n",
    "    all_doc_links = set()\n",
    "    for docs_config in topic_docs_config.values():\n",
    "        for doc in docs_config:\n",
    "            all_doc_links.add(doc['doc_link'])\n",
    "    \n",
    "    print(f\"Total unique document links to query: {len(all_doc_links)}\")\n",
    "    \n",
    "    # Step 2: Batch query all SourceData with eager loading and extract data within session\n",
    "    with session_factory() as db:\n",
    "        source_data_list = db.query(SourceData).options(\n",
    "            joinedload(SourceData.content_store)\n",
    "        ).filter(\n",
    "            SourceData.link.in_(list(all_doc_links))\n",
    "        ).all()\n",
    "        \n",
    "        print(f\"Found {len(source_data_list)} SourceData records in database\")\n",
    "        \n",
    "        # Extract all data within session to avoid DetachedInstanceError\n",
    "        link_to_doc_data = {}\n",
    "        for sd in source_data_list:\n",
    "            link_to_doc_data[sd.link] = {\n",
    "                \"source_id\": sd.id,\n",
    "                \"source_name\": sd.name,\n",
    "                \"source_content\": sd.effective_content,  # Access within session\n",
    "                \"source_link\": sd.link,\n",
    "                \"source_attributes\": sd.attributes,\n",
    "            }\n",
    "    \n",
    "    # Step 3: Assemble documents by topic in memory\n",
    "    for topic_name, docs_config in topic_docs_config.items():\n",
    "        for doc_config in docs_config:\n",
    "            doc_link = doc_config['doc_link']\n",
    "            doc_data = link_to_doc_data.get(doc_link)\n",
    "            \n",
    "            if doc_data:\n",
    "                doc_info = {\n",
    "                    **doc_data,\n",
    "                    \"topic_name\": topic_name,\n",
    "                }\n",
    "                result_topic_docs[topic_name].append(doc_info)\n",
    "            else:\n",
    "                print(f\"Warning: No SourceData found for doc_link: {doc_link}\")\n",
    "    \n",
    "    return dict(result_topic_docs)\n",
    "\n",
    "# Get all documents grouped by topic using tidb_product_docs configuration\n",
    "database_uri = os.getenv(\"GRAPH_DATABASE_URI\")\n",
    "all_topic_docs = get_documents_by_topic(database_uri, tidb_product_docs)\n",
    "\n",
    "# Display available topics\n",
    "print(\"Available topics:\")\n",
    "for topic, docs in all_topic_docs.items():\n",
    "    print(f\"  {topic}: {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = list(all_topic_docs.keys())\n",
    "topic_names = sorted(topic_names)\n",
    "topic_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = list(all_topic_docs.keys())\n",
    "topic_names = sorted(topic_names,reverse=True)\n",
    "for i, topic_name in enumerate(topic_names):\n",
    "    if topic_name != \"TiDBCloud Product Documentation - tidbcloud/Vector Search (Beta)/Reference\":\n",
    "        continue\n",
    "    # if i % 3 != 0:\n",
    "    #    continue\n",
    "    topic_docs = all_topic_docs[topic_name]\n",
    "    logger.info(\"processing topic: %s, number of docs: %d\", topic_name, len(topic_docs))\n",
    "    try:\n",
    "        result = graph_builder.build_knowledge_graph(\n",
    "            topic_name,\n",
    "            topic_docs\n",
    "        )\n",
    "\n",
    "        logger.info(\"\\n=== Memory Knowledge Graph Construction Results ===\")\n",
    "        logger.info(f\"Topic: {result['topic_name']}\")\n",
    "        logger.info(f\"Documents processed: {result['documents_processed']}\")\n",
    "        logger.info(f\"Documents failed: {result['documents_failed']}\")\n",
    "        logger.info(f\"Cognitive maps generated: {result['cognitive_maps_generated']}\")\n",
    "        logger.info(f\"Triplets extracted: {result['triplets_extracted']}\")\n",
    "        logger.info(f\"Total entities created: {result['entities_created']}\")\n",
    "        logger.info(f\"Total relationships created: {result['relationships_created']}\")\n",
    "\n",
    "        # Print global blueprint information\n",
    "        blueprint_info = result.get(\"global_blueprint\", {})\n",
    "        logger.info(f\"\\nGlobal Blueprint:\")\n",
    "        logger.info(\n",
    "            f\"  - Processing instructions: {blueprint_info.get('processing_instructions', '')}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"  - Processing items: {blueprint_info.get('processing_items', {})}\"\n",
    "        )\n",
    "\n",
    "        logger.info(\"\\n🎉 Memory knowledge graph construction completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to build knowledge graph: {e}\", exc_info=True)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        result = graph_builder.enhance_knowledge_graph(\n",
    "            topic_name,\n",
    "            topic_docs,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to ehance knowledge graph: {e}\", exc_info=True)\n",
    "        continue\n",
    "\n",
    "    logger.info(\"enhance knowledge graph result: %s\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Similarity based Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knowledge_graph.query import search_relationships_by_vector_similarity, query_topic_graph\n",
    "\n",
    "query = \"Where are li ming now?\"\n",
    "res = search_relationships_by_vector_similarity(query, similarity_threshold=0.2, top_k=20)\n",
    "context = \"\"\n",
    "entities = set()\n",
    "relationships = []\n",
    "\n",
    "for index, row in res.iterrows():\n",
    "    entities.add(f\"{row['source_entity']} {row['source_entity_description']}\")\n",
    "    entities.add(f\"{row['target_entity']} {row['target_entity_description']}\")\n",
    "    relationships.append(f\"{row['source_entity']} {row['relationship_desc']} {row['target_entity']}\")\n",
    "\n",
    "context = \"Entities:\\n\" + \"\\n\".join(entities) + \"\\n\\nRelationships:\\n\" + \"\\n\".join(relationships)\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.factory import LLMInterface\n",
    "\n",
    "llm_client = LLMInterface(\"bedrock\", \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\")\n",
    "response =llm_client.generate(f\"\"\"Given the following context\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "answer the question: {query}\n",
    "\"\"\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
